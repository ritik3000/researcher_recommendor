{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_json(\"jsons/dblp-ref-0.json\",lines=True)\n",
    "df2=pd.read_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list_collab_5=pickle.load(open(\"pickles/author_list_collab_5.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=pickle.load(open(\"pickles/authors_label_main_dataset.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_json(\"jsons/Major_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure visualization and identification for ultrasound (US) arthral structure images is essential in US-guided arthrosis operation and diagnosis for arthrosis disorder. US imaging can image arthrosis tissues and cartilaginous, but it often depicts the structure poorly, because of speckle, reverberation, shadowing feature and other artifacts. The enhancement of arthrosis structure visualization remains great challenge in US images. This paper proposes an improved diffusion to remove reverberation noise, smoothing speckles and improve the visualization of arthrosis structure in US images. The structure tensor from automatic detection is applied to keep edges structure. The potential of the method is demonstrated with the results compared to nonlinear diffusion. The preliminary results show that improved diffusion is a robust method to enhance surfaces visualization of US arthrosis structure. The method shows promise in a clinical application of arthrosis operation planning and arthrosis diagnosis.\n",
      "A two level branch and bound algorithm is developed to solve an altered form of the standard flow-shop scheduling problem modeled as a bilevel programming problem. The flow-shop scheduling problem considered here differs from the standard problem in that operators are assigned to the machines and each operator has a different time table for the jobs on each machine. The shop owner is considered the top planner and assigns the operators to the machines in order to minimize the total flowtime while the customer is the bottom planner and decides on a job schedule in order to minimize the makespan.\n",
      "With the increasing amount of data and the need to integrate data from multiple data sources, a challenging issue is to find near duplicate records efficiently. In this paper, we focus on efficient algorithms to find pairs of records such that their similarities are above a given threshold. Several existing algorithms rely on the prefix filtering principle to avoid computing similarity values for all possible pairs of records. We propose new filtering techniques by exploiting the ordering information; they are integrated into the existing methods and drastically reduce the candidate sizes and hence improve the efficiency. Experimental results show that our proposed algorithms can achieve up to 2.6x - 5x speed-up over previous algorithms on several real datasets and provide alternative solutions to the near duplicate Web page detection problem.\n",
      "Traditional DRAM has faced more challenges in the memory subsystem. Meanwhile, more types of memories become available as new technologies have been developed in many areas. In this case, the unified memory architecture should be changed to a heterogeneous one to utilize the new memories and obtain optimal performance in terms of memory access latency and life time. In this paper, a hierarchical model is studied and compared with a flat model. To evaluate our designs, the system bus trace is collected for realistic trace-driven simulation. We use typical server benchmark SPEC jbb2005 and typical desktop benchmarks Quake 3 and SYSmark 2007 as our evaluation workloads. The experimental results show that the performance of proposed hierarchical model is very stable in writing access and its average reading access time is not sensitive to its associativity.\n",
      "DDoS attacks are major threats in current computer networks. However, DDoS attacks are difficult to be quickly detected. In this paper, we introduce a system that only extracts several important attributes from network traffic for DDoS attack detection in real computer networks. We collect a large set of DDoS attack traffic by implementing various DDoS attacks as well as normal data during normal usage. Information Gain and Chi-square methods are used to rank the importance of 41 attributes extracted from the network traffic with our programs. Bayesian networks as well as C4.5 are then employed to detect attacks as well as to determine what size of attributes is appropriate for fast detection. Empirical results show that only using the most important 9 attributes, the detection accuracy remains the same or even has some improvements compared with that of using all the 41 attributes based on Bayesian Networks and C4.5 methods. Only using several attributes also improves the efficiency in terms of attributes constructing, models training as well as intrusion detection.\n",
      "This correspondence considers the stability problem for a class of linear switched systems with time-varying delay in the sense of Hurwitz convex combination. The bound of derivative of the time-varying delay can be an unknown constant. It is concluded that the stability result for linear switched systems still holds for such systems with time-varying delay under a certain delay bound. Moreover, the delay bound of guaranteeing system stability can be easily obtained based on linear matrix inequalities (LMIs). As a special case, when the time-varying delay becomes constant, the criterion obtained in this correspondence is less conservative than existing ones. The reason for less conservativeness is also explicitly explained in this correspondence. Simulation examples illustrate the effectiveness of the proposed method.\n",
      "The idle computational resources of CSCW environment that is composed of computer clusters are mined to construct the multi-cluster grid in order to support the computation-intensive tasks. For fitting the state changes of idle computing resources during the computing process, the techniques of cooperation and migration agents are adopted. Through the concurrent dataflow, the supper element, and the density factor, the dynamic buffer pool (DBP) was built up. By using of the Grid techniques, the cooperative computing agent, the cooperative communication agent, and DBP, a cooperative communication system (CCMS) for parallel computing agent was designed and implemented. The experimental results show that CCMS can increase the speed-up of parallel computing task. It can be fit for the computation in CSCW based on Internet.\n",
      "Attributes construction and selection from audit data is the first and very important step for anomaly intrusion detection. In this paper, we present several cross frequency attribute weights to model user and program behaviors for anomaly intrusion detection. The frequency attribute weights include plain term frequency (TF) and various forms of term frequency-inverse document frequency (tfidf), referred to as Ltfidf, Mtfidf and LOGtfidf. Nearest Neighbor (NN) and k-NN methods with Euclidean and Cosine distance measures as well as principal component analysis (PCA) and Chi-square test method based on these frequency attribute weights are used for anomaly detection. Extensive experiments are performed based on command data from Schonlau et al. The testing results show that the LOGtfidf weight gives better detection performance compared with plain frequency and other types of weights. By using the LOGtfidf weight, the simple NN method and PCA method achieve the better masquerade detection results than the other 7 methods in the literature while the Chi-square test consistently returns the worst results. The PCA method is suitable for fast intrusion detection because of its capability of reducing data dimensionality while NN and k-NN methods are suitable for detection of a small data set because of its no need of training process. A HTTP log data set collected in a real environment and the sendmail system call data from University of New Mexico (UNM) are used as well and the results also demonstrate the effectiveness of the LOGtfidf weight for anomaly intrusion detection.\n",
      "This letter considers the problem of resource sharing between two selfish nodes in cooperative relay networks. In our system, each node can act as a source as well as a potential relay, and both nodes are willing to achieve an optimal signal-to-noise ratio (SNR) increase by adjusting their power levels for cooperative relaying. We formulate this problem as a two-person bargaining game, and use the Nash bargaining solution (NBS) to achieve a win-win strategy for both nodes. Simulation results indicate the NBS resource sharing is fair in that the degree of cooperation of a node only depends on how much contribution its partner can make to its SNR increase.\n",
      "This article deals with the problem of L 2-gain for a class of networked control systems with time-varying network delay in both forward and feedback channels. An improved network predictive control scheme is proposed to compensate the effects of network delay and data dropout using a switching control strategy. Based on the average dwell time method, the restrictions that the original delay-compensation strategy imposes on the subsystems are relaxed and a sufficient condition for weighted L 2-gain is developed for a class of switching signal. An example illustrates the effectiveness of the proposed method.\n",
      "In this paper, a simple but efficient approach for blind image splicing detection is proposed. Image splicing is a common and fundamental operation used for image forgery. The detection of image splicing is a preliminary but desirable study for image forensics. Passive detection approaches of image splicing are usually regarded as pattern recognition problems based on features which are sensitive to splicing. In the proposed approach, we analyze the discontinuity of image pixel correlation and coherency caused by splicing in terms of image run-length representation and sharp image characteristics. The statistical features extracted from image run-length representation and image edge statistics are used for splicing detection. The support vector machine (SVM) is used as the classifier. Our experimental results demonstrate that the two proposed features outperform existing ones both in detection accuracy and computational complexity.\n",
      "A location service is an essential prerequisite for geographic routing protocols for MANETs. We present VHLS, a new distributed location service protocol, that features a dynamic location server selection mechanism and adapts to network traffic workload, minimizing the overall location service overhead. We demonstrate that the ratio of location queries to updates is an important performance parameter in such protocols. Our analysis and simulations show that VHLS provides better query success rates, location service quality, and geographic routing performance than the GLS and GHLS protocols. VHLS also scales well as the network size and traffic workload increases.\n",
      "The appearance of the source terms in modeling non-equilibrium flow problems containing finite-rate chemistry or combustion poses additional numerical difficulties beyond that for solving non-reacting flows. A well-balanced scheme, which can preserve certain non-trivial steady state solutions exactly, may help minimize some of these difficulties. In this paper, a simple one-dimensional non-equilibrium model with one temperature is considered. We first describe a general strategy to design high-order well-balanced finite-difference schemes and then study the well-balanced properties of the high-order finite-difference weighted essentially non-oscillatory (WENO) scheme, modified balanced WENO schemes and various total variation diminishing (TVD) schemes. The advantages of using a well-balanced scheme in preserving steady states and in resolving small perturbations of such states will be shown. Numerical examples containing both smooth and discontinuous solutions are included to verify the improved accuracy, in addition to the well-balanced behavior.\n",
      "This paper presents Sora, a fully programmable software radio platform on commodity PC architectures. Sora combines the performance and fidelity of hardware SDR platforms with the programmability and flexibility of general-purpose processor (GPP) SDR platforms. Sora uses both hardware and software techniques to address the challenges of using PC architectures for high-speed SDR. The Sora hardware components consist of a radio front-end for reception and transmission, and a radio control board for high-throughput, low-latency data transfer between radio and host memories. Sora makes extensive use of features of contemporary processor architectures to accelerate wireless protocol processing and satisfy protocol timing requirements, including using dedicated CPU cores, large low-latency caches to store lookup tables, and SIMD processor extensions for highly efficient physical layer processing on GPPs. Using the Sora platform, we have developed a demonstration radio system called SoftWiFi. SoftWiFi seamlessly interoperates with commercial 802.11a/b/g NICs, and achieves equivalent performance as commercial NICs at each modulation.\n",
      "A fundamental goal for Cloud computing is to group resources to accomplish tasks that may require strong computing or communication capability. In this paper we design specific resource sharing technology under which IO peripherals can be shared among Cloud members. In particular, in a personal Cloud that is built up by a number of personal devices, IO peripherals at any device can be applied to support application running at another device. We call this IO sharing composable IO because it is equivalent to composing IOs from different devices for an application. We design composable USB and achieve pro-migration USB access, namely a migrated application running at the targeted host can still access the USB IO peripherals at the source host. This is supplementary to traditional VM migration under which application can only use resources from the device where the application runs. Experimental results show that through composable IO applications in personal Cloud can achieve much better user experience.\n",
      "An approach is proposed for abnormal sections detection in video sequences. In this approach, firstly the histogram is selected to describe the color change in the section, and then the histograms of the frames selected from the section compose the histogram matrix. In order to improve the process efficiency, the principal components analysis (PCA) is used to reduce dimensions of the histogram matrix. Secondly the information of frames differences is used to describe the motion change information, and the information twice frame difference is used to describe the change rate. Lastly, the results gotten from the features are fused to get the final result. This method is experimented with some samples. The results show that this method can realize the abnormal detections and rank classification of video sequences, and the results are better than the other related techniques.\n",
      "The query focused multi-document summarization tasks usually tend to answer the queries in the summary. In this paper, we suggest introducing an effective feature which can represent the relation of key terms in the query. Here, we adopt the feature of term proximity commonly used in the field of information retrieval, which has improved the retrieval performance according to the relative position of terms. To resolve the problem of data sparseness and to represent the proximity in the semantic level, concept expansion is conducted based on WordNet. By leveraging the term importance, the proximity feature is further improved and weighted according to the inverse term frequency of terms. The experimental results show that our proposed feature can contribute to improving the summarization performance.\n",
      "JL-2, as a new version of the JL reconfigurable mobile robot system, features not only a docking and 3D posture adjusting capability between its robots, but also a multi-functional docking gripper. The basic concept of JL is that the robots in the system can simultaneously perform basic tasks in flat terrains, and in the case of rugged terrains, the robots can interconnect to enhance their locomotion capabilities. This paper introduces new designs for JL-2 by which the docking mechanism can be used as a simple gripper with 3 DOFs. Then the technologies of the docking mechanism are discussed in detail, including the workspace of the docking gripper, the docking procedure and analyses of the self-aligning ability. Then the workspaces of the posture adjusting mechanisms between two docked robots are analyzed to clarify the reconfiguration ability of JL-2. At last, a series of real experiments are proposed to test the designs and analyses and the basic performance of JL-2.\n",
      "Knowledge element relation recognition is to mine intrinsic and hidden relations, i.e., preorder, analogy and illustration from knowledge element set, which can be used in knowledge organization and knowledge navigation system. This paper focuses on what information is employed to recognize knowledge element relations. First, a formal definition of knowledge element and the types of relation are given. Next, an algorithm for knowledge element sort is proposed to gain the sequence number of knowledge element. Then, information of term, type, distance, knowledge element relation level and document level is selected to represent candidate relation instances. Evaluation on the four data sets related to “computer” discipline, using Support Vector Machines, shows that term, type and distance features contribute to most of the performance improvement, and incorporation of all features can achieve excellent performance of relation recognition, whose F1 Micro-averaged measure is above 83%.\n",
      "Three algorithms for processing joins on attributes of a textual type are presented and analyzed in this paper. Since such joins often involve document collections of very large size, it is very important to find efficient algorithms to process them. The three algorithms differ according to whether the documents themselves or the inverted files on the documents are used to process the join. Our analysis and simulation results indicate that the relative performance of these algorithms depends on the input document collections, the system characteristics and the input query. For each algorithm, the type of input document collection with which the algorithm is likely to perform well is identified.\n",
      "Autonomous docking is an essential capability for self-reconfigurable robots. In this paper, a measurement system based on ultrasonic sensors for measuring distance between two mobile modules is firstly introduced. Then, triangle localization arithmetic for measuring the relative position/orientation between two mobile modules to implement docking is analyzed. Considering inherent drawbacks of triangle localization arithmetic by experiments, a novel arithmetic of centering alignment based on the concept that the received ultrasonic signal intensity should be balanced when the two modules are centered and faced each other is proposed, and involved parameters are discussed and optimized. Following experiments indicate that the centering alignment arithmetic is high-efficiency and good-reliability and can implement the docking of JL-2 modules accurately. The centering alignment arithmetic proposed in this paper can be generally applied to other planar docking process.\n",
      "While the development of Mobile WiMAX devices is still an ongoing process, complete and accurate simulations become more important in order to study the performance of Mobile WiMAX based broadband wireless access networks. To further improve network simulation models for Mobile WiMAX, we have theoretically modeled the Hybrid Automatic Retransmission reQuest (HARQ) mechanism and evaluated its performance and accuracy. In this paper, we present the design and implementation methodology of the Mobile WiMAX HARQ simulation model in system-level network simulators in order to gain the benefit of link quality improved by enabling HARQ. Our results show that HARQ achieves higher throughput even at low signal strength which can be considered as non-line-of-sight (NLOS) environments. We believe the implementation of HARQ in the network simulation model and its evaluation studies will provide profound understanding to Mobile WiMAX field test studies.\n",
      "Cognitive radio makes it possible for an unlicensed user to access a licensed spectrum opportunistically on the basis of non-interfering. This paper addresses the problem of resource allocation for multiaccess channel (MAC) of OFDMA-based cognitive radio networks, taking into account of the interference temperature constraints. The objective is to maximize the system utility, which is used to quantify different quality-of-service (QoS) requirements of different users. Firstly, a theoretical framework is provided, where necessary and sufficient conditions for optimal subcarrier assignment and power allocation are presented under certain constraints. Then, an effective algorithm is devised for more practical conditions based on Lagrangian duality theory, where subgradient/ellipsoid method is applied for Lagrangian multipliers iteration. With polynomial time complexities, the proposed resource allocation algorithm is proved to achieve optimal system performance by numerical results.\n",
      "A color image splicing detection method based on gray level co-occurrence matrix (GLCM) of thresholded edge image of image chroma is proposed in this paper. Edge images are generated by subtracting horizontal, vertical, main and minor diagonal pixel values from current pixel values respectively and then thresholded with a predefined threshold T. The GLCMs of edge images along the four directions serve as features for image splicing detection. Boosting feature selection is applied to select optimal features and Support Vector Machine (SVM) is utilized as classifier in our approach. The effectiveness of the proposed method has been demonstrated by our experimental results.\n",
      "Half duplex devices are widely used in today's wireless networks. These devices can only send or receive, but not do both at the same time. In this paper, we use cooperative decode-forward relay strategies to increase the throughput of half-duplex wireless networks. Due to the half duplex constraint, relays need to carefully choose their transmission states in order to maximize the throughput. We show that the transmission schedule optimization can be formulated as a linear programming problem. Although the number of possible states grows exponentially as the number of relays increases, only a small subset of these states needs to be used in the optimal transmission schedule. This observation allows us to use heuristic algorithms to solve for near-optimal schedule in large networks. Our numerical results show that the decode-forward strategy can provide nearly 3 times more throughput than the traditional multi-hop relaying strategy in half duplex wireless networks.\n",
      "Digital images can be easily tampered with image editing tools. The detection of tampering operations is of great importance. Passive digital image tampering detection aims at verifying the authenticity of digital images without any a prior knowledge on the original images. There are various methods proposed in this filed in recent years. In this paper, we present an overview of these methods in three levels, that is low level, middle level, and high level in semantic sense. The main ideas of the proposed approaches at each level are described in detail, and some comments are given.\n",
      "In H.264/AVC video encoding, zero quantized discrete cosine transform (DCT) coefficients are quite common in low bit-rate video application. The computations of DCT and quantization can be remarkably reduced if zero quantized DCT coefficients are detected prior to DCT and quantization. Many methods have been applied to early detect most zero quantized DCT coefficients. But the detection ratios of zero quantized DCT coefficients need to be improved when quantization parameter (QP) is small. We present an adaptive method to detect zero quantized DCT coefficients. When QP exceeds a certain value, a new threshold is derived to efficiently detect the all-zero blocks (AZBs) without any video degradation. Otherwise, a concept of fourteen-zeros block (FZB), which means only two coefficients are non-zero in 16 DCT coefficients of a 4 times 4 block, is proposed. An innovative method to process FZBs is proposed to reduce the redundant computations. Experimental results show that the proposed adaptive method achieves approximately 8%-20% computational savings, compared with that of the existing methods.\n",
      "We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and our syntax-based translation system. On a large-scale Chinese-English translation task, we obtain statistically significant improvements of +1.5 Bleu and + 1.1 Bleu, respectively. We analyze the impact of the new features and the performance of the learning algorithm.\n",
      "The universal approximation theorem of the fuzzy logic systems (FLS) is utilized to develop an adaptive control scheme for a class of nonlinear MIMO systems by the backstepping technique. The MIMO systems consist of some subsystems and each subsystem is able to be reputed as non-affine pure-feedback structure. The external disturbances appear in each equation of each subsystem and the disturbance coefficients are assumed to be unknown functions rather than constant one. The two main advantages of the developed scheme are that (1) it does not require a priori knowledge of the signs of the control gains and (2) only one parameter is needed to be adjusted online in controller design procedure for each subsystem. It is proven that, under the appropriate assumptions, the developed scheme can achieve that all the signals in the closed-loop system are bounded and the tracking errors converge to a small neighborhood around zero. Effectiveness of the developed scheme is illustrated by the simulation example.\n",
      "In this paper, a distributed power and end-to-end rate control algorithm is proposed in the presence of licensed users. By Lagrangian duality theory, the optimal power and rate control solution is given for the unlicensed users while satisfying the interference temperature limits to licensed users. It is obtained that transmitting with either 0 or the maximum node power is the optimal scheme. The synchronous and asynchronous distributed algorithms are proposed to be implemented at the nodes and links. The convergence of the proposed algorithms are proved. Finally, further discussion on the utility-based fairness is provided for the proposed algorithms. Numerical results show that the proposed algorithm can limit the interference to licensed user under a predefined threshold.\n",
      "Protein function prediction is one of the central problems in computational biology. We present a novel automated protein structure-based function prediction method using libraries of local residue packing patterns that are common to most proteins in a known functional family. Critical to this approach is the representation of a protein structure as a graph where residue vertices (residue name used as a vertex label) are connected by geometrical proximity edges. The approach employs two steps. First, it uses a fast subgraph mining algorithm to find all occurrences of family-specific labeled subgraphs for all well characterized protein structural and functional families. Second, it queries a new structure for occurrences of a set of motifs characteristic of a known family, using a graph index to speed up Ullman’s subgraph isomorphism algorithm. The confidence of function inference from structure depends on the number of family-specific motifs found in the query structure compared with their distribution in a large non-redundant database of proteins. This method can assign a new structure to a specific functional family in cases where sequence alignments, sequence patterns, structural superposition and active site templates fail to provide accurate annotation.\n",
      "Path planning for redundant robotic manipulators received continuous interest in the past decades. Most efforts focused on random sampling-based methods, such as Probabilistic Roadmap method (PRM) and Rapidly-Exploring Random Tree (RRT), since they are suitable for planning in high-dimensional configuration space. Given the workspace goal position and orientation of the end-effector, however, explicitly calculating a collision-free and reachable goal configuration for robot joint angles in the presence of joint limits and self-collisions is not a trivial work. The difficulty forms a bottleneck for the broader applicability of the randomized path planning methods. In this paper, a novel two-stage approach is presented to implicitly solve the formation of inverse kinematics (IK) problems, which employs a variant of RRT to embed the process of IK calculation into construction and exploration of the tree-based data structure. Combined with bidirectional RRT-Connect algorithm, the two-stage approach can efficiently address the path planning problem for general redundant manipulators. The algorithm has been implemented and several 2-D and 3-D experiments demonstrate the effectiveness of the method.\n",
      "In many delay-tolerant applications, information is opportunistically exchanged between mobile devices that encounter each other. In order to affect such information exchange, mobile devices must have knowledge of other devices in their vicinity. We consider scenarios in which there is no infrastructure and devices must probe their environment to discover other devices. This can be an extremely energy-consuming process and highlights the need for energy-conscious contact-probing mechanisms. If devices probe very infrequently, they might miss many of their contacts. On the other hand, frequent contact probing might be energy inefficient. In this paper, we investigate the tradeoff between the probability of missing a contact and the contact-probing frequency. First, via theoretical analysis, we characterize the tradeoff between the probability of a missed contact and the contact-probing interval for stationary processes. Next, for time-varying contact arrival rates, we provide an optimization framework to compute the optimal contact-probing interval as a function of the arrival rate. We characterize real-world contact patterns via Bluetooth phone contact-logging experiments and show that the contact arrival process is self-similar. We design STAR, a contact-probing algorithm that adapts to the contact arrival process. Instead of using constant probing intervals, STAR dynamically chooses the probing interval using both the short-term contact history and the long-term history based on time of day information. Via trace-driven simulations on our experimental data, we demonstrate that STAR requires three to five times less energy for device discovery than a constant contact-probing interval scheme.\n",
      "This study presents the applicability of support vector machine (SVM) ensemble for traffic incident detection. The SVM has been proposed to solve the problem of traffic incident detection, because it is adapted to produce a nonlinear classifier with maximum generality, and it has exhibited good performance as neural networks. However, the classification result of the practically implemented SVM depends on the choosing of kernel function and parameters. To avoid the burden of choosing kernel functions and tuning the parameters, furthermore, to improve the limited classification performance of the real SVM, and enhance the detection performance, we propose to use the SVM ensembles to detect incident. In addition, we also propose a new aggregation method to combine SVM classifiers based on certainty. Moreover, we proposed a reasonable hybrid performance index (PI) to evaluate the performance of SVM ensemble for detecting incident by combining the common criteria, detection rate (DR), false alarm rate (FAR), mean time to detection (MTTD), and classification rate (CR). Several SVM ensembles have been developed based on bagging, boosting and cross-validation committees with different combining approaches, and the SVM ensemble has been tested on one real data collected at the I-880 Freeway in California. The experimental results show that the SVM ensembles outperform a single SVM based AID in terms of DR, FAR, MTTD, CR and PI. We used one non-parametric test, the Wilcoxon signed ranks test, to make a comparison among six combining schemes. Our proposed combining method performs as well as majority vote and weighted vote. Finally, we also investigated the influence of the size of ensemble on detection performance.\n",
      "In this paper, we present the first distinguishing attack on HMAC and NMAC based on MD5 without related keys, which distinguishes the HMAC/NMAC-MD5 from HMAC/NMAC with a random function. The attack needs 297 queries, with a success probability 0.87, while the previous distinguishing attack on HMAC-MD5 reduced to 33 rounds takes 2126.1 messages with a success rate of 0.92. Furthermore, we give distinguishing and partial key recovery attacks on MD x  -MAC based on MD5. The MD x  -MAC was proposed by Preneel and van Oorschot in Crypto'95 which uses three subkeys derived from the initial key. We are able to recover one 128-bit subkey with 297 queries.\n",
      "We address the problem of unsupervised ensemble ranking in this paper. Traditional approaches either combine multiple ranking criteria into a unified representation to obtain an overall ranking score or to utilize certain rank fusion or aggregation techniques to combine the ranking results. Beyond the aforementioned  combine-then-rank  and  rank-then-combine  approaches, we propose a novel  rank-learn-combine  ranking framework, called Interactive Ranking (iRANK), which allows two base rankers to \"teach\" each other before combination during the ranking process by providing their own ranking results as feedback to the others so as to boost the ranking performance. This mutual ranking refinement process continues until the two base rankers cannot learn from each other any more. The overall performance is improved by the enhancement of the base rankers through the mutual learning mechanism. We apply this framework to the sentence ranking problem in query-focused summarization and evaluate its effectiveness on the DUC 2005 data set. The results are encouraging with consistent and promising improvements.\n",
      "This paper combines Tangible Augmented Reality and shape grammar into collaborative design learning to bridge the gaps such as the difficulties of imaging the spatial form in a complex content and the obstacle of communication during the collaborative design. This work has been successful in mapping out a space of technical possibilities and providing a possible system setup to pursue the innovative idea. It not only describes the latent trends and assumptions that might be used to motivate and guide the design in cooperative work, but also makes links with existing research in cognitive science and education.\n",
      "Cognitive radio makes it possible for an unlicensed user to access a licensed spectrum opportunistically on the basis of non-interfering. This paper addresses the problem of joint route selection and resource allocation in OFDMA-based multi-hop cognitive radio networks, in the objective of optimizing different types of end-to-end performance. Aiming to solve it optimally, we first show that this problem of optimal resource allocation can be formulated as a convex optimization problem and identify its necessary and sufficient conditions. Based on this conclusion, we propose an iterative algorithm that can be implemented in a distributed manner. This algorithm applies Lagrangian duality theory and the Frank-Wolfe method. The scheme thus converges to a globally optimal solution. We present numerical results from using the algorithm to provide insight into the optimal cross-layer design, e.g., the relationship between bottleneck throughput and hops, and the effect of Interference Temperature constraints.\n",
      "In this paper, we study the problem of scheduling sensor activity to cover a set of targets with known locations such that all targets can be monitored all the time and the network can operate as long as possible. A solution to this scheduling problem is to partition all sensors into some sensor covers such that each cover can monitor all targets and the covers are activated sequentially. In this paper, we propose to provide information coverage instead of the conventional sensing disk coverage for target. The notion of information coverage is based on estimation theory to exploit the collaborative nature of geographically distributed sensors. Due to the use of information coverage, a target that is not within the sensing disk of any single sensor can still be considered to be monitored (information covered) by the cooperation of more than one sensor. This change of the problem settings complicates the solutions compared to that by using a disk coverage model. We first define the target information coverage (TIC) problem and prove its NP-completeness. We then propose a heuristic to approximately solve our problem. Simulation results show that our heuristic is better than an existing algorithm and is close to the upper bound when only the sensing disk coverage model is used. Furthermore, simulation results also show that the network lifetime can be significantly improved by using the notion of information coverage compared with that by using the conventional definition of sensing disk coverage. Copyright © 2008 John Wiley & Sons, Ltd.\n",
      "Consisting of sensors and vehicles, UnderWater Sensor Networks (UWSNs) are deployed to perform collaborative monitoring tasks over a given region, such as water quality monitoring, mining equipment monitoring, oceanographic data collection, pollution surveillance, etc. However, comparing with terrestrial wireless sensor networks, it is more crucial to prolong network lifetime for UWSNs since the varying characteristics of the underwater environment and superior difficulty of un- derwater device maintenance. In this paper we present a three- dimensional hemisphere model for UWSNs and prove that the improvement in network lifetime by utilizing one Automatic Underwater Vehicle (AUV) is upper bounded by a factor of eight and the AUV only needs to stay within a two hop radius of the sink. We further propose an underwater aggregation routing algorithm UARA to prolong lifetime by utilizing one AUV. Finally, we perform extensive simulations to validate our conclusions.\n",
      "In cognitive radio networks, the unlicensed users can utilize the unoccupied licensed spectrum opportunistically. In this paper, we propose a joint power and end-to-end rate control algorithm considering restricting the interference to licensed users. By duality theory, the optimal resource allocation solution is given for the unlicensed users while satisfying the interference temperature limits. An asynchronous algorithm is proposed to be implemented in practical networks. Finally, we give a discussion to the proposed algorithm's performance on fairness. Numerical results show that the proposed algorithm can limit the interference to licensed user under a predefined threshold while maintaining a satisfied data rate fairly.\n",
      "An approach is proposed for abnormal sections detection and rank classification in video sequences. In this approach, we give the definition of the abnormality of the video sections and create an abnormal factor to evaluate the abnormality. Firstly, the histogram is selected to describe the color change in the section, and then the histograms of the frames selected from the section compose the histogram matrix. In order to improve the process efficiency, the principal components analysis is used to reduce dimensions of the histogram matrix. Secondly, the information of frames differences is used to describe the motion change information, and the information twice frame difference is used to describe the change rate. Lastly, the results from the features are fused to get the final result. This method is experimented with some samples. The results show that this method can realize the abnormal detection and rank classification of video sequences, and the results are better than the other related techniques.\n",
      "Detection of web attacks is an important issue in current defense-in-depth security framework. In this paper, we propose a novel general framework for adaptive and online detection of web attacks. The general framework can be based on any online clustering methods. A detection model based on the framework is able to learn online and deal with \"concept drift\" in web audit data streams. Str-DBSCAN that we extended DBSCAN to streaming data as well as StrAP are both used to validate the framework. The detection model based on the framework automatically labels the web audit data and adapts to normal behavior changes while identifies attacks through dynamical clustering of the streaming data. A very large size of real HTTP Log data collected in our institute is used to validate the framework and the model. The preliminary testing results demonstrated its effectiveness.\n",
      "Based on the modular concept, this paper presents two caterpillar robot prototypes which are inspired by two typical caterpillars: Inchworm and Pine Caterpillar. The inchworm robot prototype features simplest kinematics and open chain architecture. Due to the fact that there is only one attachment module supporting the inchworm robot during crawling, we apply an Unsymmetrical Phase Method (UPM) to realize a stable crawling gait for it. A pine caterpillar robot is derived from combining two inchworm robots together. The crawling gait of it features a repetitive changing chain: Open-Closed-Open. Besides the UPM in open chain states, a four-links kinematic model is applied to control the corresponding joints to transfer the crawling wave along the robot body in the closed chain state. These two prototypes are all constructed and, and their crawling locomotion abilities have been tested on vertical glasses respectively.\n",
      "In this paper, we present a new distinguishing attack which works for secret prefix MAC based on 65-step (12-76) SHA-1. By birthday paradox, we first guarantee the existence of an internal collision at the output of the first iteration, then identify it by choosing the second message block smartly, and finally distinguish the specific MAC from a random function by making use of a near-collision differential path. The complexity of our new distinguisher is 280.9 queries with success probability 0.51. In comparison, we also present a distinguisher on secret prefix MAC instantiated with 63-step (8-70) SHA-1 according to Wang's method introduced at FSE 2009 [21], which needs about 2157 queries with success probability 0.70.\n",
      "Abstract   In this paper, a generalized           G    ′      G        -expansion method is used to seek more general exact solutions of the (2 + 1)-dimensional Broer–Kaup equations. As a result, non-travelling wave solutions with three arbitrary functions are obtained including hyperbolic function solutions, trigonometric function solutions and rational solutions. The proposed method is more powerful than Wang et al.’s method in [M.L. Wang, X.Z. Li, J.L. Zhang, The           G    ′      G        -expansion method and travelling wave solutions of nonlinear evolution equations in mathematical physics, Phys. Lett. A 372 (2008) 417–423] and it can be used for many other nonlinear evolution equations in mathematical physics.\n",
      "Spatial multiple access holds the promise to boost the capacity of wireless networks when an access point has multiple antennas. Due to the asynchronous and uncontrolled nature of wireless LANs, conventional MIMO technology does not work efficiently when concurrent transmissions from multiple stations are uncoordinated. In this paper, we present the design and implementation of a crosslayer system, called SAM, that addresses the challenges of enabling spatial multiple access for multiple devices in a random access network like WLAN. SAM uses a  chain-decoding  technique to reliably recover the channel parameters for each device, and iteratively decode concurrent frames with misaligned symbol timings and frequency offsets. We propose a new MAC protocol, called CCMA, to enable concurrent transmissions by different mobile stations while remaining backward compatible with 802.11. Finally, we implement the PHY and MAC layer of SAM using the Sora high-performance software radio platform. Our evaluation results under real wireless conditions show that SAM can improve network uplink throughput by 70% with two antennas over 802.11.\n",
      "A cross layer Unequal Error Protection (UEP) scheme based upon the multimedia position-value (P-V) partitioning has been recently proposed to only improve energy-distortion performance in Wireless Multimedia Sensor Networks (WMSN). There is also a great potential to adapt this UEP scheme into multimedia stream integrity protection. However, performance gains in multimedia transmissions with considerations of energy, distortion, and authentication in WMSN is a challenge. This difficulty results from the media codec dependency on both the content of multimedia stream authentication and protection on the integrity process itself. In this article, we propose a novel scheme which optimally matches the application layer multimedia codec dependency to both lower layer resource allocation requirements and the upper layer stream authentication. The goal of this scheme is to optimize the integrity and quality performance gain within energy constraint. The contribution of the proposed approach is two folds. Firstly, a stream authentication scheme matched with P-V codec dependency is proposed which significantly reduces additional authentication dependency overhead. Secondly, a new resource allocation scheme is proposed to improve energy-distortion-authentication performance with regard to codec dependency. Simulation studies demonstrate that the proposed scheme significantly impacts achieving multimedia transmission quality with energy efficiency and authentication assurance.\n",
      "Subgraph patterns are widely used in graph classification, but their effectiveness is often hampered by large number of patterns or lack of discrimination power among individual patterns. We introduce a novel classification method based on pattern co-occurrence to derive graph classification rules. Our method employs a pattern exploration order such that the complementary discriminative patterns are examined first. Patterns are grouped into co-occurrence rules during the pattern exploration, leading to an integrated process of pattern mining and classifier learning. By taking advantage of co-occurrence information, our method can generate strong features by assembling weak features. Unlike previous methods that invoke the pattern mining process repeatedly, our method only performs pattern mining once. In addition, our method produces a more interpretable classifier and shows better or competitive classification effectiveness in terms of accuracy and execution time.\n",
      "In this paper, we propose a novel approach to automatically generating, instead of manually designing, discriminative visual features for face detection. The features are composed by multiple local features (e.g., Haar features), and such features can capture not only the local texture information but also their spatial configurations. Therefore, the proposed feature contains rich semantic information so that the classifier built on a set of such features can achieve high accuracy and high efficiency. Experimental results show that the proposed approach outperforms the techniques based on local features and the state-of-the-art discriminative features for face detection.\n",
      "In this paper, the Exp-function method is used to obtain general solutions of a first-order nonlinear ordinary differential equation with a fourth-degree nonlinear term. Based on the first-order nonlinear ordinary equation and its general solutions, new and more general exact solutions with free parameters and arbitrary functions of the (2+1)-dimensional dispersive long wave equations are obtained, from which some hyperbolic function solutions are also derived when setting the free parameters as special values. It is shown that the Exp-function method with the help of symbolic computation provides a straightforward and very effective mathematical tool for solving nonlinear evolution equations in mathematical physics.\n",
      "Cognitive relays form a special cooperation relationship among users in cognitive radio networks, and help increase the transmission rates of both primary users and secondary users. However, we observe that in certain scenarios, the use of relays may deteriorate the performance. In this paper, we propose a novel scheme in the MAC layer, called CodeAssist, by using network coding. It renders every relayed packet useful and recoverable as long as a sufficient number of coded packets are received by the intended terminal. CodeAssist is designed to apply network coding in every relay buffer, and moreover, leads to a lower bound of relayed packets for each secondary user. We also show numerical results for further demonstrations of the improved performance with CodeAssist.\n",
      "HMM (Hidden Markov model) has been used successfully to analyze various types of time series. To fit time series with HMM, the number of hidden states should be determined before learning other parameters, since it has great impact on the complexity and precision of the fitting HMM. However this becomes too difficult when there is not enough prior knowledge about the observed series, which will lead to the increasing mean error in prediction process. To overcome this shortcoming, a prediction algorithm PAAMS for time series based on adaptive model selection is proposed. In PAAMS, the model can be dynamically updated when the prediction mean error increases. During the update process, an automatic model selection method AMSA is applied to get the best hidden state number and other model parameters. The proposed method AMSA is based on clustering, in which the number of hidden states is considered as the number of clusters. The feasibility and effectiveness of proposed prediction algorithm are explained. Experiments on American stock price data set are done and the results show that the PAAMS algorithm can achieve higher precision than that of previous study on the same data sets based on fixed model techniques.\n",
      "Predicting the intentions of an observed agent and taking corresponding countermeasures is the essential part for the future proactive intrusion detection systems (IDS) as well as intrusion prevention systems (IPS). In this paper, an approach of dynamic Bayesian network with transfer probability estimation was developed to predict whether the goal of system call sequences is normal or not, with early-warnings being launched, so as to ensure that some appropriate countermeasures could be taken in advance. Since complete set of system call state transfer can hardly be built in real environments, the empirical results show that the newly emerging system call transfer would have great impact on the prediction performance if we straightly use dynamic Bayesian network without transfer probability estimation. Therefore, we estimate the probability of new state transfer to predict the goals of system call sequences together with those in conditional probability table (CPT). It surmounts the difficulties of manually selecting compensating parameters with dynamic Bayesian network approach [Feng L, Guan X, Guo S, Gao Y, Liu P. Predicting the intrusion intentions by observing system call sequences. Computers & Security 2004; 23/3: 241-252] and obviously makes our prediction model more applicable. The University of New Mexico (UNM) and KLINNS data sets were analyzed and the experimental results show that it performs very well for predicting the goals of system call sequences with high accuracy and furthermore dispenses with much more manual work for selecting compensating parameters.\n",
      "Accurate network planning is critical with deployments of large outdoor wireless networks. Propagation and path loss considerations are key factors in predicting performance of wireless networks. We are currently studying large-scale wireless networks for US railway environments. In this study, both theoretical and measurement based propagation models indicate that the average received signal power decreased logarithmically with distance. The average large-scale path loss is expressed as a function of distance between the transmitter and the receiver by using a path loss exponent n and the close-in reference distance d 0 . However it is very important to select a suitable space reference distance that is appropriate for a specific propagation environment. In this paper, a deterministic approach is proposed to select an appropriate reference distance for a given propagation environment. Also a Minimum Mean Square Error (MMSE) based method is introduced to compute path loss exponents. Furthermore the comparison between the site specific measurement data collected from our testbed and the estimated values verifies the accuracy of our proposed method.\n",
      "The evolution of the web server contents and the emergence of new kinds of intrusions make necessary the adaptation of the intrusion detection systems (IDS). Nowadays, the adaptation of the IDS requires manual -- tedious and unreactive -- actions from system administrators. In this paper, we present a self-adaptive intrusion detection system which relies on a set of local model-based diagnosers. The redundancy of diagnoses is exploited, online, by a meta-diagnoser to check the consistency of computed partial diagnoses, and to trigger the adaptation of defective diagnoser models (or signatures) in case of inconsistency. This system is applied to the intrusion detection from a stream of HTTP requests. Our results show that our system 1) detects intrusion occurrences sensitively and precisely, 2) accurately self-adapts diagnoser model, thus improving its detection accuracy.\n",
      "Empowering users to access databases using simple keywords can relieve the users from the steep learning curve of mastering a structured query language and understanding complex and possibly fast evolving data schemas. In this tutorial, we give an overview of the state-of-the-art techniques for supporting keyword search on structured and semi-structured data, including query result definition, ranking functions, result generation and top-k query processing, snippet generation, result clustering, query cleaning, performance optimization, and search quality evaluation. Various data models will be discussed, including relational data, XML data, graph-structured data, data streams, and workflows. We also discuss applications that are built upon keyword search, such as keyword based database selection, query generation, and analytical processing. Finally we identify the challenges and opportunities of future research to advance the field.\n",
      "In this paper, the authors analyze the stability of a class of interconnected systems with subsystem unmodeled dynamics and dynamic interactions employing decentralized adaptive controllers designed by Wen, Zhou, and Wang (2008) in the presence of actuator failures. It will be shown that the global stability of the remaining closed-loop system is still ensured and the outputs are also regulated to zero when some subsystems break down.\n",
      "We propose a novel framework of autonomic intrusion detection that fulfills online and adaptive intrusion detection in unlabeled audit data streams. The framework owns ability of self-managing: self-labeling, self-updating and self-adapting. Affinity Propagation (AP) uses the framework to learn a subject's behavior through dynamical clustering of the streaming data. The testing results with a large real HTTP log stream demonstrate the effectiveness and efficiency of the method.\n",
      "Comparison of a group of multiple observer segmentations is known to be a challenging problem. A good segmentation evaluation method would allow different segmentations not only to be compared, but to be combined to generate a \"true\" segmentation with higher consensus. Numerous multi-observer segmentation evaluation approaches have been proposed in the literature, and STAPLE in particular probabilistically estimates the true segmentation by optimal combination of observed segmentations and a prior model of the truth. An Expectation---Maximization (EM) algorithm, STAPLE's convergence to the desired local minima depends on good initializations for the truth prior and the observer-performance prior. However, accurate modeling of the initial truth prior is nontrivial. Moreover, among the two priors, the truth prior always dominates so that in certain scenarios when meaningful observer-performance priors are available, STAPLE can not take advantage of that information. In this paper, we propose a Bayesian decision formulation of the problem that permits the two types of prior knowledge to be integrated in a complementary manner in four cases with differing application purposes: (1) with known truth prior; (2) with observer prior; (3) with neither truth prior nor observer prior; and (4) with both truth prior and observer prior. The third and fourth cases are not discussed (or effectively ignored) by STAPLE, and in our research we propose a new method to combine multiple-observer segmentations based on the maximum a posterior (MAP) principle, which respects the observer prior regardless of the availability of the truth prior. Based on the four scenarios, we have developed a web-based software application that implements the flexible segmentation evaluation framework for digitized uterine cervix images. Experiment results show that our framework has flexibility in effectively integrating different priors for multi-observer segmentation evaluation and it also generates results comparing favorably to those by the STAPLE algorithm and the Majority Vote Rule.\n",
      "An ideal engineering computational environment for solving design problems should be flexible, which is characterized by helping designers to establish solution strategy conveniently and integrating design resources effectively. In this paper, a flexible engineering computational system called Engineering Computational Service Grid (ECSG) based on Grid portal technology and workflow technology is presented, and the architecture of ECSG is described detailedly. For managing solution strategy of engineering computational job, the structure of BPEL (Business Process Execution Language) based workflow management subsystem is put forward, and an application scenario of ECSG is given. The research results aforementioned can clarify the functional system of engineering computational environment and provide an approach to constructing flexible engineering computational environment.\n",
      "This paper develops several new techniques of cryptanalyzing MACs based on block ciphers, and is divided into two parts.#R##N##R##N#The first part presents new distinguishers of the MAC construction Alred and its specific instance Alpha-MAC based on AES. For the Alred construction, we first describe a general distinguishing attack which leads to a forgery attack directly with the complexity of the birthday attack. A 2-round collision differential path of Alpha-MAC is adopted to construct a new distinguisher with about 265.5 chosen messages and 265.5 queries. One of the most important results is to use this new distinguisher to recover the internal state, which is an equivalent subkey of Alpha-MAC. Moreover, our distinguisher on Alred construction can be applied to the MACs based on CBC and CFB encryption modes.#R##N##R##N#The second part describes the first impossible differential attack on MACs-Pelican, MT-MAC-AES and PC-MAC-AES. Using the birthday attack, enough message pairs that produce the inner near-collision with some specific differences are detected, then the impossible differential attack on 4-round AES to the above mentioned MACs is performed. For Pelican, our attack recovers its internal state, which is an equivalent subkey. For MT-MAC-AES, the attack turns out to be a subkey recovery attack directly. The complexity of the two attacks is 285.5 chosen messages and 285.5 queries. For PC-MAC-AES, we recover its 256-bit key with 285.5 chosen messages and 2128 queries.\n",
      "Traditional remote collaboration technologies and platforms are found restrained and cumbersome for supporting geographically dispersed design activities. This paper discusses some of these limitations and argues how these limitations could possibly impair efficient communication among designers. The paper also develops a model for supporting remote collaborative design among geographically distributed designers. This model is named Spatial Faithful Groupware (SFG), which is based on Single Display Groupware (SDG) model and Mixed Presence Groupware (MPG) model. The SFG model is also demonstrated with justified discussions in an urban design scenario, as compared with SDG and MPG.\n",
      "In this paper, we investigate our previously developed run-length based features for multi-class blind image steganalysis. We construct a Support Vector Machine classifier for multi-class recognition for both spatial and frequency domain based steganographic algorithms. We also study hierarchical and non-hierarchical multi-class schemes and compare their performance for steganalysis. Experimental results demonstrate that our approach is able to classify different stego images according to their embedding techniques based on appropriate supervised learning. It is also shown that the hierarchical scheme performs better in our experiments.\n",
      "In this paper, we study the performance of IEEE 802.11a/b in a large-scale mobile railway networks and introduce our developed passive measurement approach. To provide a comprehensive evaluation, we built an outdoor multi-hop multi-interface railroad testbed (UNL-FRA Testbed), which consists of eight access points deployed along 3.5 mile of railroad track. We propose a novel large-scale passive measurement approach that synchronizes the system clocks of our monitoring systems, merges packet traces collected from multiple wireless channels across a multi-hop network, and enables a global performance view for the entire monitored network and across multiple layers. Based on the testing data collected from 15 field experiments carried out using BNSF locomotives and HyRail vehicles over a period of 18 months we conclude that in typical outdoor 802.11 railway environments the wireless link quality, the channel assignment scheme, and the handoff latency have much more significant impacts on the performance than the velocity. Furthermore, we discuss the implications of our conclusions on guaranteeing the quality of mobile services. We believe this is the first analysis on such a scale for 802.11-family railway networks.\n",
      "Recently, a new routing protocol notes as Enhanced Tree Routing (ETR) has been designated for wireless sensor networks. However, when using the ETR to determine next-hop neighbor, only the reduced hops via them are employed, but the residual energies of one-hop neighbors are not considered. To overcome this flaw of ETR, We propose an improved ETR with Energy Awareness (ETREA) for sensor network. This protocol takes both the reduced hops and the residual energies into account; it is more comprehensive to determine the optimized neighbors for packet forwarding. It is shown that such an effective decision can be made when there are far more than one-hop neighbors. Simulation results show that the ETREA not only decreases the death number of nodes in network but also achieves the energy balance as well as maximizes the whole network’s lifetime.\n",
      "In the whole working cycle of cognitive radio (CR), the primary job is to sense wireless communication environment of secondary users (SUs), therefore, spectrum sensing is the foundation and prerequisite for the application of CR. In order to get excellent detection performance, cooperative spectrum sensing has been applied widely. However, in cooperative spectrum sensing process, the detection results of the SUs have a great degree of uncertainty, leading to severe impact on the detection performances of CR. In this paper, we propose a novel cooperative spectrum sensing scheme based on fuzzy integral theory and optimization method, in which the reliability of local spectrum sensing is taken into account in the final decision whether the primary user (PU) is present or not and the optimization method is used to find the optimal fuzzy measures. Moreover, simulation results show that the proposed scheme performs better than other fusion strategies implementing in cooperative spectrum sensing scheme. Finally, almost all of the fusion strategies for cooperative spectrum sensing scheme in CR networks are discussed and analyzed.\n",
      "This article studies the problem of stability analysis for a class of networked control systems (NCS), whose control gain is assumed to be known. A switched delay system model is obtained based on the event-time-driven scheme. To solve the stability problem of the new NCS model, a new approach based on the Lyapunov functional exponential estimation method is introduced. By using this method, sufficient conditions are developed to guarantee exponential stability of the considered system. The results obtained may be less conservative than the existing ones from the perspective of maximum allowable transfer interval because the unavailable time is considered in this article. An example is given to show the effectiveness of the proposed method.\n",
      "This paper describes several case studies concerning protein function inference from its structure using our novel approach described in the accompanying paper. This approach employs family-specific motifs, i.e. three-dimensional amino acid packing patterns that are statistically prevalent within a protein family. For our case studies we have selected families from the SCOP and EC classifications and analyzed the discriminating power of the motifs in depth. We have devised several benchmarks to compare motifs mined from unweighted topological graph representations of protein structures with those from distance-labeled (weighted) representations, demonstrating the superiority of the latter for function inference in most families. We have tested the robustness of our motif library by inferring the function of new members added to SCOP families, and discriminating between several families that are structurally similar but functionally divergent. Furthermore we have applied our method to predict function for several proteins characterized in structural genomics projects, including orphan structures, and we discuss several selected predictions in depth. Some of our predictions have been corroborated by other computational methods, and some have been validated by independent experimental studies, validating our approach for protein function inference from structure.\n",
      "Motion planning for virtual human upper body including torso and arm received continuous interest in computer graphics community in the past year. Though a variety of motion planning approaches have been proposed to address the problem of 7 DOF (degrees of freedom) arm manipulation, planning the motion of entire upper body chain for general manipulation task is remained unresolved as there is no explicit inverse kinematics (IK) solution for the redundant chain. In this paper, a novel unified framework that combines the workspace goal-oriented heuristics and a variant of random sampling strategy called Goal-oriented Rapidly-exploring random tree (Goal_RRT) planner is proposed to efficiently resolve manipulation planning problems for upper body chain without the need of explicit IK solutions. Experimental results demonstrate the method is fast and reliable.\n",
      "It is widely believed that IEEE 802.11 standard is aimed mainly for fixed indoor wireless local area networks and is not suited for mobile applications, even though the IEEE 802.11b systems may work in either infrastructure mode or ad hoc mode. The impact of node mobility on ad hoc network performance has already been studied intensively, but these studies mostly do not consider temporal fluctuations of the mobile wireless channel due to the Doppler shift. An investigation of the mobility impact on the performance of IEEE 802.11b ad hoc systems with Rician/Rayleigh fading under different node velocities is presented. A comprehensive and in-depth analysis of the impacts of a multitude of different signal distortions on an IEEE 802.11b system performance is also presented. Specifically, the authors study the bit-error rate performances with respect to node velocities for different modulation schemes. The simulation results show that, owing to its extremely low implementation and deployment cost, the current IEEE 802.11b standard has its potential to be deployed in a mobile ad hoc environment if the line-of-sight path between transmitter and receiver exists.\n",
      "In this paper, we introduce a new classifier ensemble approach, applied to tissue segmentation in optical images of the uterine cervix. Ensemble methods combine the predictions of a set of diverse classifiers. The main contribution of our approach is an effective way of combination based on each classifier's performance level—namely, the sensitivity p and specificity q, which also produces an optimal estimate of the true segmentation. In comparison with previous work [1] that utilizes the STAPLE algorithm [2] for performance level based combination, this work achieves multiple-observer segmentation in a Bayesian decision framework using the maximum a posterior (MAP) principle, considering each classifier as an observer. In our experiments, we applied our method and several other popular ensemble methods to the problem of detecting Acetowhite regions in cervical images. On 100 images, the overall performance of the proposed method is better than: (i) an overall classifier learned using the entire training set, (ii) average voting ensemble, (iii) ensemble based on the STAPLE algorithm; it is comparable to that of majority voting and that of the (manually picked) best-performing individual classifier in the ensemble set.\n",
      "This paper proposed a new text categorization model based on the combination of modified back propagation neural network (MBPNN) and latent semantic analysis (LSA). The traditional back propagation neural network (BPNN) has slow training speed and is easy to trap into a local minimum, and it will lead to a poor performance and efficiency. In this paper, we propose the MBPNN to accelerate the training speed of BPNN and improve the categorization accuracy. LSA can overcome the problems caused by using statistically derived conceptual indices instead of individual words. It constructs a conceptual vector space in which each term or document is represented as a vector in the space. It not only greatly reduces the dimension but also discovers the important associative relationship between terms. We test our categorization model on 20-newsgroup corpus and reuter-21578 corpus, experimental results show that the MBPNN is much faster than the traditional BPNN. It also enhances the performance of the traditional BPNN. And the application of LSA for our system can lead to dramatic dimensionality reduction while achieving good classification results.\n",
      "In this study, we present our findings of important IEEE 802.16d performance characteristics. This study is part of our ongoing study of wireless communications technologies for the Federal Railroad Administration and provides details about our approach to modelling various channel effects, such as Doppler shift and Rician fading, details of the effect of link quality conditions, expected throughput and distance characteristics, as well as the impact of multi-hop and multi-user scenarios. Based on our simulation model, we can find a maximum throughput of approximately 69 Mbits sec-1 and a maximum distance of 31 miles, making IEEE 802.16d a promising candidate of addressing many critical requirements for large-scale deployments in environments such as the railroad industry. Our goal is to provide a comprehensive analysis that will assist others in understanding 802.16d performance and guiding future network deployments.\n",
      "In this paper, a novel vibrating method based on the principle of vibrating suction method is presented, which is called pulse vibrating method. To discuss this method in depth and evaluate its performance, simplified mathematical model based on some assumptions for both the previous sin vibrating method and the pulse vibrating method are built, and a new experimental platform is developed as well to verify the validity of the mathematical models. The experiments indicate that the experiments meet the mathematical model with only small deviation caused by some unknown factors. The experimental results also show that the pulse vibrating method is much better than the sin vibrating method for higher negative air pressure and less power consumption. At the end of this paper, conclusion is given and future work is proposed to further analyze the principle of the vibrating suction method.\n",
      "We build on improving hit-and-run's (IHR) prior success as a Monte Carlo random search algorithm for global optimization by generalizing the algorithm's sampling distribution. Specifically, in place of the uniform step-size distribution in IHR, we employ a family of parameterized step-size distributions to sample candidate points. The IHR step-size distribution is a special instance within this family. This parameterization is motivated by recent results on efficient decentralized search in the so-called Small World problems. To improve the performance of the algorithm, we adaptively tune the parameter based on the success rate of obtaining improving points. We present analytical and numerical results on simple spherical programmes to illustrate the key ideas of the relationship between the parametrization and algorithm performance. These results are then extended to global optimization problems with Lipschitz continuous objective functions. Our preliminary numerical results demonstrate the potential benefit of considering parameterized versions of IHR.\n",
      "In this paper, we study the problem of continuous monitoring of reverse k nearest neighbor queries. Existing continuous reverse nearest neighbor monitoring techniques are sensitive towards objects and queries movement. For example, the results of a query are to be recomputed whenever the query changes its location. We present a framework for continuous reverse k nearest neighbor queries by assigning each object and query with a rectangular safe region such that the expensive recomputation is not required as long as the query and objects remain in their respective safe regions. This significantly improves the computation cost. As a by-product, our framework also reduces the communication cost in client-server architectures because an object does not report its location to the server unless it leaves its safe region or the server sends a location update request. We also conduct a rigid cost analysis to guide an effective selection of such rectangular safe regions. The extensive experiments demonstrate that our techniques outperform the existing techniques by an order of magnitude in terms of computation cost and communication cost.\n",
      "Transmissions of large sized images can be a bottleneck for a wireless sensor network (WSN) due to its limited resources. Security can be another concern. This paper proposes a collaborative transmission scheme for image sensors to utilize inter-sensor correlations to decide the transmission and security sharing patterns based on the path diversities. Our proposed approach for secret image sharing on multiple node-disjoint paths for image delivery is to achieve high security without any key distribution and management, and thus the key management related problems do not exist. The energy efficiency is another major contribution made in this paper. This scheme does not only allow each image sensor to transmit optimal fractions of overlapped images through appropriate transmission paths in an energy-efficient way, but also provides unequal protection to overlapped image regions by path selections and adaptive bit error rate (BER) requirement. The simulation results show that the proposed scheme can achieve considerable gains in terms of network lifetime extension, image transmission security enhancement, image quality improvement, and energy efficiency for wireless sensor networks.\n",
      "We prove the following simple uniqueness theorem: Let A and B be two integral symmetric matrices with the same irreducible characteristic polynomial. If there exist rational orthogonal matrices Q such that B=Q^TAQ, then Q is unique up to a sign. An application of the theorem to the reconstruction conjecture is provided.\n",
      "In this paper, we develop a multiscale local discontinuous Galerkin (LDG) method to simulate the one-dimensional stationary Schrodinger-Poisson problem. The stationary Schrodinger equation is discretized by the WKB local discontinuous Galerkin (WKB-LDG) method, and the Poisson potential equation is discretized by the minimal dissipation LDG (MD-LDG) method. The WKB-LDG method we propose provides a significant reduction of both the computational cost and memory in solving the Schrodinger equation. Compared with traditional continuous finite element Galerkin methodology, the WKB-LDG method has the advantages of the DG methods including their flexibility in h-p adaptivity and allowance of complete discontinuity at element interfaces. Although not addressed in this paper, a major advantage of the WKB-LDG method is its feasibility for two-dimensional devices.\n",
      "Object recognition in stereo sequences is a simulation of human visual systems on how to analyze and understand various scenes. A pair of stereo sequences is a type of complicated information with huge amount of raw data and features associated with different parameter spaces. Therefore the automatic object recognition in stereo sequences is a difficult and unsolved task challenging many researchers. This paper puts its emphasis on solving this problem based on the data fusion theory. A new algorithm is proposed and experimented on real data. The results show that the accuracy of the object recognition is improved by applying the fusion of both 3D and motion parameters.\n",
      "We consider the problem of using query transformation to compute consistent answers when queries are posed to virtual XML data integration systems, which are specified following the local-as-view approach. This is achieved in two steps. First the given query is transformed to a new query with global constraints considered, then the new query is rewritten to queries on the underlying data sources by reversing rules in view definitions. The XPath query on the global system can be transformed in XQuery. We implement prototypes of our method, and evaluate our framework and algorithms in the experiment.\n",
      "Previously, we proposed Minimum Average Routing Path Clustering Problem (MARPCP) in multi-hop USNs. The goal of this problem is to find a clustering of a USN so that the average clustering-based routing path from a node to it nearest underwater sink is minimized. We relaxed MARPCP to a special case of Minimum Weight Dominating Set Problem (MWDSP), namely MWDSP-R. In addition, we showed the Performance Ratio (PR) of α-approximation algorithm for MWDSP-R is 3α for MARPCP. Based on this result, we showed the existence of a (15 + ∊)-approximation algorithm for MARPCP. In this paper, we first establish the NP-completeness of both MARPCP and MWDSP-R. Then, we propose a PTAS for MWDSP-R. By combining this result with our previous one, we have a (3 + ∊)-approximation algorithm for MARPCP.\n",
      "Studying the association between quantitative phenotype (such as height or weight) and single nucleotide polymorphisms (SNPs) is an important problem in biology. To understand underlying mechanisms of complex phenotypes, it is often necessary to consider joint genetic effects across multiple SNPs. ANOVA (analysis of variance) test is routinely used in association study. Important findings from studying gene-gene (SNP-pair) interactions are appearing in the literature. However, the number of SNPs can be up to millions. Evaluating joint effects of SNPs is a challenging task even for SNP-pairs. Moreover, with large number of SNPs correlated, permutation procedure is preferred over simple Bonferroni correction for properly controlling family-wise error rate and retaining mapping power, which dramatically increases the computational cost of association study.   In this article, we study the problem of finding SNP-pairs that have significant associations with a given quantitative phenotype. We propose an efficient algorithm, FastANOVA, for performing ANOVA tests on SNP-pairs in a batch mode, which also supports large permutation test. We derive an upper bound of SNP-pair ANOVA test, which can be expressed as the sum of two terms. The first term is based on single-SNP ANOVA test. The second term is based on the SNPs and independent of any phenotype permutation. Furthermore, SNP-pairs can be organized into groups, each of which shares a common upper bound. This allows for maximum reuse of intermediate computation, efficient upper bound estimation, and effective SNP-pair pruning. Consequently, FastANOVA only needs to perform the ANOVA test on a small number of candidate SNP-pairs without the risk of missing any significant ones. Extensive experiments demonstrate that FastANOVA is orders of magnitude faster than the brute-force implementation of ANOVA tests on all SNP pairs. The principles used in FastANOVA can be applied to categorical phenotypes and other statistics such as Chi-square test.\n",
      "Keyphrases can provide a brief summary of documents. Keyphrase extraction, defined as automatic selection of important phrases within the body of a document, is important in some fields. Generally the keyphrase extraction process is seen as a classification task, where feature selection and learning model are the key problems. In this paper, different shallow features are surveyed and the commonly used learning methods are compared. The experimental results demonstrate that the detailed survey of shallow features plus a simpler method can more enhance the extraction performance.\n",
      "Distributed lightpath provisioning in wavelength-division multiplexing (WDM) networks has gained wide research interests. In this article, we study the performance of distributed lightpath provisioning in WDM networks with dynamic routing and wavelength assignment (RWA). Specifically, we consider the case where routing of each lightpath is calculated based on globally flooded link-state information, and wavelength assignment is decided through local information exchanges. Simulation results show that such schemes steadily outperform those schemes with only global flooding or only local information exchanges. More significantly, the impacts of various factors on the proposed scheme, including RWA algorithm, network topology, number of wavelengths per fiber, global flooding interval, and traffic load, have been evaluated. Such evaluations help to achieve some insights useful for the future developments of efficient lightpath provisioning schemes.\n",
      "People encounter more information than they can possibly use every day. But all information is not necessarily of equal value. In many cases, certain information appears to be better, or more trustworthy, than other information. And the challenge that most people then face is to judge which information is more credible. In this paper we propose a new problem called Corroboration Trust, which studies how to find credible news events by seeking more than one source to verify information on a given topic. We design an evidence-based corroboration trust algorithm called TrustNewsFinder, which utilizes the relationships between news articles and related evidence information (person, location, time and keywords about the news). A news article is trustworthy if it provides many pieces of trustworthy evidence, and a piece of evidence is likely to be true if it is provided by many trustworthy news articles. Our experiments show that TrustNewsFinder successfully finds true events among conflicting information and identifies trustworthy news better than the popular search engines. Copyright © 2009 John Wiley & Sons, Ltd.\n",
      "In this paper, we present an efficient fast anomaly intrusion detection model incorporating a large amount of data from various data sources. A novel method based on non-negative matrix factorization (NMF) is presented to profile program and user behaviors of a computer system. A large amount of high-dimensional data is collected in our experiments and divided into smaller data blocks by a specific scheme. The system call data is divided into blocks by processes, while command data is divided into consecutive blocks with a fixed length. The frequencies of individual elements in each block of data are computed and placed column by column as data vectors to construct a matrix representation. NMF is employed to reduce the high-dimensional data vectors and anomaly detection can be realized as a very simple classifier in low dimensions. Experimental results show that the model presented in this paper is promising in terms of detection accuracy, computation efficiency and implementation for fast intrusion detection.\n",
      "Rigid docking is important for field modular self-reconfigurable robot system, and 6-dimension position/orientation offset should be conquered during alignment. This paper builds geometric constraint model of fundamental aligning methods and defines a 2-element state vector to evaluate position/orientation offset. Then, a 3-DOF hybrid rigid docking mechanism is introduced to achieve rigid docking in rugged terrain. This paper also gives out its maximum allowable offset. Based on this solution, Jl-2 prototype rigid docking mechanism is developed and tested. Following experiments shows that this docking mechanism is suitable for rigid docking of field modular self-reconfigurable robots in rugged terrain.\n",
      "In this paper, we investigated the end-to-end throughput of a chain in Cognitive Radio Networks (CRNs). We found that, the end-to-end throughput is dependant on both the primary usage patterns and the transmission scheduling scheme being used. In addition, to increase the end-to-end throughput of a Cognitive Radio (CR) chain, the scheduling scheme should be adjusted according to the primary usage patterns of the CR links in the chain. In the paper, firstly, we proposed an algorithm to approximate the achievable end-to-end throughput considering the primary usage patterns by abstraction and iteration. Then, a novel layered packets transmission scheduling scheme was proposed in attempt to realize the approximated end-to-end throughput. Finally, extensive simulations were conducted and results showed that, using proposed transmission scheduling scheme, the achievable end-to-end throughput of a CR chain is increased by considering the primary usage patterns and the final end-to-end throughput is close to the approximation.\n",
      "It is a fundamental and important task to extract key phrases from documents. Generally, phrases in a document are not independent in delivering the content of the document. In order to capture and make better use of their relationships in key phrase extraction, we suggest exploring the Wikipedia knowledge to model a document as a semantic network, where both n-ary and binary relationships among phrases are formulated. Based on a commonly accepted assumption that the title of a document is always elaborated to reflect the content of a document and consequently key phrases tend to have close semantics to the title, we propose a novel semi-supervised key phrase extraction approach in this paper by computing the phrase importance in the semantic network, through which the influence of title phrases is propagated to the other phrases iteratively. Experimental results demonstrate the remarkable performance of this approach.\n",
      "Content trust is one of the main components in the research of information retrieval. As it gets easier to add information to the Web via HTML pages, wikis, blogs, and other documents, it gets tougher to distinguish accurate or trustworthy information from inaccurate or untrustworthy information on the Web. Current technology of spam detection is based on binary metric, that is binary classification is adapted in the spam detection. In order to meet the users' need and preference, more accurate metric is needed in the content trust as well as in detecting spam information. In this paper, we use the notion of content trust for spam detection, and regard it as a ranking problem. Besides traditional text feature attributes, information quality based evidence is introduced to define the trust feature of spam information, and a novel content trust learning algorithm based on these evidence is proposed. Finally, a Web spam detection system is developed and the experiments on the real Web data are carried out, which show the proposed method performs very well in practice.\n",
      "A cost-effective spectrum sharing architecture is proposed to enable the legacy noncognitive secondary system to coexist with the primary system. Specifically, we suggest to install a few intermediate nodes, namely, the cognitive relays, to conduct the spectrum sensing and coordinate the spectrum access. To achieve the goal of win-win between primary and secondary systems, the cognitive relay may act as a cooperator for both of them, and an Opportunistic Cognitive Relaying (OCR) scheme is specially devised. In this scheme, the cognitive relay opportunistically switches among three different working modes, that is, Relay for Primary Link (RPL), Relay for Secondary Link (RSL), or Relay for Neither of the Links (RNL), respectively, based on the channel-dependent observation of both systems. In addition, the transmit power for cognitive relay and secondary transmitter in eachmode are optimally determined by maximizing the transmission rate of secondary system while keeping or even reducing the outage probability of primary system. Simulation results validate the efficiency of the proposed spectrum sharing scheme.\n",
      "Recently, local stereo matching has experienced largeprogress by the introduction of adaptive support-weights. Inthis paper, we aim at eliminating negative effects of occlusionsby proposing an occlusion-based method to improvetraditional support weights. Weights of occluded points aregreatly reduced while computing matching costs, initial disparitiesand final disparities. Experimental results on the Middlebury images demonstratethat our method is very effective in improving disparitiesof points around occluded areas and depth discontinuities.According to the Middlebury benchmark, theproposed algorithm is now the top performer among localstereo methods. Moreover, this approach can be easily integratedinto nearly all existing support weights strategies.\n",
      "Generalized Hough Transform-based methods have been successfully applied to object detection. Such methods have the following disadvantages: (i) manual labeling of training data ; (ii) the off-line construction of codebook. To overcome these limitations, we propose an unsupervised moving object detection algorithm with on-line Generalized Hough Transform. Our contributions are two-fold: (i) an unsupervised training data selection algorithm based on Multiple Instance Learning (MIL); (ii) an on-line Extremely Randomized Trees construction algorithm for on-line codebook adaptation. We evaluate the proposed algorithm on three video datasets. The experimental results show that the proposed algorithm achieves comparable performance to the supervised detection method with manual labeling. They also show that the proposed algorithm outperforms the previously proposed unsupervised learning algorithm.\n",
      "This brief deals with the problem of guaranteed cost control for a class of uncertain networked control systems with time-varying delay. An improved predictive controller design strategy is proposed to compensate for the delay and data dropout in both the forward and backward channels to achieve the desired control performance. The varying controller gains which are designed to vary with delays can lead to less conservative results. Meanwhile, an algorithm involving a convex optimization problem is presented to achieve a suboptimal guaranteed cost. Furthermore, a numerical simulation and a practical experiment are given to illustrate the effectiveness of the proposed method.\n",
      "Channel allocation in cognitive radio networks completely determines the realizability and efficiency of cognitive radio since it is the final step before the cognitive subscribers can use the spectrum holes. In this paper, we consider channel allocation in cognitive radio networks as a resource allocation problem under the circumstance that the allocation of transmission rate, link and transmission power for secondary users are restricted. And considering the impact of primary user activity on available channels originally, we formulate such resource allocation problem as a binary integer optimal programming, and then we design two algorithms to solve this problem and compare their performances.Finally, numerical results show that the proposed channel allocation model and strategies are quite feasible.\n",
      "Considerable efforts have been spent in studying subgraph problem. Traditional subgraph containment query is to retrieve all database graphs which contain the query graph g. A variation to that is to find all occurrences of a particular pattern(the query) in a large database graph. We call it subgraph matching problem. The state of art solution to this problem is GADDI. In this paper, we will propose a more efficient index and algorithm to answer subgraph matching problem. The index is based on the label distribution of neighbourhood vertices and it is structured as a multi-dimensional vector signature. A novel algorithm is also proposed to further speed up the isomorphic enumeration process. This algorithm attempts to maximize the computational sharing. It also attempts to predict some enumeration state is impossible to lead to a final answer by eagerly pruning strategy. We have performed extensive experiments to demonstrate the efficiency and the effectiveness of our technique.\n",
      "Compared with traditional testing, Computerized Adaptive Testing owes incomparable advantages. Such as flexibility, reduce the test length and measurement accuracy. There are some components in CAT, the most one is the item selection algorithm. To perform adaptive test, the most frequently adopted method is based on the maximum information (MI) of items to select the examination questions, with the view to draw the most accurate estimation for tester's capacity. There exists, however, flaws of unbalanced item-exposure as well as unequalled usage of item pool in this method. In this paper, we propose a new item selection algorithm CBIS to solve those problems, and then compare our method with MI method by an experiments. The experiment results are promising.\n",
      "Graphs are prevailingly used in many applications to model complex data structures. In this paper, we study the problem of super-graph containment search. To avoid the NP-complete subgraph isomorphism test, most existing works follow the filtering-verification framework and select graph-features to build effective indexes, which filter false results (graphs) before conducting the costly verification. However, searching features multiple times in the query graphs yields huge redundant computation, which leads to the emergence of the computation-sharing framework. This paper follows the roadmap of computation-sharing framework to efficiently process supergraph containment queries. Firstly, database graphs are clustered into disjoint groups for sharing the computation cost within each group. While it is shown NP-hard to maximize the computation-sharing benefits of a clustering, efficient algorithm is developed to approximate the optimal solution with an approximation factor of 1/2. A novel prefix-sharing indexing technique, PrefIndex, is then proposed based on which efficient query processing algorithm integrating both filtering and verification is developed. Finally, PrefIndex is enhanced with multi-level sharing and suffix-sharing to further avoid redundant computation. An extensive empirical study demonstrates the efficiency and scalability of our techniques which achieve orders of magnitudes of speed-up against the state-of-the-art techniques.\n",
      "This paper presents an algorithm using discriminative sparse representations to segment tissues in optical images of the uterine cervix. Because of the large variations in the image appearance caused by the changing of illumination and specular reflection, the different classes of color and texture features in optical images are often overlapped with each other. Using sparse representations they can be transformed to higher dimension with sparse constraints and become more linearly separated. Different from the previous reconstructive sparse representation, the discriminative method considers positive and negative samples simultaneously, which means that these generated dictionaries can be discriminative and perform better for their own classes but worse for the others. New data can be reconstructed from its sparse representations and positive and/or negative dictionaries. Classification can be achieved based on comparing the reconstructive errors. In the experiments we used our method to automatically segment the biomarker AcetoWhite (AW) regions in an archive of the uterine cervix. Compared with the other general methods including SVM, nearest neighbor and reconstructive sparse representations, our approach showed higher sensitivity and specificity.\n",
      "Frequent serial episodes within an event sequence describe the behavior of users or systems about the application. Existing mining algorithms calculate the frequency of an episode based on overlapping or non-minimal occurrences, which is prone to over-counting the support of long episodes or poorly characterizing the followed-by-closely relationship over event types. In addition, due to utilizing the Apriori-style level wise approach, these algorithms are computationally expensive. In this paper, we propose an efficient algorithm MANEPI (Minimal And Non-overlapping EPIsode) for mining more interesting frequent episodes within the given event sequence. The proposed frequency measure takes both minimal and non-overlapping occurrences of an episode into consideration and ensures better mining quality. The introduced depth first search strategy with the Apriori Property for performing episode growth greatly improves the efficiency of mining long episodes because of scanning the given sequence only once and not generating candidate episodes. Moreover, an optimization technique is presented to narrow down search space and speed up the mining process. Experimental evaluation on both synthetic and real-world datasets demonstrates that our algorithms are more efficient and effective.\n",
      "Real-time medical data about patients' physiological status can be collected simply by using wearable medical sensors based on a body sensor network. However, we lack an efficient, reliable, and secure BSN platform that can meet increasing needs in e-healthcare applications. Many such applications require a BSN to support multiple data rates with reliable and energyefficient data transmission. In this article we propose a secure and resource-aware BSN architecture to enable real-time healthcare monitoring, especially for secure wireless electrocardiogram data streaming and monitoring. A cross-layer framework was developed based on unequal resource allocation to support efficient biomedical data monitoring. In this framework important information (e.g., critical ECG data) is identified, and extra resources are allocated to protect it. Furthermore, BSN resource factors are exploited to guarantee a strict requirement of real-time performance. In this work we integrate biomedical information processing and transmission in a unified platform, where secure data transmission in a BSN proceeds with energy efficiency and minimum delay. In particular, we present a wearable ECG device consisting of small and low-power healthnode sensors for wireless three-lead ECG monitoring. Experimental and simulation results demonstrate that the proposed framework can support real-time wireless biomedical monitoring applications.\n",
      "This paper introduces an innovative data compression methodology in an adaptive way guaranteeing signal interpretation quality and energy efficiency in wireless body area sensor network. The approach is based on the discrete cosine transform. Under different channel conditions, the resulting wavelet coefficients are thresholded adaptively to match a user-specified percentage of root-mean-square difference. The nonzero coefficients of the thresholded vector are quantized adaptively by the linear quantizer of the lowest possible resolution. Signal interpretation quality and energy consuming are influenced by threshold Th, quantization length Q, and modulation method M which can be taken as a discrete optimization problem. A specific case study is designed to achieve better energy efficiency and guaranteed signal interpretation quality than typical efforts. The simulation results show that the proposed compression scheme can achieve considerable gains for ECG signals in wireless body area sensor networks.\n",
      "K Nearest Neighbor search has many applications including data mining, multi-media, image processing, and monitoring moving objects. In this paper, we study the problem of KNN over multi-valued objects. We aim to provide effective and efficient techniques to identify KNN sensitive to relative distributions of objects.We propose to use quantiles to summarize relative-distribution-sensitive K nearest neighbors. Given a query Q and a quantile φ ∈ (0, 1], we firstly study the problem of efficiently computing K nearest objects based on a φ-quantile distance (e.g. median distance) from each object to Q. The second problem is to retrieve the K nearest objects to Q based on overall distances in the “best population” (with a given size specified by φ-quantile) for each object. While the first problem can be solved in polynomial time, we show that the 2nd problem is NP-hard. A set of efficient, novel algorithms have been proposed to give an exact solution for the first problem and an approximate solution for the second problem with the approximation ratio 2. Extensive experiment demonstrates that our techniques are very efficient and effective.\n",
      "Wireless multimedia sensor networks (WMSNs) support many acoustic applications for audio surveillance, animal tracking/vocalization, human health monitoring, etc. However, resource constraints in sensor networks (such as limited battery power, bandwidth/computation capability, etc.) pose challenges for the quality and security of audio data transmission and processing. The security is a critical issue since audio information can be accessed or even manipulated in WMSNs. In order to ensure security, audio quality and energy efficiency, we propose an index-based selective audio encryption scheme for WMSNs. The scheme protects data transmissions by incorporating both resource allocation and selective encryption based on modified discrete cosine transform (MDCT). In this proposed scheme, the audio data importance is leveraged using the MDCT audio index, and wireless audio data transmission proceeds with energy efficient selective encryption. The simulation results show that the proposed approach offers a significant gain in terms of energy efficiency, encryption performance and audio transmission quality.\n",
      "This paper focuses on the feedback scheme for downlink Multi-User Multiple-Input Multiple-Output (MU-MIMO) transmission with linear precoding. When the maximum transmission rank per user is limited to one, the multiple-antenna receiver uses a weight vector to transform MIMO channel matrix into an equivalent Multiple-Input Single-Output (MISO) channel vector. Then a Channel Directional Index (CDI) and a Channel Quality Indicator (CQI) are calculated and fed back to the transmitter. The existing Multiuser Eigenmode Transmission (MET) method projects the MIMO channel to its maximum eigenvector. That results in mismatched CDI and CQI when the Minimum Mean-Squared Error (MMSE) filter or the Zero-Forcing (ZF) filter is actually used for detection. In order to solve this problem, new limited feedback schemes with a quasi-MMSE weight and a quasi-ZF weight are proposed in this paper. The proposed schemes can avoid such feedback information mismatch and maximize the Signal to Interference plus Noise Ratio (SINR). Simulation results verify that the proposed schemes outperform the MET method significantly in term of sum rate.\n",
      "In this paper, we propose a new computational model for visual saliency derived from the information maximization principle. The model is inspired by a few well acknowledged biological facts. To compute the saliency spots of an image, the model first extracts a number of sub-band feature maps using learned sparse codes. It adopts a fully-connected graph representation for each feature map, and runs random walks on the graphs to simulate the signal/information transmission among the interconnected neurons. We propose a new visual saliency measure called Site Entropy Rate (SER) to compute the average information transmitted from a node (neuron) to all the others during the random walk on the graphs/network. This saliency definition also explains the center-surround mechanism from computation aspect. We further extend our model to spatial-temporal domain so as to detect salient spots in videos. To evaluate the proposed model, we do extensive experiments on psychological stimuli, two well known image data sets, as well as a public video dataset. The experiments demonstrate encouraging results that the proposed model achieves the state-of-the-art performance of saliency detection in both still images and videos.\n",
      "Knowledge element relation extraction is to find predefined relations between pairs of knowledge elements from text documents. As a novel form for organization and management of knowledge resources, knowledge element relation can be utilized to establish knowledge navigation system, knowledge retrieval system and collaborative knowledge construction system. In this paper, we employ conditional random fields (CRFs) to extract relations between knowledge elements from natural language documents by treating the relation extraction task as a sequence labeling problem. We first introduce three rules to generate candidate relation instances, and then incorporate various features including terms, semantic type, distance and context information to represent candidate relation instances. Experimental evaluation shows that our method achieves better performance than previous work. It also indicates that CRFs outperform other probabilistic models i.e. hidden Markov model and maximum entropy, and show effective in knowledge element relation extraction.\n",
      "Recently, much study has been directed toward summarizing event data, in the hope that the summary will lead us to a better understanding of the system that generates the events. However, instead of offering a global picture of the system, the summary obtained by most current approaches are piecewise, each describing an isolated snapshot of the system. We argue that the best summary, both in terms of its minimal description length and its interpretability, is the one obtained with the understanding of the internal dynamics of the system. Such understanding includes, for example, what are the internal states of the system, and how the system alternates among these states. In this paper, we adopt an algorithmic approach for event data summarization. More specifically, we use a hidden Markov model to describe the event generation process. We show that summarizing events based on the learned hidden Markov Model achieves short description length and high interpretability. Experiments show that our approach is both efficient and effective.\n",
      "Multicast support is critical and a desirable feature of multi-radio wireless mesh networks. However, the nature of wireless network, i.e. broadcasting and interference, makes it a challenge to efficiently support multicast services. In this paper, we study how to build a multicast structure with maximum average throughput in terms of minimum transmission time and channel interference in a multi-radio wireless mesh network. The proposed MT3-DA mechanism takes both route setup and channel assignment into account. It first builds the multicast tree based on minimum transmission time between neighboring routers, and then uses destination-aware channel assignment strategy to minimize interference through assigning low-interference overlapping channel to the router that holds more descendant destinations. Simulation results show that the proposed MT3-DA can construct the multicast trees which have higher throughput than that of 2-MCM and M4 in multi-radio wireless mesh networks.\n",
      "The functions of proteins are closely related to their subcellular locations. In the post-proteomics era, the amount of gene and protein data grows exponentially, which necessitates the prediction of subcellular localization by computational means. This paper proposes mitigating the computation burden of alignment-based approaches to subcellular localization prediction by using the information provided by the N-terminal sorting signals. To this end, a cascaded fusion of cleavage site prediction and profile alignment is proposed. Specifically, the informative segments of protein sequences are identified by a cleavage site predictor. Then, only the informative segments are applied to a homology-based classifier for predicting the subcellular locations. Experimental results on a newly constructed dataset show that the method can make use of the best property of both approaches and can attain an accuracy higher than using the full-length sequences. Moreover, the method can reduce the computation time by 20 folds. We advocate that the method will be important for biologists to conduct large-scale protein annotation or for bioinformaticians to perform preliminary investigations on new algorithms that involve pairwise alignments.\n",
      "As the popularity of WebVR applications rises in recent years, a great disparity exists between the huge 3D content on servers and the limited cache capacity on clients. In order to alleviate server loading, peer-to-peer (P2P)-based 3D content distribution has been proposed recently. However, given the limited cache sizes at clients, how to maintain and replace the cache content effectively becomes an important issue. We present a progressive scene replacement mechanism (PSRM) in this paper to support interactive walkthrough in P2P-based large-scale WebVR worlds. First, we define a new metric called preservation degree, based on both the visual attention paid by the user and the content's potential relevance for sharing. Second, cached objects are replaced progressively with ascending order in terms of their preservation degrees. Experimental results have shown that PSRM enables users with limited cache to walkthrough a large-scale WebVR world with high visual quality; while download requests handled by the servers are reduced significantly.\n",
      "As a promising tool for identifying genetic markers underlying phenotypic differences, genome-wide association study (GWAS) has been extensively investigated in recent years. In GWAS, detecting epistasis (or gene–gene interaction) is preferable over single locus study since many diseases are known to be complex traits. A brute force search is infeasible for epistasis detection in the genomewide scale because of the intensive computational burden. Existing epistasis detection algorithms are designed for dataset consisting of homozygous markers and small sample size. In human study, however, the genotype may be heterozygous, and number of individuals can be up to thousands. Thus, existing methods are not readily applicable to human datasets. In this article, we propose an efficient algorithm, TEAM, which significantly speeds up epistasis detection for human GWAS. Our algorithm is exhaustive, i.e. it does not ignore any epistatic interaction. Utilizing the minimum spanning tree structure, the algorithm incrementally updates the contingency tables for epistatic tests without scanning all individuals. Our algorithm has broader applicability and is more efficient than existing methods for large sample study. It supports any statistical test that is based on contingency tables, and enables both family-wise error rate and false discovery rate controlling. Extensive experiments show that our algorithm only needs to examine a small portion of the individuals to update the contingency tables, and it achieves at least an order of magnitude speed up over the brute force approach.\n",
      "We describe a new approach for inferring the functional relationships between nonhomologous protein families by looking at statistical enrichment of alternative function predictions in classification hierarchies such as Gene Ontology (GO) and Structural Classification of Proteins (SCOP). Protein structures are represented by robust graph representations, and the fast frequent subgraph mining algorithm is applied to protein families to generate sets of family-specific packing motifs, i.e., amino acid residue-packing patterns shared by most family members but infrequent in other proteins. The function of a protein is inferred by identifying in it motifs characteristic of a known family. We employ these family-specific motifs to elucidate functional relationships between families in the GO and SCOP hierarchies. Specifically, we postulate that two families are functionally related if one family is statistically enriched by motifs characteristic of another family, i.e., if the number of proteins in a family containing a motif from another family is greater than expected by chance. This function-inference method can help annotate proteins of unknown function, establish functional neighbors of existing families, and help specify alternate functions for known proteins.\n",
      "We present a new method for identifying gene sets associated with labeled samples, where the labels can be case versus control, or genotype differences. Existing approaches to this problem assume that variations observed within a group are due primarily to noise and they, therefore, look for significant mean shifts between groups. Biological evidence suggests variations can also result from the coordination of genes. Our method attempts to identify and assess the significance of changes in gene-gene correlation patterns. We model gene-gene correlations using principal component analysis and compare their significance to a baseline of a linear models generated by random permutations of the sample labels. Simulation results show that our method detects changes that are undetectable by Hotelling's  T  2  method. Its performance on real data is comparable to existing methods with the additional capability of detecting changes in gene-interactions between sample groups.\n",
      "In order to improve the veracity and reliability of a traffic model built, or to extract important and valuable information from collected traffic data, the technique of outlier mining has been introduced into the traffic engineering domain for detecting and analyzing the outliers in traffic data sets. Three typical outlier algorithms, respectively the statistics-based approach, the distance-based approach, and the density-based local outlier approach, are described with respect to the principle, the characteristics and the time complexity of the algorithms. A comparison among the three algorithms is made through application to intelligent transportation systems (ITS). Two traffic data sets with different dimensions have been used in our experiments carried out, one is travel time data, and the other is traffic flow data. We conducted a number of experiments to recognize outliers hidden in the data sets before building the travel time prediction model and the traffic flow foundation diagram. In addition, some artificial generated outliers are introduced into the traffic flow data to see how well the different algorithms detect them. Three strategies-based on ensemble learning, partition and average LOF have been proposed to develop a better outlier recognizer. The experimental results reveal that these methods of outlier mining are feasible and valid to detect outliers in traffic data sets, and have a good potential for use in the domain of traffic engineering. The comparison and analysis presented in this paper are expected to provide some insights to practitioners who plan to use outlier mining for ITS data.\n",
      "Iterative message passing algorithms (iMPAs) which are generalized from the well-known turbo principle can reach a rapid pseudo-noise (PN) sequence acquisition at low computational complexity. However, its performance will degrade at low signal-to-noise ratio (SNR). In this paper, a soft information improvement using multiple samples in one chip is proposed. Meanwhile, to mitigate the timing error which will affect the information improvement, a Maximum-Likelihood (ML) estimation without significant increase on the complexity is introduced. Simulation results show that proposed method can realize rapid PN code acquisition at lower SNR than existing method.\n",
      "In cognitive radio networks, multiple spectrum opportunities can be used together to satisfy the service requirement by spectrum aggregation. In this paper, an admission control algorithm and a spectrum assignment strategy are proposed in order for both increasing the spectrum aggregation aware access capacity and decreasing the channel switch times when the channel states change. Considering different bandwidth requirement of secondary users, the proposed greedy admission algorithm takes limited aggregation capability into account. The channel switch times of secondary users at sensing moments is minimized based on the prediction of primary activities and the corresponding channel state transitions. The concept of outage probability is introduced into the scheme to indicate the probability of channel switch. The numerical results show the performance improvement of the proposed algorithms.\n",
      "High speed of processing massive audit data is crucial for an anomaly Intrusion Detection System (IDS) to achieve real-time performance during the detection. Abstracting audit data is a potential solution to improve the efficiency of data processing. In this work, we propose two strategies of data abstraction in order to build a lightweight detection model. The first strategy is exemplar extraction and the second is attribute abstraction. Two clustering algorithms, Affinity Propagation (AP) as well as traditional k-means, are employed to extract the exemplars, and Principal Component Analysis (PCA) is employed to abstract important attributes (a.k.a. features) from the audit data. Real HTTP traffic data collected in our institute as well as KDD 1999 data are used to validate the two strategies of data abstraction. The extensive test results show that the process of exemplar extraction significantly improves the detection efficiency and has a better detection performance than PCA in data abstraction.\n",
      "In this paper, we introduce a concept of Annotation Based Query Answer, and a method for its computation, which can answer queries on relational databases that may violate a set of functional dependencies. In this approach, inconsistency is viewed as a property of data and described with annotations. To be more precise, every piece of data in a relation can have zero or more annotations with it and annotations are propagated along with queries from the source to the output. With annotations, inconsistent data in both input tables and query answers can be marked out but preserved, instead of being filtered in most previous work. Thus this approach can avoid information loss, a vital and common deficiency of most previous work in this area. To calculate query answers on an annotated database, we propose an algorithm to annotate the input tables, and redefine the five basic relational algebra operations (selection, projection, join, union and difference) so that annotations can be correctly propagated as the valid set of functional dependency changes during query processing. We also prove the soundness and completeness of the whole annotation computing system. Finally, we implement a prototype of our system, and give some performance experiments, which demonstrate that our approach is reasonable in running time, and excellent in information preserving.\n",
      "Random forest is an excellent ensemble learning method, which is composed of multiple decision trees grown on random input samples and splitting nodes on a random subset of features. Due to its good classification and generalization ability, random forest has achieved success in various domains. However, random forest will generate many noisy trees when it learns from the data set that has high dimension with many noise features. These noisy trees will affect the classification accuracy, and even make a wrong decision for new instances. In this paper, we present a new approach to solve this problem through weighting the trees according to their classification ability, which is named Trees Weighting Random Forest (TWRF). Here, Out-Of-Bag, which is the training data subset generated by Bagging and not involved in building decision tree, is used to evaluate the tree. For simplicity, we choose the accuracy as the index that notes tree’s classification ability and set it as the tree’s weight. Experiments show that TWRF has better performance than the original random forest and other traditional methods, such as C45, Naive Bayes and so on.\n",
      "With the availability of various digital image edit tools, seeing is no longer believing. In this paper, we focus on tampered region localization for image forensics. We propose an algorithm which can locate tampered region(s) in a lossless compressed tampered image when its unchanged region is output of JPEG decompressor. We find the tampered region and the unchanged region have different responses for JPEG compression. The tampered region has stronger high frequency quantization noise than the unchanged region. We employ PCA to separate different spatial frequencies quantization noises, i.e. low, medium and high frequency quantization noise, and extract high frequency quantization noise for tampered region localization. Post-processing is involved to get final localization result. The experimental results prove the effectiveness of our proposed method.\n",
      "Analysis the positive and negative sentiments about each topic of the product are very useful to the customers and manufacturers. In this paper we propose a new topic sentiment mixture model which we call Semi-supervised Co-LDA model to obtain the positive and negative opinions from the reviews about each product. The Semi-supervised Co-LDA can model the topic and sentiment of the product reviews simultaneously. The Semi-supervised Co-LDA model we proposed is a semi-supervised model, which utilizes the well-written expert reviews as labeled data. The Co-LDA model has an additional advantage that can integrate expert opinions and ordinary opinions. Empirical experiments on the online reviews datasets from CNET show that this approach is effective for topic sentiment analysis of the product. The Co-LDA model is quite general, which can be applied to many fields such as modeling opinions in weblogs, user behavior prediction.\n",
      "In this paper, we present a new analysis on co-training, a representative paradigm of disagreement-based semi-supervised learning methods. In our analysis the co-training process is viewed as a combinative label propagation over two views; this provides a possibility to bring the graph-based and disagreementbased semi-supervised methods into a unified framework. With the analysis we get some insight that has not been disclosed by previous theoretical studies. In particular, we provide the sufficient and necessarycondition for co-training to succeed. We also discuss the relationship to previous theoretical results and give some other interesting implications of our results, such as combination of weight matrices and view split.\n",
      "Cognitive radio makes it possible for an unlicensed user to access a spectrum opportunistically on the basis of non-interfering to licensed users. This paper addresses the problem of resource allocation for multiaccess channel (MAC) of OFDMA-based cognitive radio networks. The objective is to maximize the system utility, which is used as an approach to balance the efficiency and fairness of wireless resource allocation. First, a theoretical framework is provided, where necessary and sufficient conditions for utility-based optimal subcarrier assignment and power allocation are presented under certain constraints. Second, based on the theoretical framework, effective algorithms are devised for more practical conditions, including ellipsoid method for Lagrangian multipliers iteration and Frank–Wolfe method for marginal utilities iteration. Third, it is shown that the proposed scheme does not have to track the instantaneous channel state via an outage-probability-based solution. In the end, numerical results have confirmed that the utility-based resource allocation can achieve the optimal system performance and guarantee fairness. Copyright © 2009 John Wiley & Sons, Ltd.#R##N##R##N#This paper addresses the problem of resource allocation for multi-access channel (MAC) of OFDMA-based cognitive radio networks, with the objective of maximizing the system utility. A theoretical framework is provided, where necessary and sufficient conditions for utility-based optimal sub-carrier assignment and power allocation are presented under certain constraints. Then effective algorithms are devised for more practical conditions, including ellipsoid method for Lagrangian multipliers iteration and Frank–Wolfe method for marginal utilities iteration. Copyright © 2009 John Wiley & Sons, Ltd.\n",
      "This paper deals with the problem of exponential stability for a class of linear discrete switched systems with constant delays. The switched systems consist of stable and unstable subsystems. Based on the average dwell time method, some switching signals will be found to guarantee exponential stability of these systems. The explicit state decay estimation is also given in the form of the solutions of linear matrix inequalities (LMIs). An example relating to networked control systems (NCSs) illustrates the effectiveness of the proposed method.\n",
      "Radio transceivers are the main source of energy consumption in wireless sensor networks (WSNs) where the source of energy supply is non-rechargeable battery. Several MAC protocols have been proposed in order to efficiently conserve energy in the link layer via duty-cycling. Low power listening (LPL) methods have been shown to outperform other schemes in lightly loaded situations which are common in environment monitoring applications. Nonetheless, as the network becomes dense, in LPL protocols such as BMAC a large number of nodes stay awake for each transmission, resulting in high levels of energy consumption. This paper introduces the informative preamble sampling (IPS) protocol in which a transmitter implicitly embeds information about its intended receiver via the power at which the preamble is transmitted. This results in far fewer nodes staying awake for each preamble. Upon hearing the preamble, a receiver executes a decision-making algorithm to decide whether to stay awake. If the decision-making algorithm is too lax, then more nodes stay awake following the preamble. On the other hand if the algorithm is too strict, it is likely that the intended receiver misses the preamble. In this paper we derive the optimal operating points for the IPS protocol. We show analytically that the IPS protocol can achieve a gain in energy by at least a factor of 2 over BMAC. We also conduct extensive simulations to show that IPS can achieve significant energy gains compared to BMAC.\n",
      "The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be O(log 1/e), contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is O(1/e), where the order of 1/e is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of 1/e is related to the parameter in Tsybakov noise.\n",
      "Substructure similarity search is to retrieve graphs that approximately contain a given query graph. It has many applications, e.g., detecting similar functions among chemical compounds. The problem is challenging as even testing subgraph containment between two graphs is NP-complete. Hence, existing techniques adopt the filtering-and-verification framework with the focus on developing effective and efficient techniques to remove non-promising graphs.   Nevertheless, existing filtering techniques may be still unable to effectively remove many \"low\" quality candidates. To resolve this, in this paper we propose a novel indexing technique, GrafD-Index, to index graphs according to their \"distances\" to features. We characterize a tight condition under which the distance-based triangular inequality holds. We then develop lower and upper bounding techniques that exploit the GrafD-Index to (1) prune non-promising graphs and (2) include graphs whose similarities are guaranteed to exceed the given similarity threshold. Considering that the verification phase is not well studied and plays the dominant role in the whole process, we devise efficient algorithms to verify candidates. A comprehensive experiment using real datasets demonstrates that our proposed methods significantly outperform existing methods.\n",
      "In this paper, radio frequency (RF) through-silicon via (TSV) designs and models are proposed to achieve high-frequency vertical connectivity for three dimensional (3D) multi-core and heterogeneous ICs. Specifically, coaxial dielectric and novel air-gap-based TSVs are designed and simulated to reduce signal degradation during RF operations. The simulation results demonstrate that these RF TSVs can provide decay-tolerance frequencies two orders of magnitude higher than simple Cu-plug TSVs. The data rate and energy per bit of the RF TSVs are summarized, providing an important guideline for future 3D high-frequency TSV designs.\n",
      "Motivation: High-density SNP data of model animal resources provides opportunities for fine-resolution genetic variation studies. These genetic resources are generated through a variety of breeding schemes that involve multiple generations of matings derived from a set of founder animals. In this article, we investigate the problem of inferring the most probable ancestry of resulting genotypes, given a set of founder genotypes. Due to computational difficulty, existing methods either handle only small pedigree data or disregard the pedigree structure. However, large pedigrees of model animal resources often contain repetitive substructures that can be utilized in accelerating computation.#R##N##R##N#Results: We present an accurate and efficient method that can accept complex pedigrees with inbreeding in inferring genome ancestry. Inbreeding is a commonly used process in generating genetically diverse and reproducible animals. It is often carried out for many generations and can account for most of the computational complexity in real-world model animal pedigrees. Our method builds a hidden Markov model that derives the ancestry probabilities through inbreeding process without explicit modeling in every generation. The ancestry inference is accurate and fast, independent of the number of generations, for model animal resources such as the Collaborative Cross (CC). Experiments on both simulated and real CC data demonstrate that our method offers comparable accuracy to those methods that build an explicit model of the entire pedigree, but much better scalability with respect to the pedigree size.#R##N##R##N#Contact: weiwang@cs.unc.edu\n",
      "With technology migration into nano and molecular scales several hybrid CMOS/nano logic and memory architectures have been proposed thus far that aim to achieve high device density with low power consumption. The discovery of the memristor has further enabled the realization of denser nanoscale logic and memory systems. This work describes the design of such a multilevel memristor memory (MLMM) system, and the design constraints imposed in the realization of such a memory. In particular, the limitations on load, bank size, number of bits achievable per device, placed by the required noise margin (NM) for accurately reading the data stored in a device are analyzed.\n",
      "During the past decade, Rapidly-exploring Random Tree (RRT) and its variants are shown to be powerful sampling based single query path planning approaches for robots in high-dimensional configuration space. However, the performance of such tree-based planners that rely on uniform sampling strategy degrades significantly when narrow passages are contained in the configuration space. Given the assumption that computation resources should be allocated in proportion the geometric complexity of local region, we present a novel single-query Multi-RRTs path planning framework that employs an improved Bridge Test algorithm to identify global important roadmaps in narrow passages. Multiple trees can grown from these sampled roadmaps to explore sub-regions which are difficult to reach. The probability of selecting one particular tree for expansion and connection, which can dynamically updated by on-line learning algorithm based on the historic results of exploration, guides the tree through narrow passage rapidly. Experimental results show that the proposed approach gives substantial improvement in planning efficiency over a wide range of single-query path planning problems.\n",
      "This paper presents an efficient kinematical solution to a multi-robot system with serial and parallel mechanisms. JL-I is a reconfigurable robot featuring active spherical joints formed by serial and parallel mechanisms endowing the robotic system with the ability of changing shapes in three dimensions. The active joint here can combine the advantages of the high rigidity of a parallel mechanism and the extended workspace of a serial mechanism. However, the kinematic analysis of the serial and parallel mechanism is always the bottleneck in designing a robot and control realization. In order to deal with this problem, the whole kinematical analysis is organized in the sequence from the direct mechanical analysis related to the serial and parallel mechanism over the numerical solutions to the simplified kinematics expression. The latest results obtained demonstrate that the deduced closure-form solution is time efficient and easy to implement while offering a satisfactory motion performance in on-site experiments.\n",
      "When data sources are virtually integrated, there is no common and centralized method to maintain global consistency, so inconsistencies with regard to global integrity constraints are very likely to occur. In this paper, we consider the problem of defining and computing consistent query answers when queries are posed to virtual XML data integration systems, which are specified following the local-as-view approach. We propose a powerful XML constraint model to define global constraints, which can express keys and functional dependencies, and which also extends the newly introduced conditional functional dependencies to XML. We provide an approach to defining XML views, which supports not only edge-path mappings but also data-value bindings to express the join operator. We give formal definitions of repair and consistent query answers with the XML data integration settings. Given a query on the global system, we present a two-step method to compute consistent query answers. First, the given query is transformed using the global constraints, such that to run the transformed query on the original global system will generate exactly the consistent query answers. Because the global instance is not materialized, the query on the global instance is then rewritten in the form of queries on the underlying data sources by reversing rules in view definitions. We illustrate that the XPath query transformations can be implemented in XQuery. Finally, we implement prototypes of our method and evaluate our algorithms in the experiments.\n",
      "Purpose – The purpose of this paper is to describe the development of a robotic cleaning system for applying on the glass facade of the control tower at the Guangzhou Airport, in Guangzhou, China.Design/methodology/approach – Four similar robotic cleaning systems are designed for a reversed cone‐shape glass facade at the top of the control tower. One system is composed of a robot moving along and cleaning the facade, and an automatic conveyer positioning, securing, supplying energy and water to, and recycling the dirty water from the robot. An on‐board controller enables the system to work in a remote control mode or a fully automated mode under the supervision of an operator.Findings – This paper presents how to integrate the attaching, moving, cleaning and securing functions into one robotic system for the high rise glass facade, and focuses on the kinematics, the control and sensor system and the cleaning navigation. In particular, the real time control method of the vacuum in the cup is discussed to e...\n",
      "High performance using minimal resources has become a serious problem for digital signal processing (DSP) applications. The number of addressable registers is a significant obstacle for centralized architecture achieving high performance of DSP applications. In this paper, we propose a novel cluster based architecture synthesis algorithm, using minimal resources with time and register constraints, which adds a new cluster rather than inserts memory operations when registers are inadequate. By counting the register, inter-cluster communications and function units requirements during scheduling, the cluster with optimal performance is selected to schedule every instruction of the application. The redundant resources of the initial configuration obtained by our algorithm are further optimized. The experiments demonstrate that, compared with the centralized architecture synthesis, our approach achieves up to 224% improvement in success rate for general cases and up to 369% improvement for cases with tight constraints, and effectively reduces the resources usage.\n",
      "Cluster ensemble has recently become a hotspot in machine learning communities. The key problem in cluster ensemble is how to combine multiple clusterings to yield a final superior result. In this paper, an Improved Non-negative Matrix Factorization (INMF) algorithm is proposed. Firstly, K-Means algorithm is performed to partition the hypergraph’s adjacent matrix and get the indicator matrix, which is then provided to NMF as initial factor matrix. Secondly, NMF is performed to get the basis matrix and coefficient matrix. Finally, clustering result is obtained via the elements in coefficient matrix. Experiments on several real-world datasets show that: (a) INMF outperforms the NMF-based cluster ensemble algorithm; (b) INMF obtains better clustering results than other common cluster ensemble algorithms.\n",
      "Nonnegative matrix factorization provides a new sight into the observed signals and has been extensively applied in face recognition, text mining and spectral data analysis. Despite the success, it is inefficient for the large-scale data set, due to the notoriously slow convergence of the multiplicative updating method. In this paper, we try to solve the problem through the parallel computing technique. Considering the limitation of the shared memory platform, the parallel algorithms are implemented on the distributed memory platform with the message passing interface library. Moreover, we adopt the two-layer cascade factorization strategy to eliminate the network consumption. The parallel implementations are evaluated on a 16-node Beowulf cluster with two data sets in different scale. The experiments demonstrate that the proposed method is effective in both precision and efficiency.\n",
      "In this paper, we present a new heuristic that generates broadcast schemes in arbitrary networks. The heuristic gives optimal broadcast time for ring, tree and grid if the originator is on the corner. Extensive simulations show that our new heuristic outperforms the best known broadcast algorithms for two different network models representing Internet and ATM networks. It also allows to generate broadcast time of networks of bigger size because its time complexity, O(|E|), is lower compared to the complexities of the other algorithms. The last advantage of the heuristic is that every node is informed via a shortest path from the originator.\n",
      "Given a set of k-dimensional objects, the skyline query finds the objects that are not dominated by others. In practice, different users may be interested in different dimensions of the data, and issue queries on any subset of k dimensions in stream environments. This paper focuses on supporting concurrent and unpredictable subspace skyline queries over data streams. Simply to compute and store the skyline objects of every subspace in stream environments will incur expensive update cost. To balance the query cost and update cost, we only maintain the full space skyline in this paper. We first propose an efficient maintenance algorithm and several novel pruning techniques. Then, an efficient and scalable two-phase algorithm is proposed to process the skyline queries in different subspaces based on the full space skyline. Furthermore, we present the theoretical analyses and extensive experiments that demonstrate our method is both efficient and effective.\n",
      "Probabilistic Latent Semantic Analysis (PLSA) is one of the latent topic models and it has been successfully applied to visual recognition tasks. However, PLSA models have been learned mainly in batch learning, which can not handle data that arrives sequentially. In this paper, we propose a novel on-line learning algorithm for learning the parameters of PLSA. Our contributions are two-fold: (i) an on-line learning algorithm that learns the parameters of a PLSA model from incoming data; (ii) a codebook adaptation algorithm that can capture the full characteristics of all the features during the learning. Experimental results demonstrate that the proposed algorithm can handle sequentially arriving data that batch PLSA learning cannot cope with, and its performance is comparable with that of the batch PLSA learning on visual recognition.\n",
      "This brief presents a frequency estimation algorithm (FEA) for an all-digital phase-locked loop (ADPLL) instead of the traditional binary frequency-searching algorithm. Based on the proposed FEA and a new fast-lock scheme, a fast-lock engine is designed to improve the lock-in time of an ADPLL design with two referenced clock cycles. An implementation of the proposed ADPLL design is realized by utilizing United Microelectronics Corporation (UMC) 0.18-μm 1P6M CMOS technology with a core area of 300 × 250 μm 2 , consisting of an acceptable input reference clock ranging from 220 kHz to 8 MHz. The ADPLL design has a frequency range of 28-446 MHz with an 8.8-ps digitally controlled oscillator resolution. Moreover, the peak-to-peak jitter of the ADPLL achieves 70 ps, respectively.\n",
      "In this paper, we study the problem of constructing quality fault-tolerant Connected Dominating Sets (CDSs)in homogeneous wireless networks, which can be defined as minimum k-Connected m-Dominating Set ((k;m)-CDS) problem in Unit Disk Graphs (UDGs). We found that every existing approximation algorithm for this problem is incomplete for k >= 3 in a sense that it does not generate a feasible solution in some UDGs. Based on these observations, we propose a new polynomial time approximation algorithm for computing (3;m)-CDSs. We also show that our algorithm is correct and its approximation ratio is a constant.\n",
      "Betweenness centrality helps researcher to master the changes of the system from the overall perspective in software network. The existing betweenness centrality algorithm has high time complexity but low accuracy. Therefore, Layer First Searching (LFS) algorithm is proposed that is low in time complexity and high in accuracy. LFS algorithm searches the nodes with the shortest to the designated node, then travels all paths and calculates the nodes on the paths, at last get the times of each node being traveled which is betweenness centrality. The time complexity of LFS algorithm is O(V2).\n",
      "To search for the Design Patterns’ influence on the software, the paper abstracts the feature models of 9 kinds of classic exiting design patterns among the 23 kinds and describes the features with algorithm language. Meanwhile, searching for the specific structure features in the network, the paper designs 9 matching algorithms of the 9 kinds design patterns mentioned above to research on the structure of the design patterns in the software network. At last, the paper analyzes the evolving trends of the software scale and the application frequency of the 9 kinds of design patterns as the software evolves, and search for the rules how these design patterns are applied into 4 kinds of typical software.\n",
      "Probabilistic topic models were originally developed and utilized for document modeling and topic extraction in Information Retrieval. In this paper, we describe a new approach for automatic learning of terminological ontologies from text corpus based on such models. In our approach, topic models are used as efficient dimension reduction techniques, which are able to capture semantic relationships between word-topic and topic-document interpreted in terms of probability distributions. We propose two algorithms for learning terminological ontologies using the principle of topic relationship and exploiting information theory with the probabilistic topic models learned. Experiments with different model parameters were conducted and learned ontology statements were evaluated by the domain experts. We have also compared the results of our method with two existing concept hierarchy learning methods on the same data set. The study shows that our method outperforms other methods in terms of recall and precision measures. The precision level of the learned ontology is sufficient for it to be deployed for the purpose of browsing, navigation, and information search and retrieval in digital libraries.\n",
      "Spectrum aggregation has recently received much attention due to the quick increasing services. This technique makes it possible that multiple spectrum bands are ultilized by the same user to satisfy the large bandwidth demand of the service and achieve better performance. This paper provides an overview on spectrum aggregation. The important research progresses on spectrum aggregation for LTE-Advanced and dynamic spectrum access are briefly described respectively. Finally, some research challenges of protocols and algorithms are addressed for future studies.\n",
      "Protein classification can be performed by representing 3-D protein structures by graphs and then classifying the corresponding graphs. One effective way to classify such graphs is to use frequent subgraph patterns as features; however, the effectiveness of using subgraph patterns in graph classification is often hampered by the large search space of subgraph patterns. In this paper, the authors present two efficient discriminative subgraph mining algorithms: COM and GAIA. These algorithms directly search for discriminative subgraph patterns rather than frequent subgraph patterns which can be used to generate classification rules. Experimental results show that COM and GAIA can achieve high classification accuracy and runtime efficiency. Additionally, they find substructures that are very close to the proteinsâ€™ actual active sites.\n",
      "The generalization performance of a learned Bayesian network largely depends on the quality of the prior provided to the learning machine. Indeed, the prior distribution is designed to provide additive domain expert knowledge to the parameters in a Bayesian network which tolerate some variance around these initial counts. The learning task is combinatorial regulates on this initial counts by the data statistics. The use of a prior distribution becomes even more critical in case of scarce data.\n",
      "We consider the averaging method for stability of rapidly switching linear systems with disturbances. We show that the notions of strong and weak averages proposed, with partial strong average defined in this note, play an important role in the context of switched systems. Using these notions of average, we show that exponential input-to-state stability (ISS) of the strong and the partial strong average system with linear gain imply exponential ISS with linear gain of the actual system. Similarly, exponential ISS of the weak average guarantees an appropriate exponential derivative ISS (DISS) property for the actual system. Moreover, using the Lyapunov method, we show that linear ISS gains of the actual system and its average converge to each other as the switching rate is increased.\n",
      "In this paper, we aim to design adaptive output feedback controllers for a class of uncertain nonlinear multiple-input single-output (MISO) systems in the presence of stuck type actuator failures. The class of MISO system has a characteristic that the relative degrees with respect to each of the inputs are not necessarily the same. We first design a pre-filter for each input and then derive the state space representation of a newly constructed plant. The design of control variable will be provided by making certain modifications on traditional backstepping technique. It will be shown that with our proposed scheme, the effects due to actuator failures can be compensated and global boundedness of the closed-loop signals is still ensured. Further, the system output can asymptotically track a given reference signal.\n",
      "Accurate performance-degradation monitoring of nanometer MOSFET digital circuits is one of the most critical issues in adaptive design techniques for overcoming the performance degradation due to aging phenomena such as negative bias temperature instability (NBTI) and hot carrier injection (HCI). Therefore, this paper proposes new on-chip aging sensor circuits which deploy a threshold voltage detector for monitoring the performance degradation of an aged MOSFET. The new aging sensor circuits measure the threshold voltage difference between a NBTI/HCI stressed MOSFET device and a NBTI/HCI unstressed MOSFET device using an inverter chain and a phase comparator and digitalize the phase difference induced by the threshold voltage difference. The proposed sensor circuits achieve a direct correlation between the threshold voltage degradation and the phase difference (a phase difference resolution of 1 ns per 0.01 V threshold voltage shift). Also, the circuits are almost independent of temperature variation due to symmetrical circuit structures. A 45 nm CMOS technology and predictive NBTI/HCI models have been used to implement and evaluate the proposed circuits. The implemented layout size is 18.58 x 7.97 μm 2 ; the post-layout power consumption is 18.57 μW during NBTI/HCI stress mode and 30.86 μW during NBTI/HCI measurement mode on average.\n",
      "Hajek and Sasaki (1988) showed that, for continuous traffic and packet radio network, the selection of paths that minimize the maximum nodal degree generates schedules of minimum-length. This result suggests that minimization of the maximum nodal degree provides good (although not necessarily optimal) performance in slotted networks with fixed-length packets. We give a multispace search algorithm that interplays structural operations in conjunction with a local search algorithm for the minimization of the maximum nodal degree. Structural operations disturb the environment of forming local minima, which makes multispace search a very natural approach to the problem. Experimental results indicate that this method has improved local search in terms of the solution quality and its sensitivity to the initial random assignment.\n",
      "Abstract#R##N##R##N#Recently the fusion of positioning and wireless communication has gained many interests due to the merits of location information for future communication systems. Both positioning and wireless communication are highly dependent on the air channel. Current channel models are well suited for communication applications but less for positioning. Therefore, we investigate and compare the channel characteristics of different floors in favour of positioning based on an outdoor to indoor broadband channel sounder measurement campaign. Power delay profiles on both floors show similar structure caused by outdoor wave propagation, and angle of arrivals mostly arrive from the same direction. Differences can be seen in the RMS delay spread, power, NLoS error and coherence characteristics. Copyright © 2010 John Wiley & Sons, Ltd.\n",
      "With the introduction of mobile Ad hoc networks (MANETs), nodes are able to participate in a dynamic network which lacks an underlying infrastructure. Before two nodes agree to interact, they must trust that each will satisfy the security and privacy requirements of the other. In this paper, using the cognition inspired method from the brain informatics (BI), we present a novel approach to improving the search efficiency and scalability of MANETs by clustering nodes based on cognitive trust mechanism. The trust relationship is formed by evaluating the level of trust using Bayesian statistic analysis, and clusters can be formed and maintained autonomously by nodes with only partial knowledge. Simulation experiments show that each node can form and join proper clusters, which improve the interaction performance of the entire network. The essence of the underlying reason is analyzed through the theory of complex networks, revealing great scalability of this method.\n",
      "Solving elliptic equations with sharp-edged interfaces is a challenging problem for most existing methods, especially when the solution is highly oscillatory. Nonetheless, it has wide applications in engineering and science. An accurate and efficient method is desired. We propose an efficient non-traditional finite element method with non-body-fitting grids to solve the matrix coefficient elliptic equations with sharp-edged interfaces. Extensive numerical experiments show that this method is second order accurate in the L^~ norm and that it can handle both sharp-edged interface and oscillatory solutions.\n",
      "Near duplicate detection benefits many applications, e.g., on-line news selection over the Web by keyword search. The purpose of this demo is to show the design and implementation of MapDupReducer, a MapReduce based system capable of detecting near duplicates over massive datasets efficiently.\n",
      "An exciting application of genetic network is to predict phenotypic consequences for environmental cues or genetic perturbations. However, de novo prediction for quantitative phenotypes based on network topology is always a challenging task.\n",
      "Similarity joins between two sets of records return pairs of records whose similarity is no less than a given threshold. More specifically, consider two sets of records,  R  and  S , a similarity function  sim (.,.) and a threshold  t , a similarity join between  R  and  S  is defined as { ( r, s ) | ( r, s ) ∈  R x S, sim(r, s) ≥ t  }. A similarity join is a generalization of the traditional equality join commonly found in database systems. A variant of the similarity join is to use a distance threshold to replace the similarity threshold. It is generally expected that the similarity threshold is close to the maximum possible value (usually 1.0), and the distance threshold is close to the minimum possible value (usually 0). For example, we may find near-duplicate documents in a document repository using a cosine similarity threshold of 0.9, or we may find pairs of incorrectly spelt queries and their correct versions in a query log with an edit distance threshold of 2.\n",
      "The Affinity Propagation (AP) clustering algorithm proposed by Frey and Dueck (2007) provides an understandable, nearly optimal summary of a data set. However, it suffers two major shortcomings: i) the number of clusters is vague with the user-defined parameter called self-confidence, and ii) the quadratic computational complexity. When aiming at a given number of clusters due to prior knowledge, AP has to be launched many times until an appropriate setting of self-confidence is found. The re-launched AP increases the computational cost by one order of magnitude. In this paper, we propose an algorithm, called K-AP, to exploit the immediate results of K clusters by introducing a constraint in the process of message passing. Through theoretical analysis and experimental validation, K-AP was shown to be able to directly generate K clusters as user defined, with a negligible increase of computational cost compared to AP. In the meanwhile, KAP preserves the clustering quality as AP in terms of the distortion. K-AP is more effective than k-medoids w.r.t. the distortion minimization and higher clustering purity.\n",
      "Image inpainting technique has been widely used for reconstructing damaged old photographs and removing unwanted objects from images. In this paper, we introduce an energy function based on object removal inpainting technology. In our system, we use the energy function to measure the quality of the reconstructed region, so a better image completion result is obtained. Several examples and comparisons are given to demonstrate the performing of gradient-based image completion approach.\n",
      "Data mining is the discovery of knowledge and useful information from the large amounts of data stored in databases. With the increasing popularity of object-oriented database systems in advanced database applications, it is important to study the data mining methods for object-oriented databases because mining knowledge from such databases may improve understanding, organization, and utilization of the data stored there. In this paper, issues on generalization-based data mining in object-oriented databases are investigated in three aspects: (1) generalization of complex objects, (2) class-based generalization, and (3) extraction of different kinds of rules. An object cube model is proposed for class-based generalization, on-line analytical processing, and data mining. The study shows that (i) a set of sophisticated generalization operators can be constructed for generalization of complex data objects, (ii) a dimension-based class generalization mechanism can be developed for object cube construction, and (iii) sophisticated rule formation methods can be developed for extraction of different kinds of knowledge from data, including characteristic rules, discriminant rules, association rules, and classification rules. Furthermore, the application of such discovered knowledge may substantially enhance the power and flexibility of browsing databases, organizing databases and querying data and knowledge in object-oriented databases.\n",
      "In this paper, we develop a robust signal space separation (rSSS) algorithm for real-time magnetoencephalography (MEG) data processing. rSSS is based on the spatial signal space separation (SSS) method and it applies robust regression to automatically detect and remove bad MEG channels so that the results of SSS are not distorted. We extend the existing robust regression algorithm via three important new contributions: 1) a low-rank solver that efficiently performs matrix operations; 2) a subspace iteration scheme that selects bad MEG channels using low-order spherical harmonic functions; and 3) a parallel computing implementation that simultaneously runs multiple tasks to further speed up numerical computation. Our experimental results based on both simulation and measurement data demonstrate that rSSS offers superior accuracy over the traditional SSS algorithm, if the MEG data contain significant outliers. Taking advantage of the proposed fast algorithm, rSSS achieves more than 75× runtime speedup compared to a direct solver of robust regression. Even though rSSS is currently implemented with MATLAB, it already provides sufficient throughput for real-time applications.\n",
      "A malware detection model based on a negative selection algorithm with penalty factor (NSAPF) is proposed in this paper. This model extracts a malware instruction library (MIL), containing instructions that tend to appear in malware, through deep instruction analysis with respect to instruction frequency and file frequency. From the MIL, the proposed model creates a malware candidate signature library (MCSL) and a benign program malware-like signature library (BPMSL) by splitting programs orderly into various short bit strings. Depending on whether a signature matches “self”, the NSAPF further divides the MCSL into two malware detection signature libraries (MDSL1 and MDSL2), and uses these as a two-dimensional reference for detecting suspicious programs. The model classifies suspicious programs as malware and benign programs by matching values of the suspicious programs with MDSL1 and MDSL2. Introduction of a penalty factor C in the negative selection algorithm enables this model to overcome the drawback of traditional negative selection algorithms in defining the harmfulness of “self” and “nonself”, and focus on the harmfulness of the code, thus greatly improving the effectiveness of the model and also enabling the model to satisfy the different requirements of users in terms of true positive and false positive rates. Experimental results confirm that the proposed model achieves a better true positive rate on completely unknown malware and a better generalization ability while keeping a low false positive rate. The model can balance and adjust the true positive and false positive rates by adjusting the penalty factor C to achieve better performance.\n",
      "Discovering functional modules in a protein-protein interaction (PPI) network is very important for understanding the organization and function of the related biological system. The main strategy for this discovery is to translate the PPI network into a mathematical graph which can be analyzed with graph theory. In this paper, we propose a Density Based Merging Search (DBMS) algorithm to discover the complexes of a PPI graph corresponding to the functional modules in the PPI network. The DBMS algorithm starts from a single vertex with the highest density of connecting, then adds it's neighbor vertexes with the sufficiently high density of connecting one by one, and finally obtain one complex when there is no vertex to be added. The same DBMS procedure can be conducted on the rest of the vertexes in the PPI graph till all the complexes are found out. It is demonstrated by the experiments on six PPI datasets that the DBMS algorithm is efficient for discovering the complexes of a PPI network.\n",
      "In this letter, a power-control scheme for maximum sum-rate is proposed for the fading multiple access channels by considering the presence of primary users. Both the average transmit-power constraints and the peak interference-temperature constraints are considered. The interference caused by cognitive users must be under a pre-specified threshold for protecting primary users. The power-control optimization is considered as a novel geometrical problem which investigates the relationship of positions of a line and a few points. At most two users transmit simultaneously for optimality and the corresponding conditions are provided for both cases. Based on the analysis, the optimal power control is given for each specific fading state. For lowering computational complexity, the power-control optimization problem is divided into two categories according to different tight constraints. Simulation results are provided for the optimal power-control performance.\n",
      "With the increasing amount of text data stored in relational databases, there is a demand for RDBMS to support keyword queries over text data. As a search result is often assembled from multiple relational tables, traditional IR-style ranking and query evaluation methods cannot be applied directly. In this paper, we study the effectiveness and the efficiency issues of answering top-k keyword query in relational database systems. We propose a new ranking formula by adapting existing IR techniques based on a natural notion of virtual document. We also propose several efficient query processing methods for the new ranking method. We have conducted extensive experiments on large-scale real databases using two popular RDBMSs. The experimental results demonstrate significant improvement to the alternative approaches in terms of retrieval effectiveness and efficiency.\n",
      "In order to enable personalized natural interaction in service robots, artificial emotion is needed which helps robots to appear as individuals. In the emotion modeling theory of emotional Markov chain model (eMCM) for spontaneous transfer and emotional hidden Markov model (eHMM) for stimulated transfer, there are three problems: 1) Emotion distinguishing problem: whether adjusting parameters of the model have any effects on individual emotions; 2) How much effect the change makes; 3) The problem of different initial emotional states leading to different resultant emotions from a given stimuli. To solve these problems, a research method of individual emotional difference is proposed based on metric multidimensional scaling theory. Using a dissimilarity matrix, a scalar product matrix is calculated. Subsequently, an individual attribute reconstructing matrix can be obtained by principal component factor analysis. This can display individual emotion difference with low dimension. In addition, some mathematical proofs are carried out to explain experimental results. Synthesizing the results and proofs, corresponding conclusions are obtained. This new method provides guidance for the adjustment of parameters of emotion models in artificial emotion theory.\n",
      "In this paper, a novel 3D CMOS nanohybrid technology, 3D CMOL, is introduced to establish FPGA chips. By combining two leading technologies, hybrid CMOS/nanoelectronic circuit (CMOL) and 3D integration, 3D CMOL can provide a feasible and more efficient fabrication/assembly process than the existing 2D CMOL. Furthermore, 3D CMOL FPGA implements circuits in three dimensions so that it can increase the density of the nanodevices and achieve higher performance compared to 2D CMOL and field programmable nanowire interconnect (FPNI). This paper presents the architecture optimization, 3D integration, defect tolerance, and performance evaluation of 3D CMOL FPGA. It is expected that this technology can lead to technology breakthroughs towards the development of the next FPGA generation.\n",
      "Graph ranking algorithms have been successfully used in multi-document summarization. Among them, the basic link analysis model has drawn much attention due to its' mutual reinforcement principle which appears to be sound for the generic summarization task. In this paper, we explore effective strategies for extending the basic link analysis model to question-oriented multi-document summarization. Three kinds of strategies, namely link re-weighting, baseset downsizing and projection, are proposed to introduce question-dependent similarity metric, adjust the node number and refine the ranking process respectively. Experimental results evaluated on the DUC data sets demonstrate that these three strategies can achieve better results.\n",
      "It is well known that the pervasive IEEE 802.11 MAC is intrinsically unfair [1, 3]. In particular, in the topology shown in Fig. 1(a), when links AB and CD both carry backlogged transmissions, the packets from sender A experience persistent collisions at node B while sender C enjoys collision-free transmission to D. Node A can transmit successfully only if it is able to “insert” its packets into the small inter-packet gaps of C's packets. Thus, we refer to the topology in Fig. 1(a) as the unfair topology and to C and A as the superior and inferior nodes respectively.\n",
      "Along with the evolution of computer viruses, the number of file samples that need to be analyzed has constantly increased. An automatic and robust tool is needed to classify the file samples quickly and efficiently. Inspired by the human immune system, we developed a local concentration based virus detection method, which connects a certain number of two-element local concentration vectors as a feature vector. In contrast to the existing data mining techniques, the new method does not remember exact file content for virus detection, but uses a non-signature paradigm, such that it can detect some previously unknown viruses and overcome the techniques like obfuscation to bypass signatures. This model first extracts the viral tendency of each fragment and identifies a set of statical structural detectors, and then uses an information-theoretic preprocessing to remove redundancy in the detectors’ set to generate ‘self’ and ‘nonself’ detector libraries. Finally, ‘self’ and ‘nonself’ local concentrations are constructed by using the libraries, to form a vector with an array of two elements of local concentrations for detecting viruses efficiently. Several standard data mining classifiers, including K-nearest neighbor (KNN), radial basis function (RBF) neural networks, and support vector machine (SVM), are leveraged to classify the local concentration vector as the feature of a benign or malicious program and to verify the effectiveness and robustness of this approach. Experimental results show that the proposed approach not only has a much faster speed, but also gives around 98% of accuracy.\n",
      "Market-driven spectrum auctions offer an efficient way to improve spectrum utilization by transferring unused or under-used spectrum from its primary license holder to spectrum-deficient secondary users. Such a spectrum market exhibits strong locality in two aspects: 1) that spectrum is a local resource and can only be traded to users within the license area, and 2) that holders can partition the entire license areas and sell any pieces in the market. We design a spectrum double auction that incorporates such locality in spectrum markets, while keeping the auction economically robust and computationally efficient. Our designs in District are tailored to cases with and without knowledge of bid distributions. An auctioneer can start from one design without any a priori information, and then switch to the other alternative after accumulating sufficient distribution knowledge. Complementary simulation studies show that spectrum utilization can be significantly improved when distribution information is available.\n",
      "Web content quality assessment is a typical static ranking problem. Heuristic content and TFIDF features based statistical systems have proven effective for Web content quality assessment. But they are all language dependent features, which are not suitable for cross-language ranking. In this paper, we fuse a series of language-independent features including hostname features, domain registration features, two-layer hyperlink analysis features and third-party Web service features to assess the Web content quality. The experiments on ECML/PKDD 2010 Discovery Challenge cross-language datasets show that the assessment is effective.\n",
      "Record linkage identifies multiple records referring to the same entity even if they are not bit-wise identical. It is thus an essential technology for data integration and data cleansing. Existing record linkage approaches are mainly relying on similarity functions based on the surface forms of the records, and hence are not able to identify complex coreference records. This seriously limits the effectiveness of existing approaches.#R##N##R##N#In this work, we propose an automatic method to extract top-k high quality transformation rules given a set of possibly coreferent record pairs. We propose an effective algorithm that performs careful local analyses for each record pair and generates candidate rules; the algorithm finally chooses top-k rules based on a scoring function. We have conducted extensive experiments on real datasets, and our proposed algorithm has substantial advantage over the previous algorithm in both effectiveness and efficiency.\n",
      "Nowadays, the most promising technology for designing optical networks is the Wavelength Division Multiplexing (WDM). This technique divides the huge bandwidth of an optical fiber link into different wavelengths, providing different available channels per link. However, when it is necessary to interconnect a set of traffic demands, a problem comes up. This problem is known as Routing and Wavelength Assignment problem, and due to its complexity (NP-hard problem), it is very suitable for being solved by using evolutionary computation. The selected heuristic is the Artificial Bee Colony (ABC) algorithm, an heuristic based on the behavior of honey bee foraging for nectar. To solve the Static RWA problem, we have applied multiobjective optimization, and consequently, we have adapted the ABC to multiobjective context (MOABC). New results have been obtained, that significantly improve those published in previous researches.\n",
      "In molecular docking, it is challenging to develop a scoring function which is accurate to conduct high throughput screenings (HTS). Most scoring functions implemented in popular docking software packages were developed with many approximations for computational efficiency, which sacrifices the accuracy of prediction. With advanced technology and powerful computational hardware nowadays, it is feasible to use rigorous scoring functions, such as Molecular Mechanics/Poisson Boltzmann Surface Area (MM/PBSA) and Molecular Mechanics/Generalized Born Surface Area (MM/GBSA) in molecular docking studies. Here we systematically investigated the performance of MM/PBSA and MM/GBSA to identify the correct binding conformations and predict the binding free energies for 98 protein/ligand complexes. Comparison studies showed that MM/GBSA (69.4%) outperformed MM/PBSA (45.5%) and many popular scoring functions to identify the correct binding conformations. Moreover, we found that molecular dynamics (MD) simulations are necessary for some systems to identify the correct binding conformations. Based on our results, we proposed the guideline for MM/GBSA to predict the binding conformations. We then tested the performance of MM/GBSA and MM/PBSA to reproduce the binding free energies of the 98 protein-ligand complexes. The best prediction of MM/GBSA model with internal dielectric 2.0, produced a Spearman correlation coefficient of 0.66, which is better than MM/PBSA (0.49) and almost all scoring functions used in molecular docking. In summary, MM/GBSA performs well for both binding pose predictions and binding free energy estimations and is efficient to re-score the top-hit poses produced by other less accurate scoring functions.\n",
      "This paper’s main contributions are three-fold. Firstly, it is shown that the two existing template matching-like definitions of the Hough transform in the literature are inadequate. Secondly, an, inherent probabilistic aspect of the Hough transform embedded in the transformation process from image space to parameter space is clarified. Thirdly, a new definition of the Hough transform is proposed which takes into account both the intersection scheme between the mapping curve (or mapping surface) and accumulator cells and the inherent probabilistic characteristics.\n",
      "We present a new Stream OLAP framework to approximately answer queries on historical stream data, in which each cell is extended from a single value to a synopsis structure. The cell synopses can be constructed by the existing well researched methods, including Fourier, DCT, Wavelet, PLA, etc. To implement the Cube aggregation operation, we develop algorithms that aggregate multiple lower level synopses into a single higher level synopsis for those synopsis methods. Our experiments provide comparison among all used synopsis methods, and confirm that the synopsis cells can be accurately aggregated to a higher level.\n",
      "This paper proposes an affinity based complex artificial immune system considering the fact that the different eptitopes located on the surface of antigen can be recognized by a set of different paratopes expressed on the surface of immune cells. A neighborhood set consisting of immune cells with different affinities to a certain input antigen is built to simulate the nature immune behavior. Furthermore, the complex numbers are adopted as the data representation, besides the weight between different layers. In the simulations, the recognition on transformation patterns is performed to illustrate that the proposed system is capable of recognizing the transformation patterns and it has obviously higher noise tolerance ability than the previous system models.\n",
      "Selective regression testing involves retesting of software systems with a subset of the test suites to verify that modifications have not adversely impacted existing functions. Although this problem has been heavily researched, it has never been discussed in the context of SaaS (Software as a service). This paper presents the specific requirements, challenges and benefits in delivering regression test selection as a service (RTaaS). We will introduce how to design and implement a RTaaS platform. An implementation of RTaaS has been piloted and improved via several real projects in China market. The real customer cases illustrate that RTaaS is a cost-effective and easy way for software project teams to leap over technical barriers and tap into advanced regression testing selection technologies.\n",
      "When a multidatabase system contains textual database systems (i.e., information retrieval systems), queries against the global schema of the multidatabase system may contain a new type of joins-joins between attributes of textual type. Three algorithms for processing such a type of joins are presented and their I/O costs are analyzed in this paper. Since such a type of joins often involves document collections of very large size, it is very important to find efficient algorithms to process them. The three algorithms differ on whether the documents themselves or the inverted files on the documents are used to process the join. Our analysis and the simulation results indicate that the relative performance of these algorithms depends on the input document collections, system characteristics, and the input query. For each algorithm, the type of input document collections with which the algorithm is likely to perform well is identified. An integrated algorithm that automatically selects the best algorithm to use is also proposed.\n",
      "Recent studies have suggested using relative distance comparisons as constraints to represent domain knowledge. A natural extension to relative comparisons is the combination of two comparisons defined on the same set of three instances. Constraints in this form, termed Relative Constraints, provide a unified knowledge representation for both partitional and hierarchical clusterings. But many key properties of relative constraints remain unknown. In this paper, we answer the following important questions that enable the broader application of relative constraints in general clustering problems: \" Feasibility: Does there exist a clustering that satisfies a given set of relative constraints? (consistency of constraints) \"Completeness: Given a set of consistent relative constraints, how can one derive a complete clustering without running into dead-ends? \" Informativeness: How can one extract the most informative relative constraints from given knowledge sources? We show that any hierarchical domain knowledge can be easily represented by relative constraints. We further present a hierarchical algorithm that finds a clustering satisfying all given constraints in polynomial time. Experiments showed that our algorithm achieves significantly higher accuracy than the existing metric learning approach based on relative comparisons.\n",
      "Anatomy is a popular technique for privacy preserving in data publication. However, anatomy is fragile under background knowledge attack and can only be applied into limited applications. To overcome these drawbacks, we develop an improved version of anatomy: permutation anonymization, a new anonymization technique that is more effective than anatomy in privacy protection, and meanwhile is able to retain significantly more information in the microdata. We present the detail of the technique and build the underlying theory of the technique. Extensive experiments on real data are conducted, showing that our technique allows highly effective data analysis, while offering strong privacy guarantees.\n",
      "Human vision has the ability to recognize color under varying illumination conditions. Retinex theory is introduced to explain how the human visual system perceives color. The main aim of this paper is to present a total variation model for Retinex. Different from the existing methods, we consider and study two important elements which include illumination and reflection. We assume spatial smoothness of the illumination and piecewise continuity of the reflection, where the total variation term is employed in the model. The existence of the solution of the model is shown in the paper. We employ a fast computation method to solve the proposed minimization problem. Numerical examples are presented to illustrate the effectiveness of the proposed model.\n",
      "Given a graph G=(V,E) with node weight w:V?R +, the minimum weighted connected vertex cover problem (MWCVC) is to seek a subset of vertices of the graph with minimum total weight, such that for any edge of the graph, at least one endpoint of the edge is contained in the subset, and the subgraph induced by this subset is connected. In this paper, we study the problem on unit disk graph. A polynomial-time approximation scheme (PTAS) for MWCVC is presented under the condition that the graph is c-local.\n",
      "Abstract: We introduce the concept of fuzzy filters of pseudo-BL-algebras and obtain some of their important properties. We further give several kinds of fuzzy filters, such as fuzzy Boolean filter, fuzzy normal filter, fuzzy prime filter, fuzzy ultrafilter and fuzzy obstinate filter and discuss the relations among the above fuzzy filters. We establish the Prime Fuzzy Filter Theorem in pseudo BL-algebras. By studying the relation between fuzzy Boolean filter and fuzzy normal filter in pseudo-BL-algebras, we solve an open problem that whether or not every Boolean filter is normal in pseudo-BL-algebras.\n",
      "This paper proposes a joint relay selection and power allocation scheme for bidirectional asymmetric traffic to minimize the weighted energy consumed per bit transmitted. The scheme can be applied to both one-way and two-way Amplify-and-Forward (AF) relay networks. In the scheme, close-form solutions for energy efficient power allocation are derived with perfect channel state information at each relay. Then the relay consuming the least energy is chosen by the sources. On the basis of the proposed scheme, we characterize the energy consumption for training and power allocation information exchange between each relay and source. Simulation results demonstrate that the proposed scheme can achieve high energy efficiency when proper number of relays is deployed. It is also shown the two-way relaying provides substantial energy saving gains compared with the one-way relaying. 1\n",
      "Guard channel (GC) is a basic scheme to ensure the Quality of Service (QoS) of handoff users, especially for different QoS requirements of multimedia traffics. However, in the cognitive radio (CR) systems, the available channels the second users (SU) can use changed dynamically due to the arrival of primary user (PU), leading to a challenge of the traditional guard channel scheme. In this paper, the guard channel schemes for second users in CR networks are studied, and a new dynamical guard channel scheme is proposed. In the proposed approach, a generally theory analysis model with Markov chain is deduced, and the performance of different guard channel schemes including dynamical guard channel (DGC), without guard channel (NGC) and fixed guard channel (FGC) are evaluated. The numerical results show that the DGC can maintain relatively better performance according different network situations.\n",
      "Low-logging frequency GPS probe data have become a major data source for large-scale freeway network traffic monitoring. A critical step in GPS data processing is map matching. However, traditional map-matching algorithms are developed for in-vehicle navigation with high-logging frequency GPS data, noting that high-logging frequencies can be 1 s, whereas low-logging frequencies can be a few minutes. Such algorithms map a new GPS positioning point instantaneously given its historical points and network topology. Using high-logging frequency data-based map-matching algorithms for low-logging frequency data can cause several problems. First, large mapping errors in previous GPS points can easily propagate to the current points. Second, one-point-a-time processing is not effective and not necessary for traffic monitoring. Multiple GPS points can be processed together to determine routes more effectively. In this article, the authors propose a map-matching framework for low-logging frequency GPS probe data. Th...\n",
      "Finding the most accessible locations has a number of applications. For example, a user may want to find an accommodation that is close to different amenities such as schools, supermarkets, and hospitals etc. In this paper, we study the problem of finding the most accessible locations among a set of possible sites. The task is converted to a top-k query that returns k points from a set of sites R with the best accessibilities. Two R-tree based algorithms are proposed to answer the query efficiently. Experimental results show that our proposed algorithms are several times faster than a baseline algorithm on large-scale real datasets under a wide range of parameter settings.\n",
      "This paper proposes a control method of trajectory planning and posture adjustment for quadruped robots' obstacle striding. The addressed mixed parabola method plans the travelling paths over obstacles in Cartesian coordinate system. The constraints on velocities, accelerations, and jerks at waypoints are employed to generate the time-efficient smooth cubic spline joint trajectories by nonlinear optimization technique. The planned trajectories maximize the compliance and flexibility of joint movements. To guarantee the static stability for the pitch-pitch type quadruped robots, a posture adjustment strategy based on the potential energy is investigated to regulate the trajectory of Center of Gravity (COG). The experimental results on quadruped robot FROG-I (Four-legged Robot for Optimal Gaits) prove the feasibility of the proposed approach of striding a board in natural environment.\n",
      "This article is concerned with the delay-dependent H∞-filtering problem for discrete-time switched systems with a state delay. By using the switched Lyapunov functional method and choosing a new Lyapunov-Krasovskii functional, and, furthermore, utilising the linearisation technique, sufficient conditions on the existence of a desired filter are formulated as strict linear matrix inequalities. Neither model transformation nor the bounding technique for cross-terms is involved. A numerical example is provided to illustrate the effectiveness of the proposed method.\n",
      "Information Extraction has recently been extended to new areas by loosening the constraints on the strict definition of the extracted information and allowing to design more open information extraction systems. In this new domain of unsupervised information extraction, we focus on the task of extracting and characterizing  a priori  unknown relations between a given set of entity types. One of the challenges of this task is to deal with the large amount of candidate relations when extracting them from a large corpus. We propose in this paper an approach for the filtering of such candidate relations based on heuristics and machine learning models. More precisely, we show that the best model for achieving this task is a Conditional Random Field model according to evaluations performed on a manually annotated corpus of about one thousand relations. We also tackle the problem of identifying semantically similar relations by clustering large sets of them. Such clustering is achieved by combining a classical clustering algorithm and a method for the efficient identification of highly similar relation pairs. Finally, we evaluate the impact of our filtering of relations on this semantic clustering with both internal measures and external measures. Results show that the filtering procedure doubles the recall of the clustering while keeping the same precision.\n",
      "Optical flow estimation is still an important task in computer vision with many interesting applications. However, the results obtained by most of the optical flow techniques are affected by motion discontinuities or illumination changes. In this paper, we introduce a brightness correction field combined with a gradient constancy constraint to reduce the sensibility to brightness changes between images to be estimated. The advantage of this brightness correction field is its simplicity in terms of computational complexity and implementation. By analyzing the deficiencies of the traditional total variation regularization term in weakly textured areas, we also adopt a structure-adaptive regularization based on the robust Huber norm to preserve motion discontinuities. Finally, the proposed energy functional is minimized by solving its corresponding Euler-Lagrange equation in a more effective multi-resolution scheme, which integrates the twice downsampling strategy with a support-weight median filter. Numerous experiments show that our method is more effective and produces more accurate results for optical flow estimation.\n",
      "The goal of this paper is to generalize the well-balanced approach for non-equilibrium flow studied by Wang et al. (2009) to a class of low dissipative high-order shock-capturing filter schemes and to explore more advantages of well-balanced schemes in reacting flows. More general 1D and 2D reacting flow models and new examples of shock turbulence interactions are provided to demonstrate the advantage of well-balanced schemes. The class of filter schemes developed by Yee et al. (1999) , Sjoegreen and Yee (2004) and Yee and Sjoegreen (2007) consist of two steps, a full time step of spatially high-order non-dissipative base scheme and an adaptive non-linear filter containing shock-capturing dissipation. A good property of the filter scheme is that the base scheme and the filter are stand-alone modules in designing. Therefore, the idea of designing a well-balanced filter scheme is straightforward, i.e. choosing a well-balanced base scheme with a well-balanced filter (both with high-order accuracy). A typical class of these schemes shown in this paper is the high-order central difference schemes/predictor-corrector (PC) schemes with a high-order well-balanced WENO filter. The new filter scheme with the well-balanced property will gather the features of both filter methods and well-balanced properties: it can preserve certainmore » steady-state solutions exactly; it is able to capture small perturbations, e.g. turbulence fluctuations; and it adaptively controls numerical dissipation. Thus it shows high accuracy, efficiency and stability in shock/turbulence interactions. Numerical examples containing 1D and 2D smooth problems, 1D stationary contact discontinuity problem and 1D turbulence/shock interactions are included to verify the improved accuracy, in addition to the well-balanced behavior.« less\n",
      "With the Web 2.0 paradigm, a huge volume of Web content is generated by users at online forums, wikis, blogs, and social networks, among others. These user-contributed contents include numerous user opinions regarding products, services, or political issues. Among these user opinions, certain comparison opinions exist, reflecting customer preferences. Mining comparison opinions is useful as these types of viewpoints can bring more business values than other types of opinion data. Manufacturers can better understand relative product strengths or weaknesses, and accordingly develop better products to meet consumer requirements. Meanwhile, consumers can make purchasing decisions that are more informed by comparing the various features of similar products. In this paper, a novel Support Vector Machine-based method is proposed to automatically identify comparison opinions, extract comparison relations, and display results with the comparison relation maps by mining the volume of consumer opinions posted on the Web. The proposed method is empirically evaluated based on consumer opinions crawled from the Web. The initial experimental results show that the performance of the proposed method is promising and this research opens the door to utilizing these comparison opinions for business intelligence.\n",
      "Image segmentation is important with applications to several problems in biology and medicine. While extensively researched, generally, current segmentation methods perform adequately in the applications for which they were designed, but often require extensive modifications or calibrations before being used in a different application. We describe an approach that, with few modifications, can be used in a variety of image segmentation problems. The approach is based on a supervised learning strategy that utilizes intensity neighborhoods to assign each pixel in a test image its correct class based on training data. We describe methods for modeling rotations and variations in scales as well as a subset selection for training the classifiers. We show that the performance of our approach in tissue segmentation tasks in magnetic resonance and histopathology microscopy images, as well as nuclei segmentation from fluorescence microscopy images, is similar to or better than several algorithms specifically designed for each of these applications.\n",
      "Many information systems development companies are facing the question on how to apply agile methods in information systems maintenance (ISM). Performing correction of software defects in ISM inevitably degenerates program structure. On the other hand, agile methods provide refactoring to improve program structure without changing its behavior. This paper builds an optimal control model to balance the tradeoff between defect correction and refactoring. We answer three questions. First, is that optimal to perform parallel defect correction and refactoring? Second, how to determine the iteration length for agile ISM if team wants to include refactoring in the iteration? Third, how long the iteration should be if team wants to improve program’s structure to a certain level at the end of the iteration? To our knowledge, this paper is the pioneer in understanding agile ISM policy analytically. Managerial implications of the results are also discussed in the paper.\n",
      "In this paper, we present a method of utilizing channel diversity to increase secrecy capacity in wireless communication. With the presence of channel diversity, an intended receiver can achieve a relatively high secrecy capacity even at low SNRs. We present a theoretical analysis on the outage probability at a normalized target secrecy capacity in Rayleigh fading environment. Our numerical results strongly support our conclusion that maximal ratio combining of channel diversity can enhance the security of the wireless communication system in normal operating scenarios.\n",
      "In this paper, we consider the problem of vision based stairway recognition. This is a fundamental task for the navigation of robot's autonomous stairway climbing, as well as stairway modeling in mapping and building reconstruction. A serious challenge of this task is the vibration of the robot and the poor illuminating conditions. Under the influence of these factors, blurred images can be produced. Therefore, a robust and efficient stairway detecting algorithm is essential. By using Gabor Filter and introducing a Fuzzy Fusion Phase-grouping (FFPG) method, the blurred stair edges in image are extracted efficiently. Then the offset parameters are estimated for the robot navigation and stairway localization. Experiment results prove the validity and effectiveness of the proposed approaches.\n",
      "We propose a new framework of joint spectrum allocation and power control to utilize open spectrum bands in cognitive radio networks (CRNs) by considering both interference temperature constraints and spectrum dynamics. We first address a simpler problem for the case of a single flow. A TDM-based power-control strategy is adopted to achieve maximum end-to-end throughput by choosing an appropriate multihop route and spectrum combination for each single flow. Then, the simpler solution is extended to the multiflow case in which interflow interference and cumulative interference temperature must be considered. Considering the overhead of switching route and spectrum, the optimal waiting time before making a switch is derived. Our in-depth simulation study has shown that the proposed algorithms utilize spectrum more efficiently than other existing algorithms.\n",
      "In this paper, we introduce a new way of certifying assembly programs. Unlike previous program logics, we extract the control-flow information from the code and generate an intermediate trail between the specification and the real code. Trails are auxiliary specifications and treated as modules in the certification process. We define a simple modular program logic called trail-based certified assembly programming (TCAP) to certify and link different parts of a program using the corresponding trails. Because the control flow information in trails is explicit, the rules are easier to design. We show that our logic is powerful enough to prove partial correctness of assembly programs with features including stack-based abstractions and self-modifying code.We also provide a semantics for TCAP and prove that the logic is sound with respect to the semantics.\n",
      "Image-based morphometry of cells, tissues, and organs is an important topic in biomedical image analysis. We propose a novel method to characterize the morphological information that discriminates between two populations of morphological exemplars (cells, organs). We first demonstrate that the application of standard techniques such as Fisher linear discriminant analysis (FDA) can lead to undesirable errors in characterizing such information. We then describe an adaptation of the FDA technique that utilizes a least squares projection error to regularize the final solution. We show results comparing the FDA, modified FDA, and principal component analysis (PCA) techniques utilizing a contour-based characterization of both simulated and real images of cell nuclei.\n",
      "The data measured by well bottom sensors can be transmitted to the surface through the drilling mud during oil drilling operations. This article introduces a data processing scheme for a wireless data transmission application via mud. The detailed signal processing procedure is given, and several data processing techniques used are discussed, mainly including data encoding and signal integrating method, signal filtering, data storage and manage method, peak detection, signal recognition, and data decoding method. The article uses M pulses in N slots to encode the values of actual parameters. A two step filtering method and a dynamic data storing and managing method are proposed. A mix peak detection method is utilized to find the position of a pulse by combining threshold method and neighbor comparison method. These techniques have been successfully used in an oil well drilling operation.\n",
      "Genome-wide association study (GWAS) aims to find genetic factors underlying complex phenotypic traits, for which epistasis or gene-gene interaction detection is often preferred over single-locus approach. However, the computational burden has been a major hurdle to apply epistasis test in the genome-wide scale due to a large number of single nucleotide polymorphism (SNP) pairs to be tested.\n",
      "Capacities of online services are mainly determined by the interactions between workload and the services of the application. As the complexity of IT infrastructure increases, it is quite difficult to match the capacities of various services without the knowledge of their behaviors. The challenge to the existing works is to keep the performance model consistent with the services under live workload, because the workload and application behaviors are varied greatly. Therefore, new methods and modeling techniques that explain large-system behaviors and help analyze their future performance are now needed to effectively handle the emerging performance issues. In this paper, we proposed an automatic approach to build and rebuild performance model according to services' history statuses. Based on these statuses, user behaviors and their corresponding internal service relations are both modeled, and the CPU time consumed by each service is also got through Kalman filter. The analyzed results of our model can explain the behaviors of both the whole system and the individual services, and give valuable information for capacity planning. At last, our work is evaluated with TPC-W bench mark, whose results can demonstrate the effectiveness of our approach.\n",
      "Middleware sharing is one of the important resource sharing approaches which enables sharing of costs across a large pool of users. However, the shared Java middleware server easily causes interference on performance between concurrent user requests. A key requirement to an effective performance isolation is the knowledge of the resource consumption of the various kinds of use requests classified according to different application context information. Direct measurement of resource consumption requires instrumentation which is impractical. In this paper, we demonstrate that CPU consumptions of various kinds of user requests on a given hardware can be approximated by a proposed Kalman filter based approach. Experimental results derived from testing the approach by using the TPC-W e-commerce suite deployed on a widely-used Java middleware server (Tomcat) illustrate the potential of this approach.\n",
      "In a cloud computing environment, a great amount of services require to be deployed into the cloud, which may lead to the redundancy and complexity of cloud-services, and ever-increasing resource consumption. Current researches try to optimize the increasing number of services with the support of buffer pool and deployment optimization algorithms. Buffer pool here performs the crucial task of storing the deployment requirements for optimization. However, issues like when to enforce the flush and which stored objects should be flushed out still lack comprehensive consideration. To address these problems, we propose an improved collaborative dynamic double buffer pool in this paper. By classifying different deployment requirements and flush fashions, we select a threshold to determine when to enforce the flush. According to this flush time, we can select the to-be-optimized objects dynamically along with necessary collaborative interaction with the service owner. Meanwhile we present the algorithms of the buffer pool flush mechanism, and the improved service deployment optimization approach. In the end, we illustrate the effectiveness and feasibility of our approach through a demonstration which guarantees relatively high optimization efficiency.\n",
      "The blood circulation system in a human body provides a unique and natural trust zone for secure data communications in wireless healthcare systems such as body area networks. Unfortunately, biometric signal authentication using physiological attributes in wireless healthcare has not been extensively studied. In this paper, we propose a data authentication approach utilizing electrocardiography (ECG) signal patterns for reducing key exchange overhead. The major contribution of this research is to apply stochastic pattern recognition techniques in wireless healthcare. In the proposed approach, the inter-pulse interval (IPI) signal pattern at transmitter side is summarized as a biometric authentication key using Gaussian mixture model (GMM). At the receiver side, a light-weight signature verification scheme is adopted that uses IPI signals gathered locally at the receiver. The proposed authentication scheme has the advantage of high sample misalignment tolerance. In our earlier work, we had demonstrated the concept of stochastic authentication for ECG signal, but the signature verification process and GMM authentication performance under time synchronization and various sample points were not discussed. Here, we present a new set of analytical and experimental results to demonstrate that the proposed stochastic authentication approach achieves a low half total error rate in ECG signals verification.\n",
      "Optimal segmentation results can be obtained by synthetic aperture radar (SAR) image segmentation based on the Markov Random Field (MRF) model. However, MRF segmentation is very time-consuming because of employing the simulated annealing algorithm to optimize energy function. To speed up SAR image segmentation based on the MRF model, this paper investigates a fast segmentation approach for SAR imagery by fusing optical imagery. First, the optical image is applied to accelerate SAR image segmentation by selecting the uncertain pixels which only attend the SAR image segmentation and marking the certain pixels which don't attend the SAR image segmentation. Second, a fast annealing strategy is proposed to shorten the annealing time under low temperature. Finally, Computer simulation is given to validate the effectiveness of the proposed method.\n",
      "Kennedy has proposed the bare bones particle swarm (BBPS) by the elimination of the velocity formula and its replacement by the Gaussian sampling strategy without parameter tuning. However, a delicate balance between exploitation and exploration is the key to the success of an optimizer. This paper firstly analyzes the sampling distribution in BBPS, based on which we propose an adaptive BBPS inspired by the cloud model (ACM-BBPS). The cloud model adaptively produces a different standard deviation of the Gaussian sampling for each particle according to the evolutionary state in the swarm, which provides an adaptive balance between exploitation and exploration on different objective functions. Meanwhile, the diversity of the swarms is further enhanced by the randomness of the cloud model itself. Experimental results show that the proposed ACM-BBPS achieves faster convergence speed and more accurate solutions than five other contenders on twenty-five unimodal, basic multimodal, extended multimodal and hybrid composition benchmark functions. The diversity enhancement by the randomness in the cloud model itself is also illustrated.\n",
      "The analysis of large-scale networks requires the parallel techniques of graph processing. Hadoop as an open-source version of Map/Reduce implementation gains its popularity by high efficiency, scalability and fault tolerance. However, Map/Redeuce as a simplified programming model tends to be used in applications with massive datasets and simple processing. In this paper, we aim to adapt Map/Reduce programming in more complex applications such as community detection in large-scale networks. We present a new model, LI-MR (Local Iteration Map/Reduce), to resolve the Map/Reduce model's problems in respect of multi-iteration and random data access. A new system called LI-Hadoop is built to implement LI-MR model based on Hadoop. Furthermore, we propose a new algorithm MR-LPA, which parallelizes LPA in order to mine community structure in large-scale networks. We evaluate the performance of LI-Hadoop by executing MR-LPA on real-world datasets. The experimental results show that our approach is both effective and efficient.\n",
      "This article is concerned with the distributed H∞ filtering problem in sensor network, and a novel two-dimensional (2-D) system-based design approach is proposed. We aim to establish a two-step filter to simplify the design of the distributed filter parameters by means of a 2-D Roessor model, such that the filtering error dynamics is asymptotically stable and the prescribed average H∞ performance constraint is met. Consensus protocol is introduced as local information fusion strategy on nodes. In terms of certain linear matrix inequalities, sufficient conditions for the solvability of the addressed problem are obtained. Finally, a numerical example is provided to demonstrate the effectiveness and applicability of the proposed design approach.\n",
      "We present an intensity neighborhood-based system for segmenting arbitrary biomedical image datasets using supervised learning. Because neighborhood methods are often associated with high-dimensional feature vectors, we explore a Principal Component Analysis (PCA) based method to reduce the dimensionality (and provide computational savings) of each neighborhood. Our results show that the system can accurately segment data in three applications: tissue segmentation from brain MR data, and histopathological images, and nuclei segmentation from fluorescence images. Our results also show that the dimension reduction method we described improves computational efficiency while maintaining similar accuracy.\n",
      "User data stored in personal information systems is growing massively. Simultaneously, this data is increasingly distributed across multiple organizational domains such as email, music databases, and photo albums, some of which are structured automatically by applications. Powerful search tools are needed to help users locate data in these expanding yet fragmented data sets. In this paper, we present a novel fuzzy search approach that considers approximate matches to structure and content query conditions. Our framework uses unified data and query processing models so that structure conditions can be approximately matched by content and vice versa. Our models also unify external structure (e.g., directories) with internal structure (e.g., XML structure), supporting integrated queries matched to a single data domain. We propose indexes and algorithms for efficient query processing. We evaluate our approach using a real data set, showing that it can leverage structure information to significantly improve search accuracy, yet is robust to mistakes in query conditions.\n",
      "In this paper, we present a cooperative transmission approach to reduce the end-to-end delay in the context of AODV based multi hop wireless networks. The underneath idea is to effectively increase the average reach of each hop so that data packets can arrive at the destination in less number of hops with lower end-to-end delay. The existing approaches of increasing transmitted power are not effective while they increase the network interferences. Unlike them, we exploit the concept of cooperative beamforming to reduce the end-to-end delay by increasing the effective communication distances and reducing the communication interferences in wireless ad-hoc networks.\n",
      "Human saccade is a dynamic process of information pursuit. Based on the principle of information maximization, we propose a computational model to simulate human saccadic scanpaths on natural images. The model integrates three related factors as driven forces to guide eye movements sequentially — reference sensory responses, fovea-periphery resolution discrepancy, and visual working memory. For each eye movement, we compute three multi-band filter response maps as a coherent representation for the three factors. The three filter response maps are combined into multi-band residual filter response maps, on which we compute residual perceptual information (RPI) at each location. The RPI map is a dynamic saliency map varying along with eye movements. The next fixation is selected as the location with the maximal RPI value. On a natural image dataset, we compare the saccadic scanpaths generated by the proposed model and several other visual saliency-based models against human eye movement data. Experimental results demonstrate that the proposed model achieves the best prediction accuracy on both static fixation locations and dynamic scanpaths.\n",
      "This paper presents \"Craniux,\" an open-access, open-source software framework for brain-machine interface (BMI) research. Developed in LabVIEW, a high-level graphical programming environment, Craniux offers both out-of-the-box functionality and amodular BMI software framework that is easily extendable. Specifically, it allows researchers to take advantage of multiple features inherent to the Lab VIEW environment for on-the-fly data visualization, parallel processing, multithreading, and data saving. This paper introduces the basic features and system architecture of Craniux and describes the validation of the system under realtime BMI operation using simulated and real electrocorticographic (ECoG) signals. Our results indicate that Craniux is able to operate consistently in real time, enabling a seamless work flow to achieve brain control of cursor movement. The Craniux software framework is made available to the scientific research community to provide a LabVIEW-based BMI software platform for future BMI research and development.\n",
      "To date, most research on software code cloning has concentrated on detection and analysis techniques and their evaluation, and most empirical studies of cloning have investigated cloning within single system versions. In this paper, we present the results of a longitudinal study of cloning among the SCSI drivers for the Linux operating system that spans 16 years of evolution. We have chosen the SCSI driver subsystem as a test subject as it is known that cloning has been embraced by these developers as a design practice: when a new SCSI card comes out that is similar to an old one, but different enough to warrant its own implementation, a new driver may be cloned from an existing one. We discuss the results of our qualitative and quantitative analyses, including how the layered architecture of the SCSI subsystem seems to have affected the use of cloning as a design tool, the likelihood of consistent and inconsistent change over time, and the predictive power of using cloning between two independent driver implementations to model the similarity between two target devices.\n",
      "First-class function pointers are common in low-level assembly languages. Higher-level features such as closures, virtual functions, and call-backs are all compiled down to assembly code with function pointers. Function pointers are, however, hard to reason about. Previous program logics for certifying assembly programs either do not support first-class function pointers, or follow Continuation-Passing-Style reasoning which does not provide the same partial correctness guarantee as in high-level languages. In this paper, we present a simple semantic model for certifying the partial correctness property of assembly programs with first-class function pointers. Our model does not require any complex domain-theoretical construction, instead, it is based on a novel step-indexed, direct-style operational semantics for our assembly language. From the model, we derive a new program logic named ISCAP (or Indexed SCAP). We use an example to demonstrate the power and simplicity of ISCAP. The semantic model, the ISCAP logic, and the soundness proofs have been implemented in the Coq proof assistant.\n",
      "Motivation: Favorable interaction between the regulatory subunit of the cAMP-dependent protein kinase (PKA) and a peptide in A-kinase anchoring proteins (AKAPs) is critical for translocating PKA to the subcellular sites where the enzyme phosphorylates its substrates. It is very hard to identify AKAPs peptides binding to PKA due to the high sequence diversity of AKAPs.#R##N##R##N#Results: We propose a hierarchical and efficient approach, which combines molecular dynamics (MD) simulations, free energy calculations, virtual mutagenesis (VM) and bioinformatics analyses, to predict peptides binding to the PKA RIIα regulatory subunit in the human proteome systematically. Our approach successfully retrieved 15 out of 18 documented RIIα-binding peptides. Literature curation supported that many newly predicted peptides might be true AKAPs. Here, we present the first systematic search for AKAP peptides in the human proteome, which is useful to further experimental identification of AKAPs and functional analysis of their biological roles.#R##N##R##N#Contact:tingjunhou@hotmail.com; tjhou@suda.edu.cn; wei-wang@ucsd.edu#R##N##R##N#Supplementary information: Supplementary data are available at Bioinformatics online.\n",
      "Granular topic extraction and modeling are fundament tasks in text analysis. Hierarchical topic clustering algorithms and hierarchical topic models are usually employed for these purposes. However, it is difficult to make a clear distinguish between each pair of hierarchical topics from the semantic granularity point of view. STG (semantic topic granularity) is proposed to indicate the details degree of topic description, and aim at providing discrimination for topics from semantic aspect. A new model, mgMTM (multi-grain mixture topic model) based on STG is then proposed to model grain topics. DCT (discrete cosine transform) is employed to provide a mechanism for computing STG, extracting grain topics and learning mgMTM. Experiments on real world datasets show that the proposed model has lower perplexity score than that of LDA model and thus has better generalization performance in describing text. Experiments also show that the description of the extracted grain topics can be well explained with respect to a dataset including topics about recent global financial crisis.\n",
      "Generation of induced pluripotent stem cells (iPSCs) and converting one cell type to another (transdifferentiation) by manipulating the expression of a small number of genes highlight the progress of cellular reprogramming, which holds great promise for regenerative medicine. A key challenge is to find the recipes of perturbing genes to achieve successful reprogramming such that the reprogrammed cells function in the same way as the natural cells.\n",
      "There is a strong need to explore green and harvestable energy in computer communications. However, adapting wireless network performance to harvested energy has largely been ignored in literature. In this paper, we propose a new resource allocation scheme to improve data delivery quality in energy harvesting enabled wireless networks. In the proposed approach, packet Automatic Repeat reQuest (ARQ) limit of each wireless node is adaptively adjusted according to harvested energy. To achieve such optimal retry adaptation, energy neutrality constraint is considered in the overall optimization process. Simulation results show that the proposed retry adaptation approach significantly improves packet delivery ratio by exploring the harvested energy.\n",
      "The purpose of this paper is to develop a general self-organized approach to multi-robot's collaborative handling problem. Firstly, an autonomous motion planning graph (AMP-graph) is described for individual movement representations. An individual autonomous motion rule (IAM-rule) based on \"free-loose\" and \"well-distributed load-bearing\" preferences is presented. By establishing the simple and effective individual rule model, an ideal handling formation can be formed by each robot moving autonomously under their respective preferences. Finally, the simulations show that both the AMP-graph and the IAMrule are valid and feasible. On this basis, the self-organized approach to collaborative hunting and handling with obstacle avoidance of multi-robot systems can be further analyzed effectively.\n",
      "We investigate stability of a class of singularly perturbed systems whose slow system is a set-valued average defined via an appropriate averaging procedure of the solutions of the continuous-time boundary layer system. An approximate hybrid system consisting of this average, the projection of the jump map in the direction of the slow states and flow and jump sets from the original dynamics is shown to approximate the actual singularly perturbed hybrid system. In particular, using forward pre-completeness of the average system we show that solutions of the actual and approximate systems are close in an appropriate sense on compact time intervals. It is also shown that global asymptotic stability of the average system implies semi-global practical asymptotic stability of the actual system. Several examples are presented to illustrate our results and relate them to previously published results in the literature.\n",
      "In this paper we address the turning maneuver technique for a quadruped robot with only the hip pitch and the knee pitch for each leg, and explore characteristics of the mediolateral reaction forces experienced by the robot legs during the stance phase. We propose a turning strategy based on the translation of the hip-knee joint space towards the deepest flexion point. Such a translation can cause the robot body inclination in the roll plane, and consequently, result in the changes of the mediolateral reaction forces. In particular, we investigate the mediolateral reaction forces during straight walking and turning. Simulation results reveal that the lateral and medial reaction forces during turning differ strongly from those during straight walking in terms of the frequency used and peak force magnitude. That is, the dominance of the lateral or medial reaction forces in frequency and magnitude contributes forces and torques requisite for the turning maneuver.\n",
      "Component concentration of sodium aluminate solution is an important quality index for alumina production. In this article, we propose a new on-line soft sensing strategy for measuring component concentration of sodium aluminate solution. With this method, on-line control can be realised in aluminate production plants. Several advance techniques are used, such as principal component analysis (PCA), neural modelling and the least square algorithm. Industry experiments are conducted in the alumina production process and the results show the effectiveness of this method.\n",
      "Discriminative subgraphs can be used to characterize complex graphs, construct graph classifiers and generate graph indices. The search space for discriminative subgraphs is usually prohibitively large. Most measurements of interestingness of discriminative subgraphs are neither monotonic nor antimonotonic with respect to subgraph frequencies. Therefore, branch-and-bound algorithms are unable to mine discriminative subgraphs efficiently. We discover that search history of discriminative subgraph mining is very useful in computing empirical upper-bounds of discrimination scores of subgraphs. We propose a novel discriminative subgraph mining method, LTS (Learning To Search), which begins with a greedy algorithm that first samples the search space through subgraph probing and then explores the search space in a branch and bound fashion leveraging the search history of these samples. Extensive experiments have been performed to analyze the gain in performance by taking into account search history and to demonstrate that LTS can significantly improve performance compared with the state-of-the-art discriminative subgraph mining algorithms.\n",
      "The experiment on the Railway Riding Quality System functions as measuring the accelerated vertical or horizontal amplitude and frequency resulting from vibration by means of the accelerate sensor, which constitutes a key testing on dynamic performance of railway vehicles. Based on the Wireless Sensor Network (WSN), this article puts forward a new method to test the stability of railway performance — the TRQTS(Train Ride Quality Testing System). The TRQTS consists of the two modules respectively as the Data Collection Platform and the Data Analysis & Processing. The former module automatically collects data concerning railway performance without any manual intervention, while the latter module involves data segmentation, signal filtering, Fast Fourier Transformation (FFT), spectrum analysis. The experiment that has been operated on the railway vehicles indicates that the TRQTS system possesses higher reliability.\n",
      "The performance of multi-service applications are known to be determined mainly by the interactions between workload and behaviors of the application. The change of workload can lead to dynamic service demands on system resources, and even cause dynamic bottleneck switches between services inside the application. In this paper, to profiling large-applications' behaviors, and help to locate the bottleneck and optimize their capacities, we focus on modeling their behavior according to the workload. Although this topic has been well studied at testing stage, building such a model under live workload remains a challenge, because the workload and application behaviors are time-varying. To tackle this problem, we propose an adaptive approach to build and rebuild performance model according to log files. Both the user behaviors and their corresponding internal service relations are modeled, and the CPU time consumed by each service is also obtained through Kalman filter, which can \"absorb\" some level of noise in real-world data. Our model can explain the behaviors of both the whole application and the individual services, and provide valuable information for capacity planning and bottleneck detection. At last, our work is evaluated with TPC-W bench mark, whose results can demonstrate the effectiveness of our approach.\n",
      "In this paper, we propose a new multiple-sink positioning problem in wireless sensor networks to best support real-time applications. We formally define this problem as the  k  -Sink Placement Problem ( k  -SPP) and prove that it is APX-complete. We show that an existing approximation algorithm for the well-known  k -center problem is a constant factor approximation of  k  -SPP. Furthermore, we introduce a new greedy algorithm for  k -SPP and prove its approximation ratio is very near to the best achievable, 2. Via simulations, we show our algorithm outperforms its competitor on average.\n",
      "In this paper, the uplink power control problem is modeled by considering both cooperative and noncooperative methods respectively to protect licensed users in cognitive radio networks. The cooperative power control optimization problem is modeled as a concave minimization problem. According to the properties of the power control optimization problem, an improved branch and bound algorithm is proposed. On the other hand, for the noncooperative power control case, a game theoretic model with the exponential pricing function is adopted to restrict the interference to the licensed users. Further, Nash equilibrium for the power control game is discussed. Finally, the performance of the proposed models is evaluated by computer simulation.\n",
      "It is both theoretically and practically important to investigate the problem of accommodating infinite number of actuator failures or faults in controlling uncertain systems. However, there is still no result available in developing adaptive controllers to address this problem. In this paper, a new adaptive failure/fault compensation control scheme is proposed for parametric strict feedback nonlinear systems. The techniques of nonlinear damping and parameter projection are employed in the design of controllers and parameter estimators, respectively. It is proved that the boundedness of all closed-loop signals can still be ensured in the case with infinite number of failures or faults, provided that the time interval between two successive changes of failure/fault pattern is bounded below by an arbitrary positive number. The performance of the tracking error in the mean square sense with respect to the frequency of failure/fault pattern changes is also established. Moreover, asymptotic tracking can be achieved when the total number of failures and faults is finite.\n",
      "Stream prediction based on episode rules of the form \"whenever a series of antecedent event types occurs, another series of consequent event types appears eventually\"has received intensive attention due to its broad applications such as reading sequence forecasting, stock trend analyzing, road traffic monitoring, and software fault preventing. Many previous works focus on the task of discovering a full set of episode rules or matching a single predefined episode rule, little emphasis has been attached to the systematic methodology of stream prediction. This paper fills the gap by constructing an efficient and effective episode predictor over an event stream which works on a three-step process of rule extracting, rule matching and result reporting. Aiming at this goal, we first propose an algorithm Extractor to extract all representative episode rules based on frequent closed episodes and their generators, then we introduce an approach Matcher to simultaneously match multiple episode rules by finding the latest minimal and non-overlapping occurrences of their antecedents, and finally we devise a strategy Reporter to report each prediction result containing a prediction interval and a series of event types. Experiments on both synthetic and real-world datasets demonstrate that our methods are efficient and effective in the stream environment.\n",
      "Real time human body motion estimation plays an important role in the perception for robotics nowadays, especially for the applications of human robot interaction and service robotics. In this paper, we propose a method for real-time 3D human body motion estimation based on 3-layer laser scans. All the useful scanned points, presenting the human body contour information, are subtracted from the learned background of the environment. For human contour feature extraction, in order to avoid the situations of unsuccessful segmentation, we propose a novel iterative template matching algorithm for clustering, where the templates of torso and hip sections are modeled with different radii. Robust distinct human motion features are extracted using maximum likelihood estimation and nearest neighbor clustering method. Subsequently, the positions of human joints in 3D space are retrieved by associating the extracted features with a pre-defined articulated model of human body. Finally we demonstrate our proposed methods through experiments, which show accurate human body motion tracking in real time.\n",
      "Explosive growth in Information Technology has enabled many innovative application areas such as large-scale outdoor vehicular networks for vehicle-to-vehicle communications. By providing time-sensitive and location-aware information, vehicular networks can contribute to a safer and more efficient driving experience. However, the performance of vehicular networks requires robust and real-time data communications and is impacted by high mobility, intermittent connectivity, and unreliability of the wireless channel. In this paper, a novel adaptive distributed cooperative medium access control (ADC-MAC) protocol is proposed in order to address the inherent problems in the IEEE 802.11 standard when employed in vehicular networks. ADC-MAC exploits spatial diversities to maximize the system throughput as well as the service range of vehicular networks. This is accomplished through adaptively selecting the most suitable helper and transmission mode for transmit/receive pairs among direct transmission (DT), cooperative relay (CR) transmission and two-hop relay (TR) transmission, in accordance with the channel quality and the positioning of relay nodes. Both our Markov Chain modeling based theoretical analysis and ns-2 simulation experiments show that our ADC-MAC protocol outperforms existing schemes under the same network scenarios and maximizes the achieved system throughput and service distance.\n",
      "With the rapid development of cloud computing, traditional TP applications are evloving into the Extreme Transaction Processing (XTP) applications which are characterized by exceptionally demanding performance, scalability, availability, security, manageability and dependability require requirements, elastic caching platforms (ECPs) are introduced to help meet these requirements. Three popular cache strategies for ECPs have been proposed, say replicated strategy, partitioned strategy and near strategy. According to our investigations, many ECPs support multiple cache strategies. In this paper, we evaluate the impact of the three cache strategies using the TPC-W benchmark. To the best of our knowledge, this paper is the first evaluation of distributed cache strategies for ECPs. The main contribution of this work is guidelines that could help system administrators decide effectively which cache strategy would perform better under different conditions. Our work shows that the selection of the best cache strategy is related with workload patterns, cluster size and the number of concurrent users. We also find that four important metrics (number of \"get\" operations, message throughput, get/put ratio, and cache hit rate) could be used to help characterize the current condition.\n",
      "In this paper, we developed a wavelet neural network (WNN) algorithm for electroencephalogram (EEG) artifact. The algorithm combines the universal approximation characteristics of neural networks and the time/frequency property of wavelet transform, where the neural network was trained on a simulated dataset with known ground truths. The contribution of this paper is two-fold. First, many EEG artifact removal algorithms, including regression based methods, require reference EOG signals, which are not always available. The WNN algorithm tries to learn the characteristics of EOG from training data and once trained, the algorithm does not need EOG recordings for artifact removal. Second, the proposed method is computationally efficient, making it a reliable real time algorithm. We compared the proposed algorithm to the independent component analysis (ICA) technique and an adaptive wavelet thresholding method on both simulated and real EEG datasets. Experimental results show that the WNN algorithm can remove EEG artifacts effectively without diminishing useful EEG information even for very noisy datasets.\n",
      "This paper considers integral input-to-state stability (iISS) for a class of hybrid time-delay systems. Discrete dynamics includes impulsive and switching signals, and continuous dynamics is not necessarily stable. Based on multiple Lyapunov-Krasovskii functionals, a dwell-time bound is explicitly given to guarantee iISS of the hybrid delayed system. Compared with existing results on related problems, the obtained stability criteria can be applied to a larger class of hybrid delayed systems. Moreover, the obtained dwell-time bound is less conservative than existing ones. At last, an example related to networked control systems (NCSs) is provided to illustrate the effectiveness of the proposed result.\n",
      "In this paper, we consider the problem of designing distributed adaptive consensus tracking controllers for multiple nonlinear systems with unknown parameters and external disturbances. The desired trajectory is time varying given by the state of a reference system, which is only available to a portion of the group of the systems. Besides, the dynamics of the reference state is bounded but unknown to all of the systems. The communication graph characterizing the interactions among the systems is assumed to have undirected, fixed and connected topology. By introducing distributed estimators for the bound of the reference dynamics, two control schemes are proposed to address the problem. In the first scheme, a sign function is employed and perfect consensus tracking can be achieved. In the second scheme, an alternative control law is developed and the chattering phenomenon caused by the sign function can be reduced. However, new challenge will be triggered which is to compensate for possible destabilizing effects of the coupling elements relating to local parameter estimation errors and the synchronization errors of the neighbors. The overall communication graph is firstly reduced to an undirected spanning tree with single system notified of the reference state. Based on this, new synchronization error for each subsystem is then defined as the weighted distance relative to only one of its neighbors. It is shown that all the synchronization errors will converge to a prescribed bound which can be made as small as desired in this case.\n",
      "With the introduction of Mobile Ad Hoc Networks (MANETs), nodes are able to participate in a dynamic network which lacks an underlying infrastructure. In this paper, we present a novel approach to improve the search efficiency and scalability of MANETs by clustering nodes based on trust mechanism. In our method, the trust relationship is formed by evaluating the level of trust using Bayesian statistic analysis, and clusters can be formed and maintained autonomously by nodes with only partial knowledge. Simulation results show that each node can form and join proper clusters based on their trust degree, and the cluster-based search algorithm with trust mechanism outperforms over those in current popular clustering models.\n",
      "The Five-Factor Model (FFM) of personality is widely used to predict cognitions, attitudes, and behaviors in management and psychological research. However, the FFM personality has seldom appeared in the information system (IS) research field. Devaraj, Easley, and Crant (2008) first introduced FFM into the context of IS acceptance. This study extends to the context of IS continuance. The proposed model is developed and empirically validated using data from a field survey in order to examine how individuals' personality traits influence their IS continuance intentions. The data were collected from a public university in China via an online survey. The findings support that user satisfaction and perceived usefulness are key to continuance intention of instant messaging use. The results also support that perceived enjoyment and perceived usefulness are positively associated with user satisfaction. Perceived enjoyment is the dominant variable affecting user satisfaction with technology use. Two personality tr...\n",
      "The performances of supervised learning techniques on image classification problems heavily rely on the quality of their training images. But the acquisition of high quality training images requires significant efforts from human annotators. In this paper, we propose a novel multi-label batch model active learning (MLBAL) approach that allows the learning algorithm to actively select a batch of informative example-label pairs from which it learns at each learning iteration, so as to learn accurate classifiers with less annotation efforts. Unlike existing methods, the proposed approach fines the active selection granularity from example to example-label pair, and takes into account the informative label correlations for active learning. And the empirical studies demonstrate its effectiveness.\n",
      "Serial episode mining is one of hot spots in temporal data mining with broad applications such as user-browsing behavior prediction, telecommunication alarm analysis, road traffic monitoring, and root cause diagnostics from faults log data in manufacturing. In this paper, as a step forward to analyzing patterns within an event sequence, we propose a novel algorithm FCEMiner (Frequent Closed Episodes Miner) for discovering all frequent closed episodes. To characterize the followed-by-closely relationships over event types well and avoid over-counting the support of long episodes, FCEMiner takes both minimal and non-overlapping occurrences of an episode into consideration. To perform iterative episode growth without generating any candidate, FCEMiner utilizes the depth-first search strategy with Apriori Property. To save the cost of post-processing on frequent episodes, FCEMiner checks the closures of some episodes during each valid episode growth. A set of performance studies on both synthetic and real-world datasets show that our algorithm is more efficient and effective.\n",
      "Given two sets of moving objects with nonzero extents, the continuous intersection join query reports every pair of intersecting objects, one from each of the two moving object sets, for every timestamp. This type of queries is important for a number of applications, e.g., in the multi-billion dollar computer game industry, massively multiplayer online games like World of Warcraft need to monitor the intersection among players' attack ranges and render players' interaction in real time. The computational cost of a straightforward algorithm or an algorithm adapted from another query type is prohibitive, and answering the query in real time poses a great challenge. Those algorithms compute the query answer for either too long or too short a time interval, which results in either a very large computation cost per answer update or too frequent answer updates, respectively. This observation motivates us to optimize the query processing in the time dimension. In this study, we achieve this optimization by introducing the new concept of time-constrained (TC) processing. Further, TC processing enables a set of effective improvement techniques on traditional intersection join algorithms. Finally, we provide a method to find the optimal value for an important parameter required in our technique, the maximum update interval. As a result, we achieve a highly optimized algorithm for processing continuous intersection join queries on moving objects. With a thorough experimental study, we show that our algorithm outperforms the best adapted existing solution by several orders of magnitude. We also validate the accuracy of our cost model and its effectiveness in optimizing the performance.\n",
      "With the shift to many-core chip multiprocessors (CMPs), a critical issue is how to effectively coordinate and manage the execution of applications and hardware resources to overcome performance, power consumption, and reliability challenges stemming from hardware and application variations inherent in this new computing environment. Effective resource and application management on CMPs requires consideration of user/application/hardware-specific requirements and dynamic adaption of management decisions based on the actual run-time environment. However, designing an algorithm to manage resources and applications that can dynamically adapt based on the run-time environment is difficult because most resource and application management and monitoring facilities are only available at the operating system level. This paper presents REEact, an infrastructure that provides the capability to specify user-level management policies with dynamic adaptation. REEact is a virtual execution environment that provides a framework and core services to quickly enable the design of custom management policies for dynamically managing resources and applications. To demonstrate the capabilities and usefulness of REEact, this paper describes three case studies--each illustrating the use of REEact to apply a specific dynamic management policy on a real CMP. Through these case studies, we demonstrate that REEact can effectively and efficiently implement policies to dynamically manage resources and adapt application execution.\n",
      "In this paper, we design a joint channel-network coding scheme based on rateless code for the three-stage two-way relay system, where two terminals send messages to each other through a relay between them. Each terminal takes one of the first two stages to encode its message using a Raptor Code and then broadcasts the result into the air, respectively. In the third stage, upon successfully decoding the corresponding messages, the relay node re-encodes them with the new Raptor Codes, and then XORs the outputs and broadcasts the result to both terminals. Together with the packets received directly in previous stages, each terminal then retrieves the desired message using an iterative decoder. Here, the degree profiles of the Raptor Codes used at each node are jointly optimized through solving a set of linear programming problems. Simulations show that, the system throughput achieved by the optimized degree profiles always outperforms the one with conventional degree profile optimized for binary erasure channel (BEC) and the conventional network coding scheme with rateless coding.\n",
      "Cross-language Web content quality assessment plays an important role in many Web content processing applications. In the previous research, natural language processing, heuristic content and term frequency-inverse document frequency features based statistical systems have proven effective for Web content quality assessment. However, these are language-dependent features, which are not suitable for cross-language ranking. This paper proposes a cross-language Web content quality assessment method. First multi-modal language-independent features are extracted. The extracting features include character features, domain registration features, two-layer hyperlink analysis features and third-party Web service features. All the extracted features are then fused. Based on the fused features, feature selection is carried out to get a new eigenspace. Finally cross-language Web content quality model on the eigenspace can be learned. The experiments on ECML/PKDD 2010 Discovery Challenge cross-language datasets demonstrate that every scale feature has discriminability; different modalities of features are complementary to each other; and the feature selection is effective for statistical learning based cross-language Web content quality assessment.\n",
      "An energy system is the one of most important parts of the steel industry, and its reasonable operation exhibits a critical impact on manufacturing cost, energy security, and natural environment. With respect to the operation optimization problem for coke oven gas, a two-phase data-driven based forecasting and optimized adjusting method is proposed, where a Gaussian process-based echo states network is established to predict the gas real-time flow and the gasholder level in the prediction phase. Then, using the predicted gas flow and gasholder level, we develop a certain heuristic to quantify the user's optimal gas adjustment. The proposed operation measure has been verified to be effective by experimenting with the real-world on-line energy data sets coming from Shanghai Baosteel Corporation, Ltd., China. At present, the scheduling software developed with the proposed model and ensuing algorithms have been applied to the production practice of Baosteel. The application effects indicate that the software system can largely improve the real-time prediction accuracy of the gas units and provide with the optimized gas balance direction for the energy optimization.\n",
      "Clustered DFT-S-OFDM has been accepted as the uplink multiple access scheme for LTE-Advanced. In this paper, a new transmit diversity with four transmit antennas is proposed for Clustered DFT-S-OFDM system. A modified Frequency Switched Transmit Diversity (FSTD) algorithm is proposed, where the multiple clusters can be divided into two groups by four antennas, thus the number of clusters on each antenna will be reduced. As a result, the PAPR performance of each antenna has been improved. Meanwhile, in order to obtain additional frequency diversity gain, the proposed scheme has introduced more channels by a symbol-based dynamic cluster allocation method, which makes one turbo coded stream transmitted multiple channels. Moreover, the Space-Time Block Coding (STBC) is employed for the two antennas in each group to achieve spatial diversity and coding gain. Simulation results show that the proposed scheme has good PAPR and Bit Block Error Rate (BLER) performance on frequency selective MIMO channels\n",
      "The Radio Signal Strength Indicator (RSSI)-based localization algorithm is an effective solution for passive object localization. However, the localization accuracy of the existing methods highly depends on the transceiver distance and deployment density. Generally speaking, to obtain higher accuracy, we need a denser sensor node deployment, which results in a higher deployment cost and more communication overheads. In this paper, we investigate this problem based on extensive measurements. According to the measurement results, we propose to localize objects using an RSSI distribution based localization (RDL) model to identify the object location by different RSSI distributions of the communicating links. Experimental results show that the RDL method can achieve higher localization accuracy with less sensor nodes.\n",
      "Multi-hop wireless networks have no fixed physical backbone infrastructure, so Connected Dominating Set (CDS) is frequently selected as virtual backbone to improve network efficiency. Interference is a big problem which prevents wireless networks from reaching efficiency; however, previous works construct CDS as virtual backbone without consideration of interference. In this paper, we consider the problem of how to construct Minimum Interference Connected Dominating Set (MICDS) in multi-channel multi-radio multi-hop wireless networks. Multi-channel communication helps to reduce interference and improve network performance. A two-phase algorithm named MIS-colouring is presented for MICDS problem, and theoretical analysis is also given. However, due to the characteristics of the problem itself, it is quite difficult to compute the approximation ratio for MIS-colouring. Simulation results show that through proper CDS structure selection and channel assignment, MIS-colouring algorithm can minimise the total interference of CDS as a virtual backbone for multi-channel multi-radio multi-hop wireless networks.\n",
      "Geometry-based optimal power control was proposed in [14] to transform the power-control problem to a new geometrical problem on the position relationship between a line and some points. This scheme provides a novel visual perspective and lowers the complexity of optimization. We generalize this scheme to a larger class of power-control optimization problems so as to maximize the network utility with multiple average and peak power constraints in wireless networks. To facilitate the handling of the geometrical model, we define a subset of geometrical models with specified characteristics, called a regular geometrical model, and derive the type of power-control problems eligible for the regular geometrical model. For such a type of problems, two strategies are proposed for the construction of the regular geometrical model. Utilizing geometrical properties, we propose a novel geometry-based optimization scheme for the general power-control problem. Its computational complexity is significantly lower than the conventional algorithms. We also provide a further discussion on irregular geometrical model cases. Finally, we provide two examples of deploying the proposed geometry-based power-control scheme.\n",
      "We study users' behavioral patterns in ephemeral social networks, which are temporarily built based on events such as conferences. From the data distribution and social theory perspectives, we found several interesting patterns. For example, the duration of two random persons staying at the same place and at the same time obeys a two-stage power-law distribution. We develop a framework to infer the likelihood of two users to meet together, and we apply the framework to two mobile social networks: UbiComp and Reality. The former is formed by researchers attending UbiComp 2011 and the latter is a network of students published by MIT. On both networks, we validate the proposed predictive framework, which significantly improve the accuracy for predicting geographic coincidence by comparing with two baseline methods.\n",
      "Cloud computing has become a scalable services consumption and delivery platform in the field of computer science. As more and more consumers delegate their tasks to cloud providers, service level agreements (SLAs) between consumers and providers emerge as a key aspect. Because of the dynamic nature of the cloud, continuous monitoring on quality-of-service attributes is necessary to enforce SLAs. In this paper, we propose a trust mechanism-based task scheduling model for cloud computing. Referring to the trust relationship models of social persons, trust relationship is built among computing nodes, and the trustworthiness of nodes is evaluated by utilizing the Bayesian cognitive method. Integrating the trustworthiness of nodes into a dynamic level scheduling algorithm, the trust dynamic level scheduling algorithm for cloud computing is proposed. Theoretical analysis and simulations prove that the proposed algorithm can efficiently meet the requirement of cloud computing workloads in trust, sacrificing fewer time costs, and assuring the execution of tasks in a secure way in cloud environment. Copyright © 2011 John Wiley & Sons, Ltd.#R##N##R##N#(A trust mechanism based task scheduling model for cloud computing is provided. Trust relationship is built among computing nodes by utilizing the Bayesian cognitive method. The trust dynamic level scheduling algorithm can efficiently meet the requirement of cloud computing workloads in trust and assuring the execution of tasks in a secure way.)\n",
      "Input-to-state stability (ISS) properties for a class of time-varying hybrid dynamical systems via averaging method are considered. Two definitions of averages, strong average and weak average, are used to approximate the time-varying hybrid systems with time-invariant hybrid systems. Closeness of solutions between the time-varying system and solutions of its weak or strong average on compact time domains is given under the assumption of forward completeness for the average system. We also show that ISS of the strong average implies semi-global practical (SGP)-ISS of the actual system. In a similar fashion, ISS of the weak average implies semi-global practical derivative ISS (SGP-DISS) of the actual system. Through a power converter example, we show that the main results can be used in a framework for a systematic design of hybrid feedbacks for pulse-width modulated control systems.\n",
      "In this paper, random switch antenna array (RSAA) is proposed to apply to the switch antenna array (SAA) frequency-modulated continuous-wave (FMCW) radar system. Firstly, the signal model and signal processing method of RSAA is analyzed and it shows that RSAA can successfully solve azimuth-velocity coupling problem. Then we suppose a method by using RSAA to reduce the switching frequency and sampling rate based on sparse signal representation with multiple measurement vectors (MMV). It is shown in simulations that the proposed algorithms yield better performance in terms of azimuth-velocity decoupling and can obtain high image accuracy with less observation data.\n",
      "Duplicate webpages can affect the user experience of search engine. This paper proposed webpage deletion algorithm based on hierarchical filtering according to the features of duplicate webpage. The webpage feature extraction is divided into three layers, which are paragraphs, sentences and words. The webpage features are formed by layer filtering redundant information. In the sentence layer paragraph sentences are extracted according to the sentence semantics, while in the word layer the sentences are denoised filtering based on statistics of the part of speech in them. This algorithm improves the noise immunity and the original coverage of the feature extraction. The experiments show that the proposed method can accurately filter out duplicate webpage.\n",
      "This paper considers deadline dependent pricing and its impact to datacenter net profit optimization. We formulate the problem by jointly maximizing total revenue as a function of individual job deadlines and minimizing electricity cost through job scheduling over different time and locations. These two complementary objectives-the maximization of revenue and the minimization of cost-are mutually-dependent due to the coupling of job completion time and scheduling decisions. Lever-aging a new approximation method for job completion time, we develop two low-complexity, distributed algorithms for the net profit optimization. Through numerical evaluations, we show the efficacy of the proposed algorithms as well as the net profit improvements.\n",
      "This article how the surrogate model method is widely used in structural reliability analysis to approximate complex limit state functions. Accurate results can only be obtained when the surrogate model for the limit state function is approximated sufficiently close to the failure region. This article develops a novel local approximation method for efficient structural reliability assessment. The adaptive Markov chain simulation is utilized to generate samples in the failure region (the \"region of most interest\"). The support vector regression technique is then used to obtain an explicit approximation of the original complex limit state function around the region of most interest. Four examples are given in the article to demonstrate the application and efficiencies of the proposed method.\n",
      "This paper presents a hybrid sensing system for mobile robot localization in large-scale indoor environments. The system operates in two sensing modes, either omni-directional vision or laser scanning, according to the environmental characteristics. For a structured corridor environment, the vision information is adopted to track the#R##N#robot pose with a predefined hybrid metric-topological map. For a semi-structured office room, the laser scanning mode is chosen to generate a sequence of relative pose transformations based on a scan matching algorithm. Kalman filters are deployed to smooth multiple scan matching results. The proposed hybrid sensing system can perform localization tasks on-the-fly, with the features of efficient map modelling and computational simplicity. Experimental results are provided to demonstrate the performance and effectiveness of the proposed techniques.\n",
      "Two dimensional contingency tables or co-occurrence matrices arise frequently in various important applications such as text analysis and web-log mining. As a fundamental research topic, co-clustering aims to generate a meaningful partition of the contingency table to reveal hidden relationships between rows and columns. Traditional co-clustering algorithms usually produce a predefined number of flat partition of both rows and columns, which do not reveal relationship among clusters. To address this limitation, hierarchical co-clustering algorithms have attracted a lot of research interests recently. Although successful in various applications, the existing hierarchial co-clustering algorithms are usually based on certain heuristics and do not have solid theoretical background.   In this paper, we present a new co-clustering algorithm with solid information theoretic background. It simultaneously constructs a hierarchical structure of both row and column clusters which retains sufficient mutual information between rows and columns of the contingency table. An efficient and effective greedy algorithm is developed which grows a co-cluster hierarchy by successively performing row-wise or column-wise splits that lead to the maximal mutual information gain. Extensive experiments on real datasets demonstrate that our algorithm can reveal essential relationships of row (and column) clusters and has better clustering precision than existing algorithms.\n",
      "In order to demonstrate the validity and the benefit of the closed-chain kinematics of four-link motional method for the gait of wall-climbing caterpillar robot, the mathematical model and the relation of kinematical parameters were built. The caterpillar robot can climb on the wall by coordinated rotation of one active joint and three passive joints. The mechanical property of the closed-chain kinematics of four-link method is analyzed. And the relation of the driving joint torque and joint angle in wall-climbing process is deduced based on coplanar arbitrary force system. The coordinated control of multiple joints and the basis for selecting driving joints were discussed for developing the wall-climbing caterpillar robot. To testify the availability of the closed-chain kinematics of four-link method, a prototype of wall-climbing caterpillar robot with three kinds of adhesion modules based on vibrating suction method is designed. A successful wall-climbing test confirms both the principles of the closed-chain kinematics of four-link method and the validity of the adhesion modules based on vibrating suction method. The results show that the basis for selecting driving joints was reasonable and that the adhesion module based on vibrating suction method can produce powerful adsorption force with small weight and volume to ensure the safety and reliability of wall-climbing.\n",
      "This paper presents Cram'er-Rao Lower Bounds (CRLBs) for hybrid distance estimation algorithms, i.e., for received signal strength (RSS) and time of arrival (TOA) as well as for RSS and time difference of arrival (TDOA) measurements. The CRLBs are first evaluated with a simple channel model that consists of additive white Gaussian noise (AWGN), path loss (PL), and shadow fading (SF) in contrast to previous works. Several different CRLBs are dereived as the SF can be treated as nuisance parameter. Further, we apply the CRLBs to the WINNER II channel model and to current communication systems. Results show that the hybrid schemes always perform equal or better than RSS, TOA or TDOA only schemes, that the fourth type CRLB is the tightest bound, and that the distance when RSS performs better than TOA or TDOA increases with decreasing bandwidth.\n",
      "Full coverage testing is commonly perceived as a mission impossible because software is more complex than ever and produces vast space to cover. This paper introduces a novel approach which uses ACSL formal specifications to define and reach test coverage, especially in the sense of data coverage. Based on this approach, we create a tool chain named FAST which can automatically generate test harness code and verify program's correctness, turning formal specification and static verification into coverage definition and dynamic testing. FAST ensures completeness of test coverage and result checking by leveraging the formal specifications. We have applied this methodology and tool chain to a real-world mission critical software project that requires high quality standard. Our practice shows using FAST detects extra code bugs that escape from other validation methods such as manually-written tests and random/fuzz tests. It also costs much less human efforts with higher bug detection rate and higher code and data coverage.\n",
      "Wireless networks using mobile relays are widely used by urban vehicles, where throughputs and transmission delays are critically performance metrics. Even though there are several wireless routing protocols, the proper data forwarding algorithm should be carefully selected based on their characteristic in various situations. This paper provides extensive NS-2 based performance evaluation of three essential wireless data forwarding algorithms used in popular Mobile Ad Hoc Networks (MANET): Ad hoc on Demand Distance Vector (AODV), Destination-Sequenced Distance Vector (DSDV), and Dynamic Source Routing (DSR). We investigate throughputs, packet drop rate, and transmission delay with various numbers of mobile nodes and realistic mobility models, and provide a useful reference guideline to determine the best wireless data forwarding protocol for vehicles in urban areas. According to our performance study, AODV is the most effective algorithm in terms of throughput and data transmission delay for mobile environments.\n",
      "Development of a universal freeway incident detection algorithm is a task that remains unfulfilled and many promising approaches have been recently explored. The partial least squares (PLS) method and artificial neural network (NN) were found in previous studies to yield superior incident detection performance. In this article, a hybrid model which combines PLS and NN is developed to detect automatically traffic incident. A real traffic data set collected from motorways A12 in the Netherlands is presented to illustrate such an approach. Data cleansing has been introduced to preprocess traffic data sets to improve the data quality in order to increase the veracity and reliability of incident model. The detection performance is evaluated by the common criteria including detection rate, false alarm rate, mean time to detection, classification rate and the area under the curve (AUC) of the receiver operating characteristic. Computational results indicate that the hybrid approach is capable of increasing detection performance comparing to PLS, and simplifying the NN structure for incident detection. The hybrid model is a promising alternative to the usual PLS or NN for incident detection.\n",
      "The steam system is one of the main energy systems in steel industry, and its operational scheduling plays a crucial role for energy utility and resources saving. For a reasonable resources operation, the accurate prediction of steam flow is required. Considering the large amount of production data in energy system, a data-driven based model is proposed to perform a time series prediction for steam flow, in which a Bayesian echo state network (ESN) is established. This method combines Bayesian theory with ESN to obtain optimal output weight via maximizing the posterior probability density of the weights to avoid over-fitting in the training process of sample data. To pursue optimized hyper-parameters in the proposed Bayesian ESN, the evidence framework based on sample data is further adopted in this work. Experimental results using the real production data from Shanghai Baosteel show the validity and practicality of the proposed data-driven based model in providing scientific decision guidance for the steam system.\n",
      "Let G\"1 and G\"2 be two graphs. The Kronecker productG\"1xG\"2 has vertex set V(G\"1xG\"2)=V(G\"1)xV(G\"2) and edge set E(G\"1xG\"2)={(u\"1,v\"1)(u\"2,v\"2):u\"1u\"2@?E(G\"1) and v\"1v\"2@?E(G\"2)}. A graph G is super connected, or simply super-@k, if every minimum separating set is the neighbors of a vertex of G, that is, every minimum separating set isolates a vertex. In this paper we show that if G is a graph with @k(G)=@d(G) and K\"n(n>=3) a complete graph on n vertices, except that G is a complete bipartite graph K\"m\",\"m (m>=1) and K\"n=K\"3, then GxK\"n is super-@k, where @k(G) and @d(G) are the connectivity and the minimum degree of G, respectively.\n",
      "Performance isolation is a key requirement for application-level multi-tenant sharing hosting environments. It requires knowledge of the resource consumption of the various tenants. It is of great importance not only to be aware of the resource consumption of a tenant's given kind of transaction mix, but also to be able to be aware of the resource consumption of a given transaction type. However, direct measurement of CPU resource consumption requires instrumentation and incurs overhead. Recently, regression analysis has been applied to indirectly approximate resource consumption, but challenges still remain for cases with non-determinism and multicollinearity. In this work, we adapts Kalman filter to estimate CPU consumptions from easily observed data. We also propose techniques to deal with the non-determinism and the multicollinearity issues. Experimental results show that estimation results are in agreement with the corresponding measurements with acceptable estimation errors, especially with appropriately tuned filter settings taken into account. Experiments also demonstrate the utility of the approach in avoiding performance interference and CPU overloading.\n",
      "Multiple and concurrent iterations during the design phase of aircraft structural parts significantly increase the lead-time and cost of new aircraft products' development. Effective communication, collaboration and coordination of designers and software tools used by them are key mechanisms to address this challenge. This article presents an agent-based collaborative design framework to facilitate the collaboration of feature-based aircraft structural parts design and analysis tools, including detailed design, machining feature recognition, feature-based manufacturability evaluation and cost estimation. A new kind of attribute adjacency graph called dynamic holistic attribute adjacency graph DHAAG is established for local feature recognition. A dynamic machining feature relation matrix DMFRM is created for incremental manufacturability analysis. Process plans are generated based on DMFRM for cost estimation, and the result will feedback to designer for improving design. A prototype system has been implemented and demonstrated through a case study in collaboration with a major aircraft manufacturer in China.\n",
      "Metric learning is a fundamental problem in computer vision. Different features and algorithms may tackle a problem from different angles, and thus often provide complementary information. In this paper, we propose a fusion algorithm which outputs enhanced metrics by combining multiple given metrics (similarity measures). Unlike traditional co-training style algorithms where multi-view features or multiple data subsets are used for classification or regression, we focus on fusing multiple given metrics through diffusion process in an unsupervised way. Our algorithm has its particular advantage when the input similarity matrices are the outputs from diverse algorithms. We provide both theoretical and empirical explanations to our method. Significant improvements over the state-of-the-art results have been observed on various benchmark datasets. For example, we have achieved 100% accuracy (no longer the bull's eye measure) on the MPEG-7 shape dataset. Our method has a wide range of applications in machine learning and computer vision.\n",
      "In this paper, the framework of MapReduce is explored for large-scale multimedia data mining. Firstly, a brief overview of MapReduce and Hadoop is presented to speed up large-scale multimedia data mining. Then, the high-level theory and low-level implementation for several key computer vision technologies involved in this work are introduced, such as 2D/3D interest point detection, clustering, bag of features, and so on. Experimental results on image classification, video event detection and near-duplicate video retrieval are carried out on a five-node Hadoop cluster to demonstrate the efficiency of the proposed MapReduce framework for large-scale multimedia data mining applications.\n",
      "Group buying is a business model where people with the same merchandise interests form a group and conduct the purchase together to achieve a discount. Third-party proxy websites negotiate with merchants for appealing deals and then provide them to end customers. We call it online group buying. Besides, there exists local group buying where the joiners, the initiator, and sometimes even the merchants are in the same local community. Such locality induces some interesting characteristics in group buying, which remain largely unexplored in the research community. This study attempts to reveal users' behaviors in group buying within the local context. We developed a mobile service called \"HappyGo\" that supports local group buying. We conducted a trial involving more than 300 users from a company office. From our findings, we believe that local group buying complements online group buying by creating a \"local\" economic circle while also providing users with social benefits.\n",
      "In this paper, the problem of non-fragile observer-based H ∞  control for discrete-time switched delay systems is investigated. Both data missing and time delays are taken into account in the links from sensors to observers and from controllers to actuators. Such problem is transformed into an H ∞  control problem for stochastic switched delay systems. Average dwell time (ADT) approach is used to obtain sufficient conditions on the solvability of such problems. An example is provided to show the effectiveness of the proposed method.\n",
      "Recently the notion of popularity and its generalizations have been investigated as a possible alternative approach to text only analysis to rank web pages in search engines (e.g. [Kle98, BP98, CDR + 98, CDDG + 98, BH98, HHMN99] among others). We have built a research prototype that incorporates many link analysis algorithms from the literature and also new algorithms to investigate the impact of the popularity on the ranking of the search engines [DGK + 99]. Our goal in the TREC8 competition was to investigate the quality of the results using the TREC data and in particular the large web track. Unfortunately we did not have the needed hardware in time to generate results for the large web track. We only participated in the Small Web Track (Text Only and Text and Link Analysis). However, our system was designed for large datasets and the quality of the TREC8 results are not representative of the system. More recently we have experimented with larger datasets and we have come to the conclusion that link analysis can signicantly increase the quality of the ranking of search engines, a conclusion that is shared by many others in the literature [BP98, PBMW98, Kle98, CDR + 98, CDDG + 98, CDG + 99]. We will report these new results in a future publication.\n",
      "A compressed sensing algorithm based on improved layered discrete cosine transform (DCT) was proposed, which only measured the high-pass coefficients of the other layers but preserving the top layer coefficients. For the reconstruction, high-pass coefficients could be recovered by the measurements. Then the image could be reconstructed by the inverse DCT transform. Simulation results demonstrated that the proposed algorithm improved the quality of the recovered image significantly. For the same compression ratio, the peak signal to noise ratio of the proposed algorithm is improved about 2∼4 dB.\n",
      "In an academic conference, it is difficult to find people that share similar research interests with you, and it is also a chore to add them into your personal online social network for later communication. Aiming at helping the conference attendees better organize their schedule and expand their social network, we designed and developed Find a Connect where we used location and encounters, together with the conference basic services, all through a web user interface, in order to get the homophily and physical interactions between users, and then to base these information for recommending new contacts to users. To demonstrate the usefulness of Find a Connect, we conducted a field trial at the UbiComp 2011 conference. Results show that users tended to consider historical physical encounter information to be the most important when they wanted to know someone or add other users as contacts, and that homophily works as a factor in users' decision to add contacts.\n",
      "The SMS4 is the first commercial block cipher published by Chinese government. It's a 32-round block cipher encrypted by 128-bit keys. By analyzing the changes of the difference between input and output pairs in each round, this paper presents a new impossible differential path of the 14- round SMS4. Using this path, a new method is submitted to crypt analyze an 18-round SMS4. The time complexity of the attack is 2^117:06 partial encryptions.\n",
      "3D map building and path planning serve as two essential tasks for mobile robot to work within a complex outdoor environment. An elevation map built from 3D laser points is utilised to extract terrain feature while ground surface character is acquired from visual information by matching characteristics vectors. By projecting the units of the elevation map into the image, the fusion of terrain feature and ground surface character is achieved using statistical method. Then the units which fuse multi-sensor information are evaluated and given traversable weights to extract constraints for autonomous path planning in outdoor scene. After clustering the units into several regions, a hierarchical path planning strategy uses A* and Probabilistic Roadmap Method (PRM) to plan region paths and unit paths, respectively. The PRM is improved by choosing the units with the largest traversable weights in the regions to ensure a uniform distribution instead of random distribution. Experiment results show the validity and practicability of the proposed approaches.\n",
      "When using echo state networks (ESNs) to establish a regression model for noisy nonlinear time series, only the output uncertainty was usually concerned in some literature. However, the unconsidered internal states uncertainty is actually important as well. In this study, an improved ESN model with noise addition is proposed, in which the additive noises describe the internal state uncertainty and the output uncertainty. In terms of the parameters determination of this prediction model, a nonlinear/linear dual estimation consisting of a nonlinear Kalman filter and a linear one is proposed to perform the supervised learning. For verifying the effectiveness of the proposed method, the noisy Mackey Glass time series and the generation flow of blast furnace gas (BFG) in steel industry practice are both employed. The experimental results demonstrate that the proposed method is effective and robust for noisy nonlinear time series prediction.\n",
      "Locality-preserving projection (LPP) is a promising manifold-based dimensionality reduction and linear feature extraction method for face recognition. However, there exist two main issues in traditional LPP algorithm. LPP does not utilize the class label information at the training stage and its performance will be affected for classification tasks. In addition, LPP often suffers from small sample size (3S) problem, which occurs when the dimension of input pattern space is greater than the number of training samples. Under this situation, LPP fails to work. To overcome these two limitations, this paper presents a novel supervised regularization LPP (SRLPP) approach based on a supervised graph and a new regularization strategy. It theoretically proves that regularization matrix approaches to the original one as the regularized parameter tends to zero. The proposed SRLPP method is subsequently applied to face recognition. The experiments are conducted on two publicly available face databases, namely ORL database and FERET database. Compared with some existing LDA-based and LPP-based linear feature extraction approaches, experimental results show that our SRLPP approach gives superior performance.\n",
      "We describe a generic method for segmenting microscopy images based on supervised statistical modeling. The idea is to utilize example input segmentations to learn a statistical model of the shape and texture of the structures to be segmented. Segmentation of a test image then can be performed by maximizing the normalized cross correlation between the model and neighborhoods in the test image, followed by a final adjustment that utilizes nonrigid registration. We demonstrate the application of the method in segmenting several types of microscopy images of cells and nuclei.\n",
      "Abstract The DBF* algorithm of sporadic task systems on multiprocessors uses the approximation of the exact demand bound function on uniprocessor as a criterion. The systems which are feasible under the partitioned paradigm are flagged as “infeasible” sometimes. In this paper, we present a novel efficient DBF(eDBF) partitioned scheduling algorithm. A criterion which tracks the demand bound function exactly as needed is used to avoid the incorrect judgment in determining whether a processor can accommodate an additional task in the new algorithm. We give the pseudo code of the new algorithm on least-number processors and fixed-number processors respectively. Then, we prove the correctness of, and evaluated the effectiveness of this new algorithm. The experimental results demonstrate that eDBF has better performance than DBF* and Density algorithms.\n",
      "The current article examines user satisfaction with instant messaging in building and maintaining social relationships with friends, family members, and others. The research model integrates motivation theory with media capacity theories to explain how the attributes of media capacity (e.g., social presence and media richness) and users' intrinsic and extrinsic motivations toward using instant messaging influence user satisfaction. Data were collected from a sample of 247 Chinese university students via an online survey. The results suggest that perceived enjoyment, perceived social presence, and perceived usefulness are key to user satisfaction. Perceived social presence and perceived media richness are positively associated with perceived enjoyment. It was also found that perceived enjoyment, perceived social presence, and perceived media richness have significant effects on perceived usefulness. Of interest, perceived enjoyment and perceived social presence have stronger effects on user satisfaction th...\n",
      "Decomposing a software system into smaller, more manageable clusters provides an insight for better comprehension of large systems for software engineers. However, invocation-awareness and dynamic view are two features which are not supported by existed software clustering visualization tools. In this paper, we presents a novel tool, named SCuV, to partition the   S  oftware into invocation-aware clusters,   C  l  u  ster them with nested containment & invocation hierarchy and   V  isualize the clustering result in granularity-adjustable way.\n",
      "In this paper the authors propose a new Region of Interest (ROI) based approach to improve wireless electronic healthcare service quality. The major contribution of this research is the adaptability of the proposed approach with regards to channel information. In the proposed approach, important bit sequence in healthcare data (e.g., Electrocardiagram - ECG) is identified as interested region and extracted for unequal treatment in error-prone wireless communication channels. Specifically, variable bit-plane coding is applied to ECG samples in ROI and non-ROI. The fine-grained bit-plane coding in ROI provides more information precision of healthcare data, while the coarse-grained bit-plane coding in non-ROI saves more communication channel bandwidth. Simulation results demonstrated the effectiveness of the proposed approach in reducing healthcare data errors with communication bandwidth constraint.\n",
      "In this paper, we investigate the TCP throughput performance enhancement for cognitive radio networks (CRNs) through lower-layer configurations. There is an interaction between TCP and the lower-layer operations. The TCP sending rate at the transport layer determines the packet arrival rate of the lower-layer buffer, and meanwhile it is influenced by the average round trip time and packet loss rate, which are determined by the lower-layer mechanisms and configurations. Therefore, an iteration process is employed to investigate the TCP throughput under given channel condition and lower-layer configurations. For each iteration, queueing analysis is done to derive the packet loss rate and average delay on wireless link, which are then used to calculated the TCP throughput. Through derivations and numerical evaluations, the impacts of lower-layer parameters, primary user (PU) activities and channel conditions on the TCP throughput are discussed. Moreover, the way how these factors influence the TCP throughput is tracked. By using the proposed analytical method, TCP throughput enhancement can be achieved through appropriately setting lower-layer configurations.\n",
      "Recognizing objectionable content draws more and more attention nowadays given the rapid proliferation of images and videos on the Internet. Although there are some investigations about violence video detection and pornographic information filtering, very few existing methods touch on the problem of violence detection in still images. However, given its potential use in violence webpage filtering, online public opinion monitoring and some other aspects, recognizing violence in still images is worth being deeply investigated. To this end, we first establish a new database containing 500 violence images and 1500 non-violence images. And we use the Bag-of-Words (BoW) model which is frequently adopted in image classification domain to discriminate violence images and non-violence images. The effectiveness of four different feature representations are tested within the BoW framework. Finally the baseline results for violence image detection on our newly built database are reported.\n",
      "Multiple-Instance learning (MIL), which relaxes training annotation granularity from instance level to instance collection (bag) level by applying bag concept, obtains increasing attentions from computer vision community. Due to its flexible annotation mechanism, MIL has been naturally utilized on a variety of computer vision problems. And numerous models have been proposed, each of which is ingeniously designed to catch certain characteristics of MIL. However different models only perform well on certain tasks, and further improvement can hardly be achieved.\n",
      "This paper considers the problem of computing the optimal trajectories of multiple mobile elements (e.g. robots, vehicles, etc.) to minimize data collection latency in wireless sensor networks (WSNs). By relying on slightly different assumption, we define two interesting problems, the k-traveling salesperson problem with neighborhood (k-TSPN) and the k-rooted path cover problem with neighborhood (k-PCPN). Since both problems are NP-hard, we propose constant factor approximation algorithms for them. Our simulation results indicate our algorithms outperform their alternatives.\n",
      "Social networking sites (SNS) such as Facebook and Twitter are becoming popular forms for finding, promoting and attending offline events and activities. Much work has looked into characterizing these social networks and their user behavior. With the emergence of event and activity-based applications such as Facebook Events, Linked In Events, Xinghui, Zaizher, and Douban, it is easier to connect with other people online and meet them offline. However, few have looked into analyzing these offline events and activities that are shared online. This paper seeks to gain insights into the user behavior around people attending offline events which are promoted online. By studying the events in Douban, we present results around the event properties, user behavior of participants and wishers to an event, and social influence to an event. We show that event distribution by participants and wishers follow the typical power-law distribution, most users attend or like short events that last several days or regular events that last less than 3 months, participants attend an event within one day after the publish time, and that there is an exponential relationship between follow probability and number of common events attended between two users and a linear relationship for common events interested in. These findings provide a better understanding on how SNS could affect user behavior in attending events, and provide guidelines on how to improve the design of event-based applications.\n",
      "A high-speed parallel residue-to-binary converter is proposed for the moduli set S/sup k/={2/sup m/-1, 2/sup 2(0)m/+1, 2/sup 2(l)m/+1, L, 2.\n",
      "The problem of H∞ filtering for continuous-time systems with pointwise time-varying delay is investigated in this paper. By applying an innovation analysis in Krein space, a necessary and sufficient condition for the existence of an H∞ filter is derived in two methods: One is the partial differential equation approach, the other is the reorganized innovation analysis approach. The former gives a solution to the proposed H∞ filtering problem in terms of the solution of a partial differential equation with boundary conditions. The later gives an analytical solution to the proposed H∞ filtering problem in terms of the solutions of Riccati and matrix differential equations.\n",
      "In this paper, an Alamouti based cooperative wireless networks for multimedia transmission is proposed. According to various requirements of video, image, voice or medical signal transmissions, the multiplexing-diversity balanced structure is provided for such comprehensive multimedia network. Simulation results of this Alamouti based cooperative network have been compared with Maximum Ratio Combining (MRC) method. Simulation results show the BER performance of proposed schedule outperforms several other spatial modulation methods.\n",
      "Inverted indexes are the fundamental index for information retrieval systems. Due to the correlation between terms, inverted lists in the index may have substantial overlap and hence redundancy. In this paper, we propose a new approach that reduces the size of inverted lists while retaining time-efficiency. Our solution is based on merging inverted lists that bear high overlap to each other and manage their content in the resulting condensed index. An efficient algorithm is designed to discover heavily-overlapped inverted lists and construct the condensed index for a given dataset. We demonstrate that our algorithm delivers considerable space saving while incurring little query performance overhead.\n",
      "Constraints are an important aspect of role-based access control (RBAC) and sometimes argued to be the principal motivation of RBAC. While role engineering is proposed to define an architectural structure of the organization’s security policies, none of the work has employed constraint mining in migrating a non-RBAC system to an RBAC system to our knowledge, thus providing the motivation for this work. In this paper, we first define a wide variety of constraints, which are the best-known ones to date, and then create a relationship between the conventional data mining technology and the constraints. We further propose an anti-association rule mining algorithm to generate the constraints. Experiments on performance study prove the superiority of the new algorithm.\n",
      "Wireless Sensor Networks (WSNs) can have high demands for real-time data transmission and processing, but this is often constrained by limited resources. Cloud Computing can act as the backend for WSNs to provide processing and storage on demand. This paper proposes a generic architecture to support the integration of sensors with the Cloud. It uses a lightweight component model and dynamic proxy-based approach to connect sensors to the Cloud. The feasibility of this approach is evaluated experimentally.\n",
      "In this paper, we focus on efficient keyword query processing for XML data based on the SLCA and ELCA semantics. We propose a novel form of inverted lists for keywords which include IDs of nodes that directly or indirectly contain a given keyword. We propose a family of efficient algorithms that are based on the set intersection operation for both semantics. We show that the problem of SLCA/ELCA computation becomes finding a set of nodes that appear in all involved inverted lists and satisfy certain conditions. We also propose several optimization techniques to further improve the query processing performance. We have conducted extensive experiments with many alternative methods. The results demonstrate that our proposed methods outperform previous methods by up to two orders of magnitude in many cases.\n",
      "Science is increasingly becoming more and more data-driven. The ability of a geographically distributed community of scientists to access and analyze large amounts of data has emerged as a significant requirement for furthering science. In data intensive computing environment with uncountable numeric nodes, resource is inevitably unreliable, which has a great effect on task execution and scheduling. Novel algorithms are needed to schedule the jobs on the trusty nodes to execute, assure the high speed of communication, reduce the jobs execution time, lower the ratio of failure execution, and improve the security of execution environment of important data. In this paper, a kind of trust mechanism-based task scheduling model was presented. Referring to the trust relationship models of social persons, trust relationship is built among computing nodes, and the trustworthiness of nodes is evaluated by utilizing the Bayesian cognitive method. Integrating the trustworthiness of nodes into a Dynamic Level Scheduling (DLS) algorithm, the Trust-Dynamic Level Scheduling (Trust-DLS) algorithm is proposed. Moreover, a benchmark is structured to span a range of data intensive computing characteristics for evaluation the proposed method. Theoretical analysis and simulations prove that the Trust-DLS algorithm can efficiently meet the requirement of data intensive workloads in trust, sacrificing fewer time costs, and assuring the execution of tasks in a security way in large-scale data intensive computing environment.\n",
      "The Internet of Things IoT has recently received considerable interest from both academia and industry that are working on technologies to develop the future Internet. It is a joint and complex discipline that requires synergetic efforts from several communities such as telecommunication industry, device manufacturers, semantic Web, and informatics and engineering. Much of the IoT initiative is supported by the capabilities of manufacturing low-cost and energy-efficient hardware for devices with communication capacities, the maturity of wireless sensor network technologies, and the interests in integrating the physical and cyber worlds. However, the heterogeneity of the \"Things\" makes interoperability among them a challenging problem, which prevents generic solutions from being adopted on a global scale. Furthermore, the volume, velocity and volatility of the IoT data impose significant challenges to existing information systems. Semantic technologies based on machine-interpretable representation formalism have shown promise for describing objects, sharing and integrating information, and inferring new knowledge together with other intelligent processing techniques. However, the dynamic and resource-constrained nature of the IoT requires special design considerations to be taken into account to effectively apply the semantic technologies on the real world data. In this article the authors review some of the recent developments on applying the semantic technologies to IoT.\n",
      "We analyze a class of singularly perturbed hybrid systems based on two auxiliary hybrid systems: the averaged system, which approximates the slow dynamics, and the boundary layer system, which approximates the fast dynamics. The average system is generated by averaging the solutions of the boundary layer system. The novelty of this work is that the boundary layer system is a hybrid system rather than a continuous-time system. This extends available results to cover new classes of hybrid systems. We illustrate how to apply our results through an example that is a power converter system under hybrid feedbacks implemented by pulse-width modulation (PWM).\n",
      "In this paper, a novel approach, namely Globality and Locality Preserving Projections (GLPP), is proposed in the study of dimensionality reduction. The method is designed to combine the ideas behind Locality Preserving (LP), Discriminating Power (DP) and Maximally Collapsing Metric Learning (MCML), resulting in a unified model. Several distinguished features are obtained from the integration design. First, the method is able to take into account both global and local information of the data set. We introduce a new formula for calculating the conditional probabilities, which can remove the locality distortions from MCML. Second, discrimination information is applied so that a projection matrix is formed which can collapse all data points of the same class closer together, while pushing points of different classes further away. Third, the proposed method guarantees a supervised convex algorithm, which is a critical feature in data processing. Furthermore on this concern, GLPP is mapped to a Graphics Processor Unit (GPU) architecture in the implementation to be appropriate for large scale data sets. Several numerical studies are conducted on a variety of data sets. The numerical results confirm that GLPP consistently outperforms most up-to-date methods, allowing high classification accuracy, good visualization and sharply decreased consuming time.\n",
      "Linz Donawitz converter gas (LDG) is one of the most important sources of fuel energy in steel industry, whose reasonable use plays a crucial role in energy saving and environment protection. In practice, online prediction of variation of gas holder level and gas demand by users is fundamental to gas utilization and scheduling activities. In this study, a least square support vector machine-based prediction model combined with the parallel strategies is proposed, in which parameter optimization is realized online by a parallel particle swarm optimization and a parallelized validation method, both being implemented with the use of a graphic processing unit. The experiments demonstrate that the online parameter optimization based model greatly improves the prediction quality compared to the version with the fixed modeling parameters. Furthermore, the parallelized strategies largely reduce the computational cost thus guaranteeing the real-time effectiveness of the practical application.\n",
      "The problem of load rebalancing is an important issue for cloud-based key-value stores. However, the new virtualization environment and the store's stateful feature make this classical issue more challenging. In this paper, we build a new load rebalancing framework for cloud-based key-value stores, namely ElastiCat. It can be used for auto reconfiguring the store system with minimal costs and no disruption to the availability of the service. To evaluate and minimize the rebalancing costs, we firstly build two interference-aware prediction models to predict the data migration time and performance impact for each action using statistical machine learning and then create a cost model to strike a right balance between them. A cost-aware rebalancing algorithm is designed to utilize the cost model and balance rate to create a rebalancing plan and guide the choice of possible rebalancing actions. To maintain the availability of storage service, we propose a lightweight piggy-back based data access protocol. Finally, we demonstrate the effectiveness of the framework as well as the cost model using YCSB.\n",
      "While the development of Mobile Worldwide Interoperability for Microware Access (WiMAX) devices is still an ongoing process, complete and accurate simulations become more important in order to study the performance of Mobile-WiMAX based broadband wireless access networks. To further improve network simulation models for Mobile WiMAX, we have theoretically modelled the hybrid automatic repeat request (HARQ) mechanism and evaluated its performance and accuracy. In this paper, we present the design and implementation methodology of the Mobile WiMAX HARQ simulation model in system-level network simulators in order to gain benefits in link quality provided by enabling HARQ. Our results show that HARQ achieves higher throughput even at low signal strength, which can be considered as a non-line-of-sight scenario.\n",
      "We present a novel answer summarization method for community Question Answering services (cQAs) to address the problem of \"incomplete answer\", i.e., the \"best answer\" of a complex multi-sentence question misses valuable information that is contained in other answers. In order to automatically generate a novel and non-redundant community answer summary, we segment the complex original multi-sentence question into several sub questions and then propose a general Conditional Random Field (CRF) based answer summary method with group L1 regularization. Various textual and non-textual QA features are explored. Specifically, we explore four different types of contextual factors, namely, the information novelty and non-redundancy modeling for local and non-local sentence interactions under question segmentation. To further unleash the potential of the abundant cQA features, we introduce the group L1 regularization for feature learning. Experimental results on a Yahoo! Answers dataset show that our proposed method significantly outperforms state-of-the-art methods on cQA summarization task.\n",
      "Telecommunication network infrastructures such as cables, satellites, and cellular towers, play an important role in maintaining the stability of society worldwide. The protection of these critical infrastructures and their supporting structure become highly challenged to both public and private organizations. The understanding of interdependency of these infrastructures is the essential step to protect these infrastructures. This paper presents a critical infrastructure detection model to discover the interdependency based on the model from social network and new telecommunication pathways, while this study focuses on social theory into computational constructs. The policy and procedure of protecting critical infrastructures are discussed, and computational results from the proposed model are presented.\n",
      "In order to save storage space of a pano-mapping table used in omni-image unwarping, a geometric symmetry method is proposed. First of all, this method partitions a 360° omni-image into eight 45° omni-image sectors. Then, we partition the pano-mapping table into eight regions accordingly, with each pano-mapping table region corresponding to exactly one omni-image sector. We analyze the geometric symmetry relationship among these omni-image sectors and pano-mapping table regions. We find that if we know the mapping data in any one pano-mapping table region, it is easy to calculate the mapping data of the other seven pano-mapping table regions. Thus, in the final step, we perform omni-image unwarping based on only one pano-mapping table region, which reduces pano-mapping table size by seven-eighths. Reducing the pano-mapping size is very useful for implementing omni-image unwarping in embedded systems. Experiments on TI DSP-based embedded systems indicate that the proposed method reduces seven-eighths of pano-mapping table size, and improves the unwarping speed by a factor of 2.74.\n",
      "To enhance network security, we propose a secret key generation and distribution method for multihop wireless OFDM networks. In this method, the inherent physical features of wireless channel signatures, including randomness and reciprocity, are exploited to generate secret keys. In addition, a network coding approach is introduced for key distribution between a pair of transmitter and receiver in a multi-hop network. Our proposed protocol constructs the secret key from multiple branches at the source and forwards partial key information through separate paths to the destination using a simple and robust network coding operation. Under our protocol, cooperative relay nodes cannot decipher the key due to the partial information available, which provides a strong encryption capability for multi-hop systems. Mathematical analysis and excessive simulations demonstrate the high effectiveness of our proposed scheme.\n",
      "The nodes in Ad Hoc networks compete channels when communicating, with the features of no center and self-organization. In traditional channel assignment strategy of MAC layer, each node does not consider the demands to channel resource of other nodes, which hinders improving the network performance. An algorithm of channel assignment based on complete and perfect information dynamic game theory is proposed, supposing all the nodes are rational and greedy. Each node selects channels dynamically by backward induction due to strategies of other nodes, thus lead to Nash equilibrium finally. Experiments show that the network throughput is improved and the packet loss rate is reduced by this method effectively.\n",
      "A class of singularly perturbed hybrid dynamical systems is analyzed. The fast states are restricted to a compact set a priori. The continuous-time boundary layer dynamics produce solutions that are assumed to generate a well-defined average vector field for the slow dynamics. This average, the projection of the jump map in the direction of the slow states, and flow and jump sets from the original dynamics define the reduced, or average, hybrid dynamical system. Assumptions about the average system lead to conclusions about the original, higher-dimensional system. For example, forward pre-completeness for the average system leads to a result on closeness of solutions between the original and average system on compact time domains. In addition, global asymptotic stability for the average system implies semiglobal, practical asymptotic stability for the original system. We give examples to illustrate the averaging concept and to relate it to classical singular perturbation results as well as to other singular perturbation results that have appeared recently for hybrid systems. We also use an example to show that our results can be used as an analysis tool to design hybrid feedbacks for continuous-time plants implemented by fast but continuous actuators.\n",
      "A novel shape descriptor, named as ratio histograms (R-histogram), is proposed to represent the relative attitude relationship between two independent shapes. For a pair of two shapes, the shapes are treated as the longitudinal segments parallel to the line connecting centroids of the two shapes, and the R-histogram is composed of the length ratios of collinear longitudinal segments. R-histogram is theoretically affine invariant due to collinear distance invariance of the affine transformation. In addition, as the computation of the length ratio weakens the noise contribution, R-histogram is robust to noise. Based on the R-histogram, the shape-matching algorithm includes two major phases: preprocessing and matching. The first phase, which can be processed off-line, is trying to obtain the R-histograms of all original shape pairs. In the second phase, for each transformed shape pair, its R-histogram is computed and the candidate matched shape pair with minimal R-histogram matching error is found. Subsequently, a voting strategy, which further improves the accuracy of shape matching, is adopted for the candidate corresponding shape pairs. Experimental results demonstrate that the proposed R-histogram is robust and efficient.\n",
      "As one database offloading strategy, elastic key-value stores are often introduced to speed up the application performance with dynamic scalability. Since the workload is varied, efficient data migration with minimal impact in service is critical for the issue of elasticity and scalability. However, due to the new virtualization technology, real-time and low-latency requirements, data migration within cloud-based key-value stores has to face new challenges: effects of VM interference, and the need to trade off between the two ingredients of migration cost, namely migration time and performance impact. To fulfill these challenges, in this paper we explore a new approach to optimize the data migration. Explicitly, we build two interference-aware models to predict the migration time and performance impact for each migration action using statistical machine learning, and then create a cost model to strike a balance between the two ingredients. Using the load rebalancing scenario as a case study, we have designed one cost-aware migration algorithm that utilizes the cost model to guide the choice of possible migration actions. Finally, we demonstrate the effectiveness of the approach using Yahoo! Cloud Serving Benchmark (YCSB).\n",
      "Motivation: We describe a statistical model to dissect the noise in transcriptional bursts in a developmental system.#R##N##R##N#Results: We assume that, at any given moment of time, each copy of a native gene inside a cell can exist in either a bursting (active) or non-bursting (inactive) state. The experimentally measured total noise in the transcriptional states of a gene in a population of cells can be mathematically dissected into two contributing components: internal and external. While internal noise quantifies the stochastic nature of transcriptional bursts, external noise is caused by cell-to-cell differences including fluctuations in activator concentration. We use our developed methods to analyze the Drosophila Bicoid (Bcd) morphogen gradient system. For its target gene hunchback (hb), the noise properties can be recapitulated by a simplified gene regulatory model in which Bcd acts as the only input, suggesting that the external noise in hb transcription is primarily derived from fluctuations in the Bcd activator input. However, such a simplified gene regulatory model is insufficient to predict the noise properties of another Bcd target gene, orthodenticle (otd), suggesting that otd transcription is sensitive to additional external fluctuations beyond those in Bcd. Our results show that analysis of the relationship between input and output noise can reveal important insights into how a morphogen gradient system works. Our study also advances the knowledge about transcription at a fundamental level.#R##N##R##N#Contact: jun.ma@cchmc.org#R##N##R##N#Supplementary information: Supplementary data are available at Bioinformatics online.\n",
      "Framework-based applications controlled by XML configuration files are quite popularly used in current commercial applications. However, most of these frameworks are complex or not well documented, which poses a great challenge for programmers to correctly utilize them. To overcome these difficulties, we propose a new tool to recommend XML configuration snippets automatically through mining tree patterns and pattern associations from the application repository with the aim of assisting the programmer to generate proper XML configurations during the production phase. In this demo, we showcase this tool by presenting the major techniques behind the tool and the typical usage scenarios of our tool.\n",
      "This paper is concerned with the problem of H\"~ output feedback control for networked control systems with packet dropouts in both sensor-to-controller and controller-to-actuator channels. Packet dropouts in these two links are modeled as two independent Markov chains, whose transition probability matrices are sparse so that it is easy to obtain. Moreover, late arrivals are considered in the model as well. Sufficient conditions for the solvability of design problems of H\"~ output feedback controllers are presented and are dependent on the upper bounds of the number of consecutive packet dropouts. The validity of the proposed approaches are illustrated by a numerical example.\n",
      "The Internet of Things (IoT) allows physical objects to be connected on the Internet. Objects in the IoT have identities, attributes and personalities in the virtual world. These objects are integrated together using intelligent interfaces. The IoT has a lot of challenges and issues that require further research before achieving a global scale. This paper presents a generic IoT architecture to modularize physical objects into the digital world. It demonstrates that the future IoT can be designed based on component-based communication and existing communication standards. To achieve integration both on a device and semantic level, physical objects and services can be virtualised as instantiated middleware components. By building ontologies, third-parties can also customize objects and services.\n",
      "Infrastructure-as-a-Service clouds offer diverse pricing options, including on-demand and reserved instances with various discounts to attract different cloud users. A practical problem facing cloud users is how to minimize their costs by choosing among different pricing options based on their own demands. In this paper, we propose a new cloud brokerage service that reserves a large pool of instances from cloud providers and serves users with price discounts. The broker optimally exploits both pricing benefits of long-term instance reservations and multiplexing gains. We propose dynamic strategies for the broker to make instance reservations with the objective of minimizing its service cost. These strategies leverage dynamic programming and approximate algorithms to rapidly handle large volumes of demand. Our extensive simulations driven by large-scale Google cluster-usage traces have shown that significant price discounts can be realized via the broker.\n",
      "Resource constrained secure deep space Inter-Planetary Multimedia Networks (IPMNs) gathering various planetary sensor information have high requirements for energy-efficient transmission, error-resilient multimedia coding and robust content authentication. However, the joint exploration of Intra-Inter video coding versatility in signal processing domain, Signature-Hash diversity in information security domain and Forward Channel Correction (FEC) channel coding application in network protocol domain has largely been ignored in literature. In this paper, we propose a new Source-Authentication-Protocol (SAP) framework to provide multimedia service quality, communication overhead efficiency and multimedia content integrity simultaneously. To provide robust video authentication while keeping bandwidth resource constraints, a novel SAP based network resource allocation scheme is proposed to improve energy efficiency and communication resource utilization in IPMN by jointly exploring the diversities in the source, the authentication and the protocol categories. Results based on simulation studies demonstrate the effectiveness of the proposed SAP scheme in achieving resource efficiency, video quality and authentication robustness, simultaneously.\n",
      "Many calssifiers which are constructed with chosen gene markers have been proposed to forecast the prognosis of patients who suffer from breast cancer. However, few of them has been applied in clinical practice because of the bad generalization, which results from the situation that markers selected by one method are very different from those obtained by anohter mothod, and thus such markers always lack discriminative capability in the other data sets.\n",
      "In this paper, we investigate the resource allocation problem in a single-user relay-aided cognitive radio (CR) underlay network. Both the CR network and the primary network operate under the orthogonal frequency-division multiplexing (OFDM) scheme. Different from the conventional resource allocation problem, the relay node here is capable of performing subcarrier permutation over two hops such that the signal received over a particular subcarrier is forwarded via a different subcarrier. The objective is to maximize the throughput of the CR network subject to a limited power budget at the secondary source and the relay node and to interference constraints at the primary receiver. Optimization is performed under a unified framework where power allocation at the source node, power allocation at the relay node, and subcarrier pairing at the two hops are jointly optimized. The joint resource-allocation scheme yields an asymptotically optimal solution. We further design a suboptimal algorithm that sacrifices little on performance but could significantly reduce computational complexity. Finally, numerical examples are provided to corroborate the proposed studies.\n",
      "The variational level set model for piecewise constant multiphase image segmentation on the plane and the related Split-Augmented-Lagrangian Projection Method (SALPM) are investigated in this paper. On the analysis of the current problems based on the variational level set method for image segmentation, we also design a rapid SALPM method for two-phase image segmentation model, getting the general model of multiple phase level set in order to facilitate the generic design and program. In addition, the concrete formula of the rapid split algorithms for multiphase image segmentation with level set model and the calculation steps are given, and simultaneously taking liver tumor CT image as examples of the multiphase segmentation. Moreover, by comparing with the traditional methods, the experiments show that our algorithm presented in this paper have higher computational efficiency and accuracy, and better the extraction of liver contour.\n",
      "In sentiment analysis, a finer-grained opinion mining method not only focuses on the view of the product itself, but also focuses on product features, which can be a component or attribute of the product. Previous related research mainly relied on explicit features but ignored implicit features. However, the implicit features, which are implied by some words or phrases, are so significant that they can express the users' opinion and help us to better understand the users' comments. It is a big challenge to detect these implicit features in Chinese product reviews, due to the complexity of Chinese. This paper is mainly centered on implicit features identification in Chinese product reviews. A novel hybrid association rule mining method is proposed for this task. The core idea of this approach is mining as many association rules as possible via several complementary algorithms. Firstly, we extract candidate feature indicators based word segmentation, part-of-speech (POS) tagging and feature clustering, then compute the co-occurrence degree between the candidate feature indicators and the feature words using five collocation extraction algorithms. Each indicator and the corresponding feature word constitute a rule (feature indicator -> feature word). The best rules in five different rule sets are chosen as the basic rules. Next, three methods are proposed to mine some possible reasonable rules from the lower co-occurrence feature indicators and non indicator words. Finally, the latest rules are used to identify implicit features and the results are compared with the previous. Experiment results demonstrate that our proposed approach is competent at the task, especially via using several expanding methods. The recall is effectively improved, suggesting that the shortcomings of the basic rules have been overcome to certain extent. Besides those high co-occurrence degree indicators, the final rules also contain uncommon rules.\n",
      "The nodes in Ad Hoc networks compete for channels when communicating, with the features of no center and self-organization. In traditional channel assignment strategy of MAC layer, nodes do not consider the demands to channel resources of other nodes, which hinders improving the network performance. Practically, in the network based on competitive MAC protocol, each node tries to maximize its payoff, while this interferes with the behavior of other nodes at the same time. Game theory is an effective tool to solve problems of distributed resources, which can be used effectively in channel assignment. In this paper, we propose a new protocol, namely, DGPCI-DCA (Dynamic Game with Perfect and Complete Information based Dynamic Channel Assignment). When all the nodes are rational and greedy, each node selects channels dynamically by backward induction according to strategies of other nodes, thus Nash equilibrium can finally be achieved. Experiments show that the network performance is effectively improved, i.e., the throughput and saturation throughput can be increased, and the packet loss rate and network delay can be reduced.\n",
      "In this paper, we focus on detecting data hiding in motion vectors of compressed video and propose a new steganalytic algorithm based on the mutual constraints of motion vectors. The constraints of motion vectors from multiple frames are analyzed and formulized by three functions, then statistical features are extracted based on these functions. Moreover, we also incorporate calibration method to improve the detection accuracy. Experimental results demonstrate that the proposed method can effectively attack typical motion-vector-based video steganography.\n",
      "Internet of Things (IoT) is an emerging area that not only requires development of infrastructure but also deployment of new services capable of supporting multiple, scalable (cloud-based) and interoperable (multi-domain) applications. In the race of designing the IoT as part of the Future Internet architecture, academic and ICT's (Information and Communication Technology) industry communities have realized that a common IoT problem to be tackled is the interoperability of the information. In this paper we review recent trends and challenges on interoperability, and discuss how semantic technologies, open service frameworks and information models can support data interoperability in the design of the Future Internet, taking the IoT and Cloud Computing as reference examples of application domains.\n",
      "This paper presents an  H  ∞  controller design method for networked control systems (NCSs) with bounded packet dropouts. A new model is proposed to represent packet dropouts satisfying a Markov process and late-arrival packets. The closed-loop NCS with a state-feedback controller is transformed into a Markov system, which is convenient for the controller synthesis. Two types of state-feedback control laws are taken into account. Sufficient conditions on the existence of controllers for stochastic stability with an  H  ∞  disturbance attenuation level are derived through a Lyapunov function dependent on the upper bound of the number of consecutive packet dropouts. A numerical example is finally provided to show the effectiveness of the proposed method.\n",
      "The technology for through-wall human detection with ultra-wideband (UWB) radar was discussed. Due to the large amount of UWB radar data, compressive sensing theory was introduced and compressed UWB radar data can be collected. The singular value decomposition algorithm was used to acquire the singular values of compressed radar data. The compressed UWB radar data were collected at two statuses of human being for gypsum wall. The experimental results showed that the singular values with a human target were increased compared with those without a target.\n",
      "This paper describes a semantic modelling scheme, a naming convention and a data distribution mechanism for sensor streams. The proposed solutions address important challenges to deal with large-scale sensor data emerging from the Internet of Things resources. While there are significant numbers of recent work on semantic sensor networks, semantic annotation and representation frameworks, there has been less focus on creating efficient and flexible schemes to describe the sensor streams and the observation and measurement data provided via these streams and to name and resolve the requests to these data. We present our semantic model to describe the sensor streams, demonstrate an annotation and data distribution framework and evaluate our solutions with a set of sample datasets. The results show that our proposed solutions can scale for large number of sensor streams with different types of data and various attributes.\n",
      "Packet delay (either one-way time or round-trip time) is a very important metric for measuring the performance of networks in a highly dynamic environment such as the Internet. Many network applications are also sensitive to packet delay or delay variation for ensuring an acceptable level of quality in providing network-based services such as VoIP, multimedia streaming, etc. A very important property of packet delay is that it is very dynamic and therefore should be measured frequently with measurement results being updated on a timely basis. Measurement of packet delay has thus generated a great deal of interest in the past years and a lot of research has been performed in the development of measurement architecture as well as specific measurement techniques. However, how to reduce network overhead resulting from measurement while achieving a reasonable level of accuracy still remains a challenge. In this paper, we propose to use delay estimation as an alternative to delay measurement for reducing measurement overhead and, in particular, examine the level of accuracy that delay estimation can achieve. With delay estimation, measurement nodes can be dynamically selected and activated and other nodes can share measurement results by performing delay estimation, thus reducing measurement overhead while supporting the dynamic requirement for delay measurement. Consequently, while measurement overhead can be reduced by activating only a subset of network nodes to perform actual measurement, desired accuracy can be achieved by exploring the correlation between delays as well as by sharing measurement results to do delay estimation based on such a correlation. We illustrate how packet delays of network nodes can correlate to each other based on topological properties and show how delays can be estimated based on such a correlation to meet accuracy requirements, which would make delay measurement in the Internet highly dynamic and adaptable to the accuracy requirements and measurement results highly reliable. We also show how delay estimation can be applied by presenting three application scenarios as well as an example to demonstrate the usefulness and effectiveness of delay estimation in the measurement of packet delays.\n",
      "Spatial data mining presents new challenges due to the large size of spatial data, the complexity of spatial data types, and the special nature of spatial access methods. Most research in this area has focused on efficient query processing of static data. This paper introduces an active spatial data mining approach which extends the current spatial data mining algorithms to efficiently support user-defined triggers on dynamically evolving spatial data. To exploit the locality of the effect of an update and the nature of spatial data, we employ a hierarchical structure with associated statistical information at the various levels of the hierarchy and decompose the user-defined trigger into a set of sub-triggers associated with cells in the hierarchy. Updates are suspended in the hierarchy until their cumulative effect might cause the trigger to fire. It is shown that this approach achieves three orders of magnitude improvement over the naive approach that re-evaluates the condition over the database for each update, while both approaches produce the same result without any delay. Moreover this scheme can support incremental query processing as well.\n",
      "We present a hierarchical kernelized classification model for the automatic classification of general questions into their corresponding topic categories in community Question Answering service (cQAs). This could save many efforts of manual classification and facilitate browsing as well as better retrieving of questions from the cQA archives. To deal with the challenge of short text message of questions, we explore and optimally combine various cQA features by introducing multiple kernel learning strategy into the hierarchical classification framework. We propose a hybrid regularization approach of combining orthogonal constraint and L 1  sparseness in our framework to promote the discriminative power on similar topics as well as sparsing the model parameters. The experimental results on a real world dataset from Yahoo! Answers demonstrate the effectiveness of our proposed model as compared to the state-of-the-art methods and strong baselines.\n",
      "This paper proposes new limited-feedback Channel State Information (CSI) calculation schemes for Zero Forcing (ZF)-precoded downlink Multi-User Multiple-Input Multiple-Output (MU-MIMO) systems. It is a common understanding that the feedback quantized by the codebook limits the performance of MU-MIMO. In this paper, through a quasi-ZF or a quasi-Minimum Mean-Squared Error (MMSE) weight, the channel matrix is transferred to one of the codebook vectors, based on which, the CSI is calculated. Thus, the quantization error is minimized. Meanwhile, the selection for the codebook vector guarantees the maximizing of the estimated Signal to Interference plus Noise Ratio (SINR). We verify that the proposed scheme obtains accurate feedback information, and the predicted weight can be the same as the optimal linear decoder as if the receiver knew all the precoder information that is fed-forward from the BS, as long as the number of antennas for each receiver equals that for the transmitter, and equals that for the total transmit data streams. Compared to the commonly used Precoding Matrix Index (PMI) based method, which uses rank-one single user (SU)-MIMO feedback, simulation results show that the proposed schemes achieve higher sum capacities in different scenarios. Moreover, since the weight can be directly used as the decoder, the feed-forward overhead is reduced.\n",
      "In this paper, we propose a three-stage rateless coded protocol for a half-duplex time-division two-way relay system, where two terminals send messages to each other through a relay between them. In the protocol, each terminal takes one of the first two stages respectively to encode its message using rateless code and broadcast the result until the relay acknowledges successful decoding. During the third stage, the relay combines and re-encodes both messages with a joint network-channel coding scheme based on rateless coding which provides incremental redundancy. Together with the packets received directly in previous stages, each terminal then retrieves the desired message using an iterative decoder. The degree profiles of the specific rateless codes, i.e., Raptor codes, implemented at both terminals and the relay, are jointly optimized for both the AWGN channel and the Rayleigh block fading channel through solving a set of linear programming problems. Simulation results show that, the system throughput as well as the error rate achieved by the optimized degree profiles always outperforms those achieved by the conventional degree profile optimized for Binary Erasure Channel (BEC) and the previous network coding scheme with rateless codes.\n",
      "This contribution introduces position estimation methods relying on observations of the received power and mean delay obtained in a wideband multi-link scenario. In particular, one- and two-step methods are introduced based on statistical models of the observed link parameters. The proposed methods are tested on data from a wideband measurement campaign. The results show that including observations of mean delay of the wideband links can notably improve positioning accuracy as compared to relying on observations of received power alone.\n",
      "Graphs are widely used to model complicated data semantics in many applications in bioinformatics, chemistry, social networks, pattern recognition, etc. A recent trend is to tolerate noise arising from various sources such as erroneous data entries and find similarity matches. In this paper, we study graph similarity queries with edit distance constraints. Inspired by the $$q$$ -gram idea for string similarity problems, our solution extracts paths from graphs as features for indexing. We establish a lower bound of common features to generate candidates. Efficient algorithms are proposed to handle three types of graph similarity queries by exploiting both matching and mismatching features as well as degree information to improve the filtering and verification on candidates. We demonstrate the proposed algorithms significantly outperform existing approaches with extensive experiments on real and synthetic datasets.\n",
      "A flash translation layer (FTL) is a software layer running in the flash controller of a NAND flash memory solid-state disk (hereafter, flash SSD). It translates logical addresses received from a file system to physical addresses in flash SSD so that the linear flash memory appears to the system like a block storage device. Since the effectiveness of an FTL significantly impacts the performance and durability of a flash SSD, FTL design has attracted significant attention from both industry and academy in recent years. In this research, we propose a new FTL called DLOOP (Data Log On One Plane), which fully exploits plane-level parallelism supported by modern flash SSDs. The basic idea of DLOOP is to allocate logs (updates) onto the same plane where their associated original data resides so that valid page copying operations triggered by garbage collection can be carried out by intraplane copy-back operations without occupying the external I/O bus. Further, we largely extend a validated simulation environment DiskSim3.0/FlashSim to implement DLOOP. Finally, we conduct comprehensive experiments to evaluate DLOOP using realistic enterprise-scale workloads. Experimental results show that DLOOP consistently outperforms a classical hybrid FTL named FAST and a morden page-mapping FTL called DFTL.\n",
      "Dense sample video patches have been used for video representation in action recognition and achieve better performance than sparse spatiotemporal local features. However, two problems of this method must be considered. First one, many video patches are from background other than human body. Second one, the descriptor is not reliable, since it is neither shift nor scale invariant. To solve these two problems, we proposed an Optimized Video Dense Sampling (OVDS) method combing with dense sampling and spatiotemporal interest points detector. OVDS densely sampled video patches with optimizing the position and scale parameters to guarantee the features are shift and scale invariant. To omit the action unrelated features, we extracted video patches only from human body regions instead of the whole videos. Experimental results on KTH, Weizmann, UCF, Hoollywood2 datasets showed that the features detected by OVDS are informative and reliable for action recognition, and achieve better performance over the existing spatiotemporal local features.\n",
      "An important prerequisite for intelligent robots to effectively perform daily tasks in indoor environments is an advanced environment model. Not only should this model contain metric level information about the geometry of the perceived environment, it should also provide abstract level information, such as topology and objects, so as to benefit high level robotic applications. In this paper, we propose a mapping system which generates a coherent semantic map for indoor environments while taking laser range data and RGB-D images as sensor input. We propose to realize this system by combining parametric environment abstraction, i.e. using a parametric model to approximate the geometry of the perceived environment, with 3D object localization. Experiments using real world data show promising results and thus confirm the usefulness of our system.\n",
      "The finite-SNR Diversity-Multiplexing Tradeoff (DMT) problem is studied in the context of spectrum aggregation, where there exist multiple noncontiguous (discrete) spectrum segments each of which has its own power limit. Firstly, we give an optimal power allocation strategy to achieve the ergodic capacity under Rayleigh fading. Secondly, two different coding schemes, i.e., coding without across sub-channels and coding across, are presented to achieve the lower and upper bound of diversity gain of this system under the same multiplexing gain. Then the finite-SNR DMT under i.i.d Rayleigh fading is derived, and its accurate expression for the coding-without-across scheme and the upper bound for the coding-across scheme are achieved respectively. The asymptotic performances of both schemes are also obtained. Under the equal sub-channel bandwidth setting, for infinite SNR, the coding-across scheme achieves M (the number of sub-channels) times diversity gain against the without-coding-across scheme. The loss of diversity gain caused by non-identical bandwidth and power limit is also analyzed.\n",
      "In multimedia communication, various data rates, security strategies, and data qualities are required for different content forms such as text, audio, images, video, and interactivity content forms. In this paper, we propose a secure and robust approach to achieve the multimedia multiple access (MA) communication using self-encoded spread spectrum (SESS). In this proposed system, SESS multiple access (SESS-MA) is a novel approach to multimedia system due to its unique secure and flexible spreading nature. Iterative detection is applied for an improved multimedia quality of service (QoS) at the receiver. The number of iterations needed is evaluated separately according to different multimedia contents. Simulation studies demonstrate that the proposed scheme ensures satisfactory data quality, security, and robustness.\n",
      "Implicit feature detection, also known as implicit feature identification, is an essential aspect of feature-specific opinion mining but previous works have often ignored it. We think, based on the explicit sentences, several Support Vector Machine (SVM) classifiers can be established to do this task. Nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better.\n",
      "Middleboxes have found widespread adoption in today's networks. They perform a variety of network functions such as WAN optimization, intrusion detection, and network-level firewalls. Processing packets to serve these functions often require multiple middlebox resources, e.g., CPU and link band-width. Furthermore, different packet traffic flows may consume significantly different amounts of various resources, depending on the network functions that are applied. Multi-resource fair queueing is therefore needed to allow flows to share multiple middlebox resources in a fair manner. In this paper, we clarify the fairness requirements of a queueing scheme and present Dominant Resource Generalized Processor Sharing (DRGPS), a fluid flow-based fair queueing idealization that strictly realizes Dominant Resource Fairness (DRF) at all times. As a form of Generalized Processor Sharing (GPS) running on multiple resources, DRGPS serves as a benchmark that practical packet-by-packet fair queueing algorithm should follow. With DRGPS, techniques and insights that have been developed for traditional fair queueing can be leveraged to schedule multiple resources. As a case study, we extend Worst-case Fair Weighted Fair Queueing (WF 2 Q) to the multi-resource setting and analyze its performance.\n",
      "In this paper, we aim to design distributed adaptive controllers for output consensus tracking of multiple nonlinear subsystems with intrinsic mismatched unknown parameters. The graph representing the communication status among subsystems is assumed to have directed and fixed topology. Only a small percentage of the subsystems can obtain the desired trajectory information, which is regarded as a virtual leader node added to the original communication graph. We first split the communication graph into a hierarchical structure according to the shortest possible path of each subsystem originated from the virtual leader. Then local adaptive controllers for subsystems in different layers can be designed in a sequential order. By introducing the estimates of the uncertainties of its neighbors located in the upper layer into the local controller of a subsystem, the transmission of parameter estimates among connected subsystems is avoided. It is proved that output consensus tracking of the overall system can be achieved asymptotically and all closed-loop signals are ensured bounded. Simulation results show the effectiveness of our scheme.\n",
      "Echo state networks (ESNs), that exhibit good performance for modeling a nonlinear or non-Gaussian dynamic system, have been widely used for time series prediction. However, estimating the output weights of the ESNs remains intractable. Extended Kalman filter (EKF) is an effective estimate method, but its computational cost is relatively high. In this study, a MapReduce framework based parallelized EKF is proposed to learn the parameters of the network, in which two MapReduce based models are designed, and each of them is composed of a set of mapper and reducer functions. The mapper receives a training sample and generates the updates of the internal states or the output weights, while the reducer merges all updates associated with the same key to produce an average value. To verify the effectiveness and the efficiency of the proposed method, an industrial data prediction problem coming from the blast furnace gas (BFG) system in steel industry is employed for the validation experiments, and the experimental results demonstrate that the proposed parallelized EKF can efficiently estimate the parameters of the ESN with good performance and computing time.\n",
      "Motivation: RNA-seq techniques provide an unparalleled means for exploring a transcriptome with deep coverage and base pair level resolution. Various analysis tools have been developed to align and assemble RNA-seq data, such as the widely used TopHat/Cufflinks pipeline. A common observation is that a sizable fraction of the fragments/reads align to multiple locations of the genome. These multiple alignments pose substantial challenges to existing RNA-seq analysis tools. Inappropriate treatment may result in reporting spurious expressed genes (false positives) and missing the real expressed genes (false negatives). Such errors impact the subsequent analysis, such as differential expression analysis. In our study, we observe that � 3.5% of transcripts reported by TopHat/Cufflinks pipeline correspond to annotated nonfunctional pseudogenes. Moreover, � 10.0% of reported transcripts are not annotated in the Ensembl database. These genes could be either novel expressed genes or false discoveries. Results: We examine the underlying genomic features that lead to multiple alignments and investigate how they generate systematic errors in RNA-seq analysis. We develop a general tool, GeneScissors, which exploits machine learning techniques guided by biological knowledge to detect and correct spurious transcriptome inference by existing RNA-seq analysis methods. In our simulated study, GeneScissors can predict spurious transcriptome calls owing to misalignment with an accuracy close to 90%. It provides substantial improvement over the widely used TopHat/Cufflinks or MapSplice/ Cufflinks pipelines in both precision and F-measurement. On real data, GeneScissors reports 53.6% less pseudogenes and 0.97% more expressed and annotated transcripts, when compared with the TopHat/Cufflinks pipeline. In addition, among the 10.0% unannotated transcripts reported by TopHat/Cufflinks, GeneScissors finds that 416.3% of them are false positives. Availability: The software can be downloaded at http://csbio.unc.edu/ genescissors/\n",
      "Efficiently answering XML keyword queries has attracted much research effort in the last decade. One key factors resulting in the inefficiency of existing methods are the common-ancestor-repetition (CAR) and visiting-useless-nodes (VUN) problems. In this paper, we propose a generic top-down processing strategy to answer a given keyword query w.r.t. LCA/SLCA/ELCA semantics. By top-down, we mean that we visit all common ancestor (CA) nodes in a depth-first, left-to-right order, thus avoid the CAR problem; by generic, we mean that our method is independent of the labeling schemes and query semantics. We show that the satisfiability of a node v w.r.t. the given semantics can be determined by v's child nodes, based on which our methods avoid the VUN problem. We propose two algorithms that are based on either traditional inverted lists or our newly proposed LLists to improve the overall performance. The experimental results verify the benefits of our methods according to various evaluation metrics.\n",
      "The objective of this study was to investigate the method of the combination of radiological and textural features for the differentiation of malignant from benign solitary pulmonary nodules by computed tomography. Features including 13 gray level co-occurrence matrix textural features and 12 radiological features were extracted from 2,117 CT slices, which came from 202 (116 malignant and 86 benign) patients. Lasso-type regularization to a nonlinear regression model was applied to select predictive features and a BP artificial neural network was used to build the diagnostic model. Eight radiological and two textural features were obtained after the Lasso-type regularization procedure. Twelve radiological features alone could reach an area under the ROC curve (AUC) of 0.84 in differentiating between malignant and benign lesions. The 10 selected characters improved the AUC to 0.91. The evaluation results showed that the method of selecting radiological and textural features appears to yield more effective in the distinction of malignant from benign solitary pulmonary nodules by computed tomography.\n",
      "For session-level quality-of-service (QoS) provisioning in cognitive radio networks (CRNs), it is essential to adopt an effective admission control scheme with appropriate parameters. In this paper, a framework for admission control and session-level performance analysis is proposed by taking into account several factors such as channel reservation for handoff secondary users (SUs), buffers for handoff SUs and newly arriving SUs, limited buffer size, and queued SUs' leaving due to impatience. The whole system is modeled by a multi-dimensional continuous-time Markov chain, where important session-level QoS performance metrics, i.e., dropping probability and blocking probability are derived.\n",
      "Multiple people tracking is an important component for different tasks such as video surveillance and human-robot interaction. In this paper, a global optimization approach is proposed for long-term tracking of an a priori unknown number of targets, particularly aim to improve the robustness in case of complex interaction and mutual occlusion. With a state-space discretization scheme, the multiple object tracking problem is formulated with a grid-based network flow model, resulting in a convex problem that can be casted into an Integer Linear Programming (ILP), then solved through relaxation. In order to allow recovery from misdetections, common heuristics such as non-maxima suppression is eschewed within observations. In addition, we show that how behavior cue can be integrated into the association affinity model, providing discriminative hints for resolving ambiguities between crossing trajectories. The validity of the proposed method is demonstrated through experiments on multiple challenging video sequences, using a calibrated multi-camera setup.\n",
      "Anomaly detection is indispensable for satisfying security services in mobile ad hoc network (MANET) applications. Often, however, a highly secure mechanism consumes a large amount of network resources, resulting in network performance degradation. To shift intrusion detection from existing security-centric design approaches to network performance centric design schemes, this paper presents a framework for designing an energy-aware and self-adaptive anomaly detection scheme for resource constrained MANETs. The scheme uses network tomography, a new technique for studying internal link performance based solely on end-to-end measurements. With the support of a module comprising a novel spatial-time model to identify the MANET topology, an energy-aware algorithm to sponsor system service, a method based on the expectation maximum to infer delay distribution, and a Self-organizing Map (SOM) neural network solution to profile link activity, the proposed system is capable of detecting link anomalies and localizing malicious nodes. Consequently, the proposed scheme offers a trade-off between overall network security and network performance, without causing any heavy network overload. Moreover, it provides an additional approach to monitor the spatial-time behavior of MANETs, including network topology, link performance and network security. The effectiveness of the proposed schemes is verified through extensive experiments.\n",
      "The Mining Software Repositories (MSR) research community has grown significantly since the first MSR workshop was held in 2004. As the community continues to broaden its scope and deepens its expertise, it is worthwhile to reflect on the best practices that our community has developed over the past decade of research. We identify these best practices by surveying past MSR conferences and workshops. To that end, we review all 117 full papers published in the MSR proceedings between 2004 and 2012. We extract 268 comments from these papers, and categorize them using a grounded theory methodology. From this evaluation, four high-level themes were identified: data acquisition and preparation, synthesis, analysis, and sharing/replication. Within each theme we identify several common recommendations, and also examine how these recommendations have evolved over the past decade. In an effort to make this survey a living artifact, we also provide a public forum that contains the extracted recommendations in the hopes that the MSR community can engage in a continuing discussion on our evolving best practices.\n",
      "Ship detection is an important application in remote sensing and Earth observation since last century. Properties of ship targets changed greatly as observation accuracy of SAR sensors is strongly increased. Ship targets become objects detailed in structure by contrast to point targets in lower resolution SAR images. In this paper, we present a radiometric-spatial analysis (RSA) based method for ship detection in high resolution SAR images.\n",
      "The theory of Ultra-wideband radar and the through wall detection of human model based on UWB radar are briefly introduced. The target criterion with wavelet packet transform is deduced and the procedure for the through wall human detection with statistical process control is constructed. The radar echo signals are collected at stationary and moving status of human being for Brick wall. The experimental results demonstrate the effective of through wall target detection based on the proposed algorithm.\n",
      "While organisational investment in complex information technologies IT keeps growing, these technologies are often applied at a superficial level and fail to attain the promised benefits. To further extract the value potential of complex IT, this study investigates employee users' innovate with IT IwIT, which is a post-acceptance behaviour that refers to individual users applying IT in novel ways to support their task performance. Drawing on the information system continuance ISC model, we propose a research framework with perceived usefulness PU and satisfaction SAT as the antecedents of IwIT. We further emphasise the contingent role of personal characteristics and include personal innovativeness with IT PIIT and IT self-efficacy ITSE as the moderators of the framework. We validate the model with data from users of two complex ITs: enterprise resource planning and business intelligence technologies. The results suggest that positioning personal factors as moderators significantly increases the explanatory power of the ISC model and offers a more comprehensive understanding about IwIT. Specifically, ITSE positively moderates the effect of PU and negatively moderates the effect of SAT on IwIT. The moderating role of PIIT, however, is subject to the specific type of IT of investigation.\n",
      "Multi-view human action recognition has gained a lot of attention in recent years for its superior performance as compared to single view recognition. In this paper, we propose a new framework for the real-time realization of human action recognition in distributed camera networks (DCNs). We first present a new feature descriptor (Mltp-hist) that is tolerant to illumination change, robust in homogeneous region and computationally efficient. Taking advantage of the proposed Mltp-hist, the noninformative 3-D patches generated from the background can be further removed automatically that effectively highlights the foreground patches. Next, a new feature representation method based on sparse coding is presented to generate the histogram representation of local videos to be transmitted to the base station for classification. Due to the sparse representation of extracted features, the approximation error is reduced. Finally, at the base station, a probability model is produced to fuse the information from various views and a class label is assigned accordingly. Compared to the existing algorithms, the proposed framework has three advantages while having less requirements on memory and bandwidth consumption: 1) no preprocessing is required; 2) communication among cameras is unnecessary; and 3) positions and orientations of cameras do not need to be fixed. We further evaluate the proposed framework on the most popular multi-view action dataset IXMAS. Experimental results indicate that our proposed framework repeatedly achieves state-of-the-art results when various numbers of views are tested. In addition, our approach is tolerant to the various combination of views and benefit from introducing more views at the testing stage. Especially, our results are still satisfactory even when large misalignment exists between the training and testing samples.\n",
      "Far field wireless recharging based on microwave power transfer (MPT) will release mobile terminals from interruption due to finite battery storage. Wireless energy transfer (WET), which integrating MPT with wireless communications, is a promising technology to provide perpetual energy supplies to wireless networks. In this paper, we propose a novel power allocation algorithm for Multiuser MIMO (MU MIMO) systems with simultaneous wireless information and power transfer (SIPT). For the purpose of universal relevance, a K +L+1 node network is investigated, in which K receivers decode information and the other L receivers harvest energy from the signals broadcasted by the base station. The proposed integrated scheme, based on the well-known Block Diagonalization (BD) preceding method, can completely suppress the multiuser interference and perform an outer-bound which is characterized by the boundary of the rate-energy (R-E) region as well.\n",
      "Providing end-to-end reliability for data transmission is still an intractable challenge for delay-sensitive wireless multimedia networks. This paper deals with an unequal error protection (UEP) scheme for scalable video delivery over packet-lossy networks using forward error correction (FEC). The proposed cross-layer approach allows the wireless multimedia networks to jointly optimize the application layer and the error protection strategies available at the lower layers. The redundancy is tuned in accordance with both the wireless channel condition (as indicated by symbol error rate) and the time limit (as indicated by parity budget). Simulations demonstrate that the proposed cross-layer grouping scheme can significantly improve video transmission quality by allocating more correction bits to the vital frames, while the time constraint is satisfied and the complexity associated with performing this group allocation algorithm is reduced.\n",
      "In this paper, we propose and evaluate a Transmission Failure Based Routing Algorithm TFBRA for low power consumption in ZigBee networks with changing mobility. We analyse the traffic with different routing algorithms which are stated in ZigBee specification. The effects of different factors on data communication are discussed. As the variation of mobility results in changing possibility of sending routing request as well as varying routing performance, the probability of transmission failure in one hop is used to describe node mobility. The TFBRA compares this rate with the threshold which is computed based on hardware condition to decide the routing scheme in ZigBee network. The scheme utilises the information in neighbour table and network address for mobility estimation without any extra communication. The simulation results show that the proposed approach achieves better performance than original routing schemes in ZigBee specification by obtaining lower network load and larger ratio of successful data transmission.\n",
      "For protocol and system design in cognitive radio networks (CRNs), it is essential to identify which system settings or environmental conditions have great impacts on the quality-of-service (QoS) performance for secondary users (SUs) and how they influence it, which are still open issues. In this paper, an analytical framework to quantify the queue dynamics of a multi-SU multichannel CRN is developed. In the analytical framework, we consider the important lower-layer mechanisms and settings, including spectrum sensing errors, medium-access control (MAC) protocols, link adaptation technologies such as adaptive modulation and coding (AMC) and automatic repeat request (ARQ), and limited buffer size. By modeling the queue dynamics as a discrete-time finite-state Markov chain (FSMC), we derive the analytical expressions of the average queue length, packet-dropping rate, and packet-collision rate. Based on these expressions, the QoS metrics including average queueing delay, packet-loss rate, and effective throughput are obtained. Simulation and numerical results are presented to verify the accuracy of the proposed analytical framework and investigate the QoS performance of SUs.\n",
      "Recently space-time interest points (STIPs) using bag-of-feature (BOF) in action recognition has been highly successful. Despite its popularity, The quantization error and the lost of semantic meaning among STIPs are the main weaknesses that severely limit the effectiveness of this method. To overcome these limitations, this paper incorporated the feature position information into coding procedure and proposed a novel Feature Position Constrained Linear Coding (FPLC) method by extending the Locality Constrained Linear Coding (LLC) approach. It first project the features into the human ROI, then codes the features locally using FPLC with the consideration of feature position. Owning to that the local area of human ROI often aggregate features extracted from the same part of human body and those features should exhibit similar values, this local coding strategy helps to alleviate the quantization error and enhance correlation between features at the same time, which helps to improve the recognition accuracy. Compared with the state-of-the-art action recognition method, experiment results demonstrated the effectiveness of the proposed method.\n",
      "How to prolong the lifetime of multi-hop-multi-path wireless mesh networks powered by finite battery resource is a critical research challenge. In this paper, we strive for energy-balancing to maximize the network lifetime, while improving multimedia service quality via exploring both hop and path diversities. Different from traditional research, which sacrifices throughput for efficient energy utilization in the networks, we proposed a new approach jointly considering 5 factors in a cross layer fashion: the wireless channel condition, the physical layer channel coding redundancy, the application layer traffic rate, the path level energy-balancing of the network, and the Quality of Service (QoS) of multimedia. In the proposed approach, we develop an optimal scheme to allocate path-level multimedia traffic and link-level channel coding redundancy jointly, which minimizes the routers' battery energy difference while satisfying a lower bound of quality requirement. The simulation results illustrate that the proposed approach can significantly improve the energy-balancing of the multi-hop-multi-path wireless mesh networks by performing the traffic and channel coding control.\n",
      "The problem of disturbance decoupling with internal stability is investigated for a class of multi-input-multi-output (MIMO) nonlinear systems in strict-feedback form. The considered nonlinear system contains unknown nonlinear uncertainties and unmeasured states. Fuzzy logic systems are first utilized to approximate the unknown nonlinear uncertainties, and then a high-gain state observer is established to estimate the unmeasured states. Based on the backstepping design approach and combining with dynamic surface control (DSC) technique, a new fuzzy adaptive output feedback control scheme is constructed. It is proved that the proposed control scheme can not only achieve internal stability of the closed-loop system, but also diminish the effect of the disturbance on output in the sense of L\"2 gain. In addition, since DSC is introduced into the backstepping design, the proposed control scheme can overcome the problem of ''explosion of complexity'' inherent in the traditional backstepping design. Simulation results and comparisons with the previous methods are included to indicate that the proposed adaptive fuzzy control approach has a satisfactory control performance.\n",
      "This paper is concerned with the distributed H\"~ filtering problem in sensor networks for discrete-time systems with missing measurements and communication link failures. The sensor measurements are unavailable randomly and the communication link between nodes may be lost. Both of these phenomena occur with known probabilities. The purpose of this problem is to design a filter on each node in the sensor network such that, for all possible measurements missing and communication link failures, the dynamics of filtering error is mean-square stable and the prescribed average H\"~ performance constraint is met. This problem is solved by mean of establishing a filter for a constructed two-dimensional (2-D) system in Roesser model. Consensus protocol is introduced into the filter model as local information fusion strategy. In terms of certain linear matrix inequalities (LMIs), sufficient conditions for the solvability of the addressed problem are obtained. Finally, a numerical example is provided to demonstrate the effectiveness and applicability of the proposed design approach.\n",
      "Continuous motorization and urbanization around the globe leads to an expansion of population in major cities. Therefore, ever-growing pressure imposed on the existing mass transit systems calls for a better technology, Intelligent Transportation Systems (ITS), to solve many new and demanding management issues. Many studies in the extant ITS literature attempted to address these issues within which various research methodologies were adopted. However, there is very few paper summarized what does optimal control theory (OCT), one of the sharpest tools to tackle management issues in engineering, do in solving these issues. It{\\textquoteright}s both important and interesting to answer the following two questions. (1) How does OCT contribute to ITS research objectives? (2) What are the research gaps and possible future research directions? We searched 11 top transportation and control journals and reviewed 41 research articles in ITS area in which OCT was used as the main research methodology. We categorized the articles by four different ways to address our research questions. We can conclude from the review that OCT is widely used to address various aspects of management issues in ITS within which a large portion of the studies aimed to reduce traffic congestion. We also critically discussed these studies and pointed out some possible future research directions towards which OCT can be used.\n",
      "This paper presents delay-dependent conditions for the solvability of the output regulation problem of time delay systems. Based on internal model approach, two classes of controller structures are studied, namely, state feedback and dynamic output feedback schemes. First, delay-dependent sufficient conditions for the stabilization of time delay systems are presented by a linearization technique under the state feedback control. A new proof of testified necessary and sufficient conditions for checking the solvability of output regulation problems is provided. Second, a dynamic compensator is designed by the output information and constructed by a delay-dependent condition. A numerical example is finally provided to validate the proposed method.\n",
      "Information Extraction (IE) is the task of automatically extracting structured information from unstructured/semi-structured machine-readable documents. Among various IE tasks, extracting actionable intelligence from ever-increasing amount of data depends critically upon Cross-Document Coreference Resolution (CDCR) - the task of identifying entity mentions across multiple documents that refer to the same underlying entity. Recently, document datasets of the order of peta-/tera-bytes has raised many challenges for performing effective CDCR such as scaling to large numbers of mentions and limited representational power. The problem of analysing such datasets is called \"big data\". The aim of this paper is to provide readers with an understanding of the central concepts, subtasks, and the current state-of-the-art in CDCR process. We provide assessment of existing tools/techniques for CDCR subtasks and highlight big data challenges in each of them to help readers identify important and outstanding issues for further investigation. Finally, we provide concluding remarks and discuss possible directions for future work.\n",
      "Evaluating similarity between sets is a fundamental task in computer science. However, there are many applications in which elements in a set may be uncertain due to various reasons. Existing work on modeling such probabilistic sets and computing their similarities suffers from huge model sizes or significant similarity evaluation cost, and hence is only applicable to small probabilistic sets. In this paper, we propose a simple yet expressive model that supports many applications where one probabilistic set may have thousands of elements. We define two types of similarities between two probabilistic sets using the possible world semantics; they complement each other in capturing the similarity distributions in the cross product of possible worlds. We design efficient dynamic programming-based algorithms to calculate both types of similarities. Novel individual and batch pruning techniques based on upper bounding the similarity values are also proposed. To accommodate extremely large probabilistic sets, we also design sampling-based approximate query processing methods with strong probabilistic guarantees. We have conducted extensive experiments using both synthetic and real datasets, and demonstrated the effectiveness and efficiency of our proposed methods.\n",
      "Public-key encryption with keyword search (PEKS) is a versatile tool. It allows a third party knowing the search trapdoor of a keyword to search encrypted documents containing that keyword without decrypting the documents or knowing the keyword. However, it is shown that the keyword will be compromised by a malicious third party under a keyword guess attack (KGA) if the keyword space is in a polynomial size. We address this problem with a keyword privacy enhanced variant of PEKS referred to as public-key encryption with fuzzy keyword search (PEFKS). In PEFKS, each keyword corresponds to an exact keyword search trapdoor and a fuzzy keyword search trapdoor. Two or more keywords share the same fuzzy keyword trapdoor. To search encrypted documents containing a specific keyword, only the fuzzy keyword search trapdoor is provided to the third party, i.e., the searcher. Thus, in PEFKS, a malicious searcher can no longer learn the exact keyword to be searched even if the keyword space is small. We propose a universal transformation which converts any anonymous identity-based encryption (IBE) scheme into a secure PEFKS scheme. Following the generic construction, we instantiate the first PEFKS scheme proven to be secure under KGA in the case that the keyword space is in a polynomial size.\n",
      "The vision of Self-Organizing Networks (SON) has been drawing considerable attention as a major axis for the development of future networks. As an essential functionality in SON, cell outage detection is developed to autonomously detect macrocells or femtocells that are inoperative and unable to provide service. Previous cell outage detection approaches have mainly focused on macrocells while the outage issue in the emerging femtocell networks is less discussed. However, due to the two-tier macro-femto network architecture and the small coverage nature of femtocells, it is challenging to enable outage detection functionality in femtocell networks. Based on the observation that spatial correlations among users can be extracted to cope with these challenges, this paper proposes a Cooperative femtocell Outage Detection (COD) architecture which consists of a trigger stage and a detection stage. In the trigger stage, we design a trigger mechanism that leverages correlation information extracted through collaborative filtering to efficiently trigger the detection procedure without inter-cell communications. In the detection stage, to improve the detection accuracy, we introduce a sequential cooperative detection rule to process the spatially and temporally correlated user statistics. In particular, the detection problem is formulated as a sequential hypothesis testing problem, and the analytical results on the detection performance are derived. Numerical studies for a variety of femtocell deployments and configurations demonstrate that COD outperforms the existing scheme in both communication overhead and detection accuracy.\n",
      "We design and evaluate a new type of wireless mesh nodes called Dyntenna nodes that are equipped with steerable omnidirectional antenna. Designed for 3D wireless mesh networks, these nodes adaptively adjust the antenna orientation to increase throughput by improving the Received Signal Strength Indicator (RSSI) reading between nodes. We demonstrate the importance of being able to programmatically orient the antenna, by presenting the measurement results from our 3D urban mesh testbed. We propose a simple antenna adjustment algorithm that can improve the throughput for 26% of one-hop paths and 35% of multi-hop paths by a median value of 31% and 46%, respectively. Our algorithm converges quickly and typically probes less than 10% of all possible antenna orientations on average.\n",
      "Classification is essential to many web applications such as focused crawling, search engine, recommendation, content filter, knowledge discovery, etc. Although traditional classification algorithms have many virtues, the unstructured and big web data application presents challenges to these traditional algorithms. Moreover, the traditional segmentation processing usually needs more time and a complete lexicon. This work focuses on an interesting and promising approach that may enhance the classification performance of web and big data applications. Based on the non-segment and the classified-centre-vector, the proposed algorithm can meet the request on big data classifications. By using positive and negative modification, it can revise the potential data bias. The experimental results and the analysis show the feasibility of this approach.\n",
      "Monochromatic noise always interferes with the interpretation of the seismic signals and degrades the quality of subsurface images obtained by further processes. Conventional methods suffer from several problems in detecting the monochromatic noise automatically, preserving seismic signals, etc. In this letter, we present an algorithm that can remove all major monochromatic noises from the seismic traces in a relatively harmless way. Our separation model is set up upon the assumption that input seismic data are composed of useful seismic signals and single-frequency interferences. Based on their diverse morphologies, two waveform dictionaries are chosen to represent each component sparsely, and the separation process is promoted by the sparsity of both components in their corresponding representing dictionaries. Both synthetic and field-shot data are employed to illustrate the effectiveness of our method.\n",
      "When a table containing individual data is published, disclosure of sensitive information should be prohibitive. (e, m)-anonymity was a new anonymization principle for preservation of proximity privacy, in publishing numerical sensitive data. It is shown to be NP-Hard to (e, m)-anonymize a table minimizing the number of suppressed cells. Extensive performance study verified our findings that our algorithm is significantly better than the traditional algorithms presented in the paper[1].\n",
      "While active rules have been applied in many areas including active databases, XML document and Semantic Web, current methods remain largely uncertain of how to detect the termination when analysing rules behaviours. Some methods are based on a logical formula for a rule set, but only those non-updatable or finitely updatable variables can be contained by a formula. Some other methods are based on triggering and activation graphs, but they do not consider whether all rules of a triggering cycle can be infinitely executed during a single cyclic execution. Most methods cannot conclude termination if a rule set contains only those cycles that can be executed for a finite number of times. This paper presents the concepts of activation path and the execution sequence of a triggering cycle as well as the method to construct a formula that is able to include updatable variables, and then many termination cases that cannot be determined by previous methods, can now be detected.\n",
      "Mapping reads to a reference sequence is a common step when analyzing allele effects in high throughput sequencing data. The choice of reference is critical because its effect on quantitative sequence analysis is non-negligible. Recent studies suggest aligning to a single standard reference sequence, as is common practice, can lead to an underlying bias depending the genetic distances of the target sequences from the reference. To avoid this bias researchers have resorted to using modified reference sequences. Even with this improvement, various limitations and problems remain unsolved, which include reduced mapping ratios, shifts in read mappings, and the selection of which variants to include to remove biases.   To address these issues, we propose a novel and generic multi-alignment pipeline. Our pipeline integrates the genomic variations from known or suspected founders into separate reference sequences and performs alignments to each one. By mapping reads to multiple reference sequences and merging them afterward, we are able to rescue more reads and diminish the bias caused by using a single common reference. Moreover, the genomic origin of each read is determined and annotated during the merging process, providing a better source of information to assess differential expression than simple allele queries at known variant positions. Using RNA-seq of a diallel cross, we compare our pipeline with the single reference pipeline and demonstrate our advantages of more aligned reads and a higher percentage of reads with assigned origins.\n",
      "In this paper, we study the total variation structured total least squares method for image restoration. In the image restoration problem, the point spread function is corrupted by errors. In the model, we study the objective function by minimizing two variables: the restored image and the estimated error of the point spread function. The proposed objective function consists of the data-fitting term containing these two variables, the magnitude of error and the total variation regularization of the restored image. By making use of the structure of the objective function, an efficient alternating minimization scheme is developed to solve the proposed model. Numerical examples are also presented to demonstrate the effectiveness of the proposed model and the efficiency of the numerical scheme.\n",
      "Adopting small cells, including femtocells, is a promising evolution of future wireless cellular systems to meet the explosive demand for high data rates. As the number of distributively deployed femtocell access points increases rapidly, interference coordination becomes the primary challenge in such heterogeneous networks. In this article, we apply several cognitive radio inspired approaches to enhance the interference coordination for femtocell networks. First, we apply spectrum sensing and statistical analysis to estimate the cross-tier interference between macrocells and femtocells. Based on this, interference coordination is investigated considering two kinds of spectrum sharing approaches. Finally, we introduce a cognitive relay scheme to improve interference coordination performance further.\n",
      "The removal of spatially correlated noise is an important step in processing multichannel recordings. Here, a technique termed the adaptive common average reference (ACAR) is presented as an effective and simple method for removing this noise. The ACAR is based on a combination of the well-known common average reference (CAR) and an adaptive noise canceling (ANC) filter. In a convergent process, the CAR provides a reference to an ANC filter, which in turn provides feedback to enhance the CAR. This method was effective on both simulated and real data, outperforming the standard CAR when the amplitude or polarity of the noise changes across channels. In many cases, the ACAR even outperformed independent component analysis. On 16 channels of simulated data, the ACAR was able to attenuate up to approximately 290 dB of noise and could improve signal quality if the original SNR was as high as 5 dB. With an original SNR of 0 dB, the ACAR improved signal quality with only two data channels and performance improved as the number of channels increased. It also performed well under many different conditions for the structure of the noise and signals. Analysis of contaminated electrocorticographic recordings further showed the effectiveness of the ACAR.\n",
      "Many audio applications such as audio surveillance and human acoustic health monitoring require security protections for audio streaming over WSNs. The process of watermarking which embeds small amounts of data (i.e., the watermark) into the original audio is an effective technique to ensure the integrity of received audio data at the receiver in energy-constrained WSNs. However, the selection of positions to embed watermark into audio streams is critical to both received audio quality and watermarking authentication performance in error-prone wireless transmission environments. In this paper we propose an approach that dynamically determines the range of middle sub-band components for embedding the watermark with minimum quality distortions, based on psycho-acoustic models and adaptive sub-band thresholds. In addition, through unequal network resource allocation schemes the proposed approach protects both middle sub-bands and high sub-bands, which include the important audio components. Our theoretical analysis and simulation results demonstrate that the proposed quality-driven energy-efficient watermarking approach for audio transmissions can achieve considerable performance gains in WSNs.\n",
      "Neighbor discovery (ND) is a basic and crucial step for initializing wireless ad hoc networks. A fast, precise, and energy-efficient ND protocol has significant importance to subsequent operations in wireless networks. However, many existing protocols have a high probability of generating idle slots in their neighbor discovering processes, which prolongs the executing duration, thus compromising their performance. In this paper, we propose a novel randomized protocol FRIEND, which is a prehandshaking ND protocol, to initialize synchronous full-duplex wireless ad hoc networks. By introducing a prehandshaking strategy to help each node be aware of activities of its neighborhood, we significantly reduce the probabilities of generating idle slots and collisions. Moreover, with the development of single-channel full-duplex communication technology, we further decrease the processing time needed in FRIEND and construct the first fullduplex ND protocol. Our theoretical analysis proves that FRIEND can decrease the duration of ND by up to 48% in comparison with classical ALOHA-like protocols. In addition, we propose HD-FRIEND for half-duplex networks and variants of FRIEND for multihop and duty-cycled networks. Both theoretical analysis and simulation results show that FRIEND can adapt to various scenarios and significantly decrease the duration of ND.\n",
      "Co-training is a famous semi-supervised learning paradigm exploiting unlabeled data with two views. Most previous theoretical analyses on co-training are based on the assumption that each of the views is sucient to correctly predict the label. However, this assumption can hardly be met in real applications due to feature corruption or various feature noise. In this paper, we present the theoretical analysis on co-training when neither view is sucient. We dene the diversity between the two views with respect to the condence of prediction and prove that if the two views have large diversity, co-training is able to improve the learning performance by exploiting unlabeled data even with insucient views. We also discuss the relationship between view insuciency and diversity, and give some implications for understanding of the dierence between co-training and co-regularization.\n",
      "Information Geometry Metric Learning (IGML) is shown to be an effective algorithm for distance metric learning. In this paper, we attempt to alleviate two limitations of IGML: (A) the time complexity of IGML increases rapidly for high dimensional data, (B) IGML has to transform the input low rank kernel into a full-rank one since it is undefined for singular matrices. To this end, two novel algorithms, referred to as Efficient Information Geometry Metric Learning (EIGML) and Scalable Information Geometry Metric Learning (SIGML), are proposed. EIGML scales linearly with the dimensionality, resulting in significantly reduced computational complexity. As for SIGML, it is proven to have a range-space preserving property. Following this property, SIGML is found to be capable of handling both full-rank and low-rank kernels. Additionally, the geometric information from data is further exploited in SIGML. In contrast to most existing metric learning methods, both EIGML and SIGML have closed-form solutions and can be efficiently optimized. Experimental results on various data sets demonstrate that the proposed methods outperform the state-of-the-art metric learning algorithms.\n",
      "Cased-based reasoning has been used in decision application with less domain knowledge. This paper presents the novel classification algorithm, which is regarded as the basis of the case-based reasoning application on traffic accident analysis and the early warning system. By using the classified centre vector-based classification algorithm, this method can be used to determine the traffic accident cases' affiliation roughly and as a result, to focus on the relevant retrieval scope. This method has been used in the traffic accident analysis and the corresponding early warning system. The experimental result shows the feasibility of the approach.\n",
      "Cognitive radio networks CRNs have received considerable attention and viewed as a promising paradigm for future wireless networking. Its major difference from the traditional wireless networks is that secondary users are allowed to access the channel if they pose no harmful interference to primary users. This distinct feature of CRNs has raised an essential and challenging question, i.e., how to accurately estimate interference to the primary users from the secondary users? In addition, spectrum sensing plays a critical role in CRNs. Secondary users have to sense the channel before they transmit. A two-state sensing model is commonly used, which classifies a channel into either busy or idle state. Secondary users can only utilize a channel when it is detected to be in idle state. In this paper, we tackle the estimation of interference at the primary receiver due to concurrently active secondary users. With the spectrum sensing, secondary users are refrained from transmitting once an active user falls into their sensing range. As a result, the maximum number of simultaneously interfering secondary users is bounded, typically ranging from 1 to 4. This significant conclusion considerably simplifies interference modeling in CRNs. The authors present all the cases with possible simultaneously interfering secondary users. Moreover, the authors derive the probability for each case. Extensive simulations are conducted and results validate the effectiveness and accuracy of the proposed approach.\n",
      "In [W. Wang and M. K. Ng, SIAM J. Imaging Sci., 6 (2013), pp. 1318--1344], we proposed and developed an image stitching algorithm by studying a variational model for automatically computing weighting mask functions on input images and stitching them together. The main aim of this paper is to further develop an image stitching algorithm using the gradients of input images. Our idea is to study a variational method for computing a stitched image by using an energy functional containing the data-fitting term based on the difference between the gradients of the stitched image and the input images, and the Laplacian regularization term based on the smoothness of weighting mask functions. The use of image gradient information allows us to automatically adjust the stitched image to handle color inconsistency across input images. In the model, we incorporate both boundary conditions of the stitched image and the weighting mask functions. The existence of a solution of the proposed energy functional is shown. We a...\n",
      "Service management is becoming more and more important within the area of IT management. How to efficiently manage and organize service in complicated IT service environments with frequent changes is a challenging issue. IT service and the related information from different sources are characterized as diverse, incomplete, heterogeneous, and geographically distributed. It is hard to consume these complicated services without knowledge assistant. To address this problem, a systematic way (with proposed toolsets and process) is proposed to tackle the challenges of acquisition, structuring, and refinement of structured knowledge. An integrated knowledge process is developed to guarantee the whole engineering procedure which utilizes Bayesian networks (BNs) as the knowledge model. This framework can be successfully applied on key tasks in service management, such as problem determination and change impact analysis, and a real example of Cisco VoIP system is introduced to show the usefulness of this method.\n",
      "Cross-modal matching has recently drawn much attention due to the widespread existence of multimodal data. It aims to match data from different modalities, and generally involves two basic problems: the measure of relevance and coupled feature selection. Most previous works mainly focus on solving the first problem. In this paper, we propose a novel coupled linear regression framework to deal with both problems. Our method learns two projection matrices to map multimodal data into a common feature space, in which cross-modal data matching can be performed. And in the learning procedure, the ell_21-norm penalties are imposed on the two projection matrices separately, which leads to select relevant and discriminative features from coupled feature spaces simultaneously. A trace norm is further imposed on the projected data as a low-rank constraint, which enhances the relevance of different modal data with connections. We also present an iterative algorithm based on half-quadratic minimization to solve the proposed regularized linear regression problem. The experimental results on two challenging cross-modal datasets demonstrate that the proposed method outperforms the state-of-the-art approaches.\n",
      "This paper proposes a multi-task deep neural network (MT-DNN) architecture to handle the multi-label learning problem, in which each label learning is defined as a binary classification task, i.e., a positive class for “an instance owns this label” and a negative class for “an instance does not own this label”. Multi-label learning is accordingly transformed to multiple binary-class classification tasks. Considering that a deep neural nets (DNN) architecture can learn good intermediate representations shared across tasks, we generalize one classification task of traditional DNN into multiple binary classification tasks through defining the output layer with a negative class node and a positive class node for each label. After a similar pretraining process to deep belief nets, we redefine the label assignment error of MT-DNN and perform the back-propagation algorithm to fine-tune the network. To evaluate the proposed model, we carry out image annotation experiments on two public image datasets, with 2000 images and 30,000 images respectively. The experiments demonstrate that the proposed model achieves the state-of-the-art performance.\n",
      "Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the  same  set of instances. Thus instances in different domains can be treated as having strict  one-to-one  relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this paper, we propose a flexible and robust framework, CGC (Co-regularized Graph Clustering), based on non-negative matrix factorization (NMF), to tackle these challenges. CGC has several advantages over the existing methods. First, it supports  many-to-many  cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. Extensive experimental results on UCI benchmark data sets, newsgroup data sets and biological interaction networks demonstrate the effectiveness of our approach.\n",
      "Although previous research has explored factors affecting trust building in websites, little research has been analysed from the perceived interactivity perspective in virtual communities VCs. A research model for verifying interactivity antecedents to trust and its impact on member stickiness behaviour is presented. Two social interactivity components and two system interactivity components are, respectively, theorised as process-based antecedents and institution-based antecedents to trust in the model. Data were collected from 310 members of VCs to test the model. The results show that connectedness and reciprocity are important antecedents to trust in members, while responsiveness and active control are important antecedents to trust in systems. The results also indicate that trust has significant influence on the members’ duration and retention, which are two dimensions of member stickiness measured in this research. These findings have theoretical implications for online interaction-related literature and critical business implications for practitioners of VCs.\n",
      "Given a query string  Q , an edit similarity search finds all strings in a database whose edit distance with  Q  is no more than a given threshold τ. Most existing methods answering edit similarity queries employ schemes to generate string subsequences as signatures and generate candidates by set overlap queries on query and data signatures.   In this article, we show that for any such signature scheme, the lower bound of the minimum number of signatures is τ p 1, which is lower than what is achieved by existing methods. We then propose several asymmetric signature schemes, that is, extracting different numbers of signatures for the data and query strings, which achieve this lower bound. A basic asymmetric scheme is first established on the basis of matching  q -chunks and  q -grams between two strings. Two efficient query processing algorithms (IndexGram and IndexChunk) are developed on top of this scheme. We also propose novel candidate pruning methods to further improve the efficiency. We then generalize the basic scheme by incorporating novel ideas of floating  q -chunks, optimal selection of  q -chunks, and reducing the number of signatures using global ordering. As a result, the Super and Turbo families of schemes are developed together with their corresponding query processing algorithms. We have conducted a comprehensive experimental study using the six asymmetric algorithms and  nine  previous state-of-the-art algorithms. The experiment results clearly showcase the efficiency of our methods and demonstrate space and time characteristics of our proposed algorithms.\n",
      "A great deal of research has been conducted on modeling and discovering communities in complex networks. In most real life networks, an object often participates in multiple overlapping communities. In view of this, recent research has focused on mining overlapping communities in complex networks. The algorithms essentially materialize a snapshot of the overlapping communities in the network. This approach has three drawbacks, however. First, the mining algorithm uses the same global criterion to decide whether a subgraph qualifies as a community. In other words, the criterion is fixed and predetermined. But in reality, communities for different vertices may have very different characteristics. Second, it is costly, time consuming, and often unnecessary to find communities for an entire network. Third, the approach does not support dynamically evolving networks. In this paper, we focus on online search of overlapping communities, that is, given a query vertex, we find meaningful overlapping communities the vertex belongs to in an online manner. In doing so, each search can use community criterion tailored for the vertex in the search. To support this approach, we introduce a novel model for overlapping communities, and we provide theoretical guidelines for tuning the model. We present several algorithms for online overlapping community search and we conduct comprehensive experiments to demonstrate the effectiveness of the model and the algorithms. We also suggest many potential applications of our model and algorithms.\n",
      "Hamming distance measures the number of dimensions where two vectors have different values. In applications such as pattern recognition, information retrieval, and databases, we often need to efficiently process  Hamming distance query , which retrieves vectors in a database that have no more than  k  Hamming distance from a given query vector. Existing work on efficient Hamming distance query processing has some of the following limitations, such as only applicable to tiny error threshold values, unable to deal with vectors where the value domain is large, or unable to attain robust performance in the presence of data skew.   In this paper, we propose HmSearch, an efficient query processing method for Hamming distance queries that addresses the above-mentioned limitations. Our method is based on improved enumeration-based signatures, enhanced filtering, and the hierarchical binary filtering-and-verification. We also design an effective dimension rearrangement method to deal with data skew. Extensive experimental results demonstrate that our methods outperform state-of-the-art methods by up to two orders of magnitude.\n",
      "With the prosperity of the Internet, many advertisers choose to deliver their advertisements by online targeting, where the ad broker is responsible for matching advertisements with users who are likely to be interested in the underlying products or services. However, this online advertisement targeting system requires user profile information and may fail due to privacy issues. In light of growing privacy concerns, we propose a privacy-aware framework for online advertisement targeting, where users are compensated for their privacy leakage and motivated to click more advertisements. In the framework, an ad broker pays a varying amount of money to users for clicking different advertisements due to distinct privacy leakage. Meanwhile advertisers send advertisements to the ad broker and determine the price per user click they need to pay. We model the interactions among advertisers, the ad broker and users as a three-stage game, where every player aims at maximizing its own utility, and Nash Equilibrium is achieved by backward induction. We further analyze the optimal strategies for advertisers, the ad broker and users. Numerical results have shown that the proposed privacy-aware framework is effective as it enables all advertisers, the ad broker and users to maximize their utilities in case of different levels of user privacy sensitivities. In addition, the proposed framework produces higher profits for advertisers and the ad broker than the traditional “paid to click” system.\n",
      "In this paper, we deal with the problem of global tracking and stabilization control of internally damped mobile robots with unknown parameters, and subject to input torque saturation and external disturbances. To overcome the difficulties due to these factors, a new adaptive scheme is proposed to ensure the bounds of the control torques as functions of only design parameters and reference trajectories and thus computable in advance. Then suitable design parameters are determined so that such bounds are within the given saturation limits. To compensate for the disturbances, we estimate their unknown bounds and employ the estimates in controller design. System stability, perfect tracking and stabilization to the origin are established. Simulation studies conducted also verify the effectiveness of the proposed scheme.\n",
      "Abstract   With respect to the problems of position mismatch, oversampling redundancies, distortion and dispersion in range profile caused by radical motion between radar and the interested targets in stepped-frequency radar system, a combined range-velocity estimation method is presented based on the compressive sensing (CS) theory. Considering the sparsity of the interested targets in the illuminated scene by the radar beam, a sparse presence model of the targets is established, and then the range and velocity of the targets are co-estimated directly in the range-velocity plane employing multi-snapshots data, thus the redundancies and mismatch problems existing in traditional method are avoided, besides the output signal-noise-ratio improves dramatically in this case. To reduce the complexity of the computing process, a dynamic sensing matrix is constructed using information acquired from pre-estimated range and velocity, following with which a singular value decomposition (SVD) of the reduced echo data is employed to further reduce the computation load. The effectiveness of the proposed method and robust performance to noise are validated by simulation and experimental results.\n",
      "Anomaly detection in big data is a key problem in the big data analytics domain. In this paper, the definitions of anomaly detection and big data were presented. Due to the sampling and storage burden and the inadequacy of privacy protection of anomaly detection based on uncompressed data, compressive sensing theory was introduced and used in the anomaly detection algorithm. The anomaly detection criterion based on wavelet packet transform and statistic process control theory was deduced. The proposed anomaly detection technique was used for through-wall human detection to demonstrate the effectiveness. The experiments for detecting humans behind a brick wall and gypsum based on ultra-wideband radar signal were carried out. The results showed that the proposed anomaly detection algorithm could effectively detect the existence of a human being through compressed signals and uncompressed data.\n",
      "With the exponential growth of hosts and traffic workloads on the Internet, collaborative web caching has been recognized as an efficient solution to alleviate web page server bottlenecks and reduce traffic. However, cache discovery, i.e., locating where a page is cached, is a challenging problem, especially in the fast growing World Wide Web environment, where the number of participating proxies can be very large. In this paper, we propose a new scheme which employs proxy affinities to maintain a dynamic distributed collaborative caching infrastructure. Web pages are partitioned into clusters according to proxy reference patterns. All proxies which frequently access some page(s) in the same web page cluster form an “information group”. When web pages belonging to a web page cluster are deleted from or added into a proxy's cache, only proxies in the associated information group are notified. This scheme can be shown to greatly reduce the number of messages and other overhead on individual proxies while maintaining a high cache hit rate. Finally, we employ trace driven simulation to evaluate our web caching scheme using three web access trace logs to verify that our caching structure can provide significant benefits on real workloads.\n",
      "Recently, there are many approaches proposed for mining roles using automated technologies. However, it lacks a tool set that can be used to aid the application of role mining approaches and update role states. In this demonstration, we introduce a tool set, RMiner, which is based on the core of WEKA, an open source data mining tool. RMiner implements most of the classic and latest role mining algorithms and provides interactive tools for administrator to update role states. The running examples of RMiner are presented to demonstrate the effectiveness of the tool set.\n",
      "Because of the always connected nature of mobile devices, as well as the unique interfaces they expose, such as short message service (SMS), multimedia messaging service (MMS), and Bluetooth, classes of mobile malware tend to propagate using means unseen in the desktop world. In this paper, we propose a lightweight malware detection system on mobile devices to detect, analyze, and predict malware propagating via SMS and MMS messages. We deploy agents in the form of hidden contacts on the device to capture messages sent from malicious applications. Once captured, messages can be further analyzed to identify a message signature as well as potentially a signature for the malicious application itself. By feeding the observed messages over time to a latent space model, the system can estimate the current dynamics and predict the future state of malware propagation within the mobility network. One distinct feature of our system is that it is lightweight and suitable for wide deployment. The system shows a good performance even when only 10% of mobile devices are equipped with three agents on each device. Moreover, the model is generic and independent of malware propagation schemes. We prototype the system on the Android platform in a universal mobile telecommunications system laboratory network to demonstrate the feasibility of deploying agents on mobile devices as well as collecting and blocking malware-carrying messages within the mobility network. We also show that the proposed latent space model estimates the state of malware propagation accurately, regardless of the propagation scheme. Copyright © 2012 John Wiley & Sons, Ltd.\n",
      "In pervasive environments, availability and reliability of a service cannot always be guaranteed. In such environments, automatic and dynamic mechanisms are required to compose services or compensate for a service that becomes unavailable during the runtime. Most of the existing works on services composition do not provide sufficient support for automatic service provisioning in pervasive environments. We propose a Divide and Conquer algorithm that can be used at the service runtime to repeatedly divide a service composition request into several simpler sub-requests. The algorithm repeats until for each sub-request we find at least one atomic service that meets the requirements of that sub-request. The identified atomic services can then be used to create a composite service. We discuss the technical details of our approach and show evaluation results based on a set of composite service requests. The results show that our proposed method performs effectively in decomposing a composite service requests to a number of sub-requests and finding and matching service components that can fulfill the service composition request.\n",
      "Driven by the increasing demands from applications such as data cleansing, integration, and bioinformatics, approximate string matching queries have gain much attention recently. In this paper, we present the design and implementation of a trie-based system which supports both string similarity search and join based on our recent work [23].\n",
      "The growth of Smartphones has bridged the telephony/SMS and the IP worlds, and this has resulted in new opportunities for financially motivated attackers. For example, some malicious campaigns in the cellular network aimed at extracting money fraudulently can do so even without any malware. Detecting and mitigating the variety of attacks in cellular network is difficult because they do not necessarily have a fixed 'signature', and new  types  of campaigns appear frequently. Further complicating matters, detecting a single malicious entity (a domain name, a phone number, or a short code) that is part of a malicious campaign, is usually not very effective, because the attacker simply moves to using another entity in its place. An effective strategy requires detecting all/most elements involved in the campaign at once. In this paper, we describe a system, based on ideas from anomaly detection and clustering, that aims to detect many different families of widespread malicious campaigns in cellular networks. The system reveals an entire campaign as a graph cluster which includes the various entities involved in the campaign and their relationship, such as malware download websites, C&C servers, spammers, etc. Using logs from both SMS and IP portions of the network for millions of users, we detect newly popular entities and cluster them to discover how they are related. By looking for cues of possible malicious behavior from any of the entities in a cluster, we attempt to ascertain whether a detected campaign might be malicious, providing valuable leads to a human analyst. Our system is live and generates daily clusters for human analysts. We provide detailed case studies of real, previously unseen families of malicious campaigns that this system has successfully brought to light.\n",
      "User profiling plays an important role in online news recommendation systems. In this paper, we analyze the relationship between users' clicking behaviors and the category of the news story to model user's interests by mining web log data of an adaptive news system. We train a Memory-based User Profile (MUP), which imitates human being's learning, remembering and forgetting mechanisms, to predict users' potential interests dynamically. We mainly focus on experimental analysis to refine the MUP scheme. Firstly, we materialize the meanings of all parameters of MUP by important factors (i.e., absorbing factor, forgetting factor, timescale and learning strength) in human being's learning and forgetting process. Secondly, we demonstrate how to determine the values of parameters for different users to reflect their distinct learning and forgetting abilities. Thirdly, we derive a threshold from MUP's recursion formula, which can be used to simply distinguish long-term and short-term interests. Our evaluations are carried out on IdoIcan's web log data, results show MUP can model user's profile effectively.\n",
      "Due to the limited number of orthogonal channels in a multi-radio multi-channel wireless mesh network (MR-WMN), overlapping channel assignment (CA) is one of the main factors that greatly affect the network capacity. In this paper, we first propose a model for measuring achieved network capacity in MR-WMNs. Then we prove that finding an optimal overlapping CA in a given MR-WMN with odd number of channels, is equivalent to finding an optimal assignment by only using its orthogonal channels. This theory allows us to use fewer channels to solve complicated CA problems. Third, we prove that in 802.11b/g-based MR-WMN, the simplified optimization problem is a Max-3-Cut problem. Although this problem is NP-hard, it has an efficient approximation algorithm that achieves approximation ratio of 1.19616 probabilistically by using the algorithm for Max-Cut whose approximation ratio is 1.1383 probabilistically. Based on the algorithm for Max-Cut, this paper proposes Max-Cut-based channel assignment (MCCA) which uses a heuristic method to adjust the result produced by the Max-Cut algorithm to achieve an even better result. Finally, we perform extensive simulations to compare the MCCA with a state-of-the-art Tabu-Search based algorithm. The results show that the Max-Cut-based overlapping CA algorithm effectively improves on the network capacity.\n",
      "Current research environments are witnessing high enormities of presentations occurring in different sessions at academic conferences. This situation makes it difficult for researchers (especially juniors) to attend the right presentation session(s) for effective collaboration. In this paper, we propose an innovative venue recommendation algorithm to enhance smart conference participation. Our proposed algorithm, Social Aware Recommendation of Venues and Environments (SARVE), computes the Pearson Correlation and social characteristic information of conference participants. SARVE further incorporates the current context of both the smart conference community and participants in order to model a recommendation process using distributed community detection. Through the integration of the above computations and techniques, we are able to recommend presentation sessions of active participant presenters that may be of high interest to a particular participant. We evaluate SARVE using a real world dataset. Our experimental results demonstrate that SARVE outperforms other state-of-the-art methods.\n",
      "Two-path successive relaying (TPSR) is an effective way to reduce the multiplex loss induced by the half-duplex operation of the relay node in a conventional relay network. One crucial issue in TPSR network is that the listening relay always suffers inevitable inter-relay interference (IRI), which degrades detection performance at the destination. In this study, a concatenated channel-and-network coding approach is proposed to solve the problem. In particular, a highly flexible channel code, namely, rateless code, is employed at the source to provide resilience to the residual IRI and reduce the retransmissions, which might break the system steady state of successive relaying. Then recognising the special interference structure, physical-layer network coding is incorporated into the forwarding scheme of the relay nodes to exploit network diversity and improve system efficiency. By extrinsic information transfer analysis, the minimum number of required code symbols for successful data recovery are calculated, and degree distribution of the rateless code is optimised.\n",
      "In the Minimum k-Path Connected Vertex Cover Problem (MkPCVCP), we are given a connected graph G and an integer k ? 2, and are required to find a subset C of vertices with minimum cardinality such that each path with length k ? 1 has a vertex in C, and moreover, the induced subgraph G[C] is connected. MkPCVCP is a generalization of the minimum connected vertex cover problem and has applications in many areas such as security communications in wireless sensor networks. MkPCVCP is proved to be NP-complete. In this paper, we give the first polynomial time approximation scheme (PTAS) for MkPCVCP in unit disk graphs, for every fixed k ? 2.\n",
      "In this paper we propose a new distributed scheme to optimize data transmission quality with application level energy neutrality consideration in harvesting wireless sensor networks. The contribution of this proposed scheme is that, the high level data acquisition precision is maximized by properly selecting distributed source coding ancestors. The quality and error robustness tradeoff is adjusted to the ambient harvestable energy in order to improve overall data transmission reliability. We also investigate the effectiveness of applying different power control strategies to different distributed source coding nodes, in order to further improve the overall DSC information gathering quality. Simulation studies demonstrate that the proposed scheme achieves improved data precision with efficient usage of harvestable energy resource.\n",
      "Bounding node-to-sink latency is an important issue of wireless sensor networks WSNs with a quality of service requirement. This paper proposes to deploy multiple sinks to control the worst case node-to-sink data latency in WSNs. The end-to-end latency in multihop wireless networks is known to be proportional to the hop length of the routing path that the message moves over. Therefore, we formulate the question of what is the minimum number of sinks and their locations to bound the latency as the minimum d-hop sink placement problem. We also consider its capacitated version. We show problems are NP-hard in unit disk graph UDG and unit ball graph, and propose constant factor approximations of the problems in both graph models. We further extend our algorithms so that they can work well in more realistic quasi UDG model. A simulation study is also conducted to see the average performance of our algorithms.\n",
      "Similarity search on time series is an essential operation in many applications. In the state-of-the-art methods, such as the R-tree based methods, SAX and iSAX, time series are by default divided into equi-length segments globally, that is, all time series are segmented in the same way. Those methods then focus on how to approximate or symbolize the segments and construct indexes. In this paper, we make an important observation: global segmentation of all time series may incur unnecessary cost in space and time for indexing time series. We develop DSTree, a data adaptive and dynamic segmentation index on time series. In addition to savings in space and time, our new index can provide tight upper and lower bounds on distances between time series. An extensive empirical study shows that our new index DSTree supports time series similarity search effectively and efficiently.\n",
      "To deal with the problem of view invariant action recognition, this paper presents a novel approach to recognize human actions across cameras via reconstructable paths. Each action is modelled as a bag of visual-words based on the spatio-temporal features. Although this action representation is sensitive to view changes, the proposed reconstructable path is able to “translate” the action descriptor of one camera view to that of another camera view. In the learning of the paths, a dictionary is learned under each view to transform the action descriptors into a sparsely represented space, and a linear mapping function is simultaneously learned to bridge the semantic gap between the source and target spaces, such that each domain structure can be fully explored and the discrimination among action categories can be well preserved after translation. Along the reconstructable paths, an unknown action from the target view can be precisely reconstructed into any source view, and thus the SVM classifiers trained in source views are able to recognize this unknown action from target view. The proposed approach is tested on the IXMAS data set, and the experimental results achieve improved accuracy about 7% compared to other existing methods, demonstrating its effectiveness for action recognition across cameras.\n",
      "Memory bandwidth severely limits the scalability and performance of today's multi-core systems. Because of this limitation, many studies that focused on improving multi-core scalability rely on bandwidth usage predictions to achieve the best results. However, existing bandwidth prediction models have low accuracy, causing these studies to have inaccurate conclusions or perform sub-optimally. Most of these models make predictions based on the bandwidth usage samples of a few trial runs. Many factors that affect bandwidth usage and the complex DRAM operations are overlooked. This paper presents DraMon, a model that predicts bandwidth usages for multi-threaded programs with low overhead. It achieves high accuracy through highly accurate predictions of DRAM contention and DRAM concurrency, as well as by considering a wide range of hardware and software factors that impact bandwidth usage. We implemented two versions of DraMon: DraMon-T, a memory-trace based model, and DraMon-R, a run-time model which uses hardware performance counters. When evaluated on a real machine with memory-intensive benchmarks, DraMon-T has average accuracies of 99.17% and 94.70% for DRAM contention predictions and bandwidth predictions, respectively. DraMon-R has average accuracies of 98.55% and 93.37% for DRAM contention and bandwidth predictions respectively, with only 0.50% overhead on average.\n",
      "This paper investigates the fault detection problem for non-uniformly sampled-data systems. No periodic assumption is made for the sampling instants. In contrast to most currently available results that are limited to strictly proper systems, measurement noises are considered. With the operators introduced to capture the inter-sampling behaviors of disturbances and faults, an offline fault detection algorithm is first derived to optimize the ratio-type design objective. It is then equivalently transformed into a recursive algorithm consisting of a discrete time-varying fault detection filter and the corresponding residual evaluation function. As repeated computation of the parity vectors is avoided, the proposed fault detection filter can help reduce the online computational burden with comparison to the existing parity relation based fault detection method.\n",
      "As the demands for mobile phone access to the data services are expanding, advantages in cellular services can be gained by offering enhanced user experience through cost-effective broadband mobile. By reducing the distance between base stations and end users, femtocells provide higher data rates and better indoor coverage. The small size of a femtocell also improves spectrum reuse, which contributes to higher spectrum efficiency. As femtocells are usually unplanned, efficient operation and maintenance are required to maximize the benefits of femtocell access. As one of the fundamental functionalities in network maintenance, a selfhealing mechanism aims to autonomously alleviate the impact of coverage or capacity loss induced by cell outage. Existing studies on the self-healing problem have focused on macrocell networks, while none of them has systematically investigated the problem in the context of femtocells networks. In this article we argue that the distinct features of the two-tier macro-femto system require dedicated architectures for self-healing femtocell networks. We present three different architectures, and further investigate their advantages and limitations. Then we call attention to the local cooperative architecture, which, with proper design, satisfies the practical requirements imposed by the salient features of femtocell networks. We further verify the benefits of the local cooperative architecture by proposing a self-healing scheme for femtocell networks.\n",
      "This paper addresses the problem that devices are reluctant to participate in local device-to-device (D2D) communications in the purpose of improving the data transmission quality. We propose a novel game-theoretic approach of joint source selection and power control, which enhances the multimedia transmission quality with latency constraint. The proposed approach first analyzes the interactions between the base station (BS) and the devices using a Stackelberg game model. The optimal transmission power and price are obtained for each source device through deriving Stackelberg equilibrium off-line, in which the BS and the device both achieve maximum utilities. Second, the BS schedules the devices according to its equilibrium of signal-to-interference-and-noise ratios in D2D links. Finally, the selected best source devices complete the multimedia transmission with their power controlled. Simulations demonstrate that our proposed scheme can significantly improve multimedia transmission quality with low complexity.\n",
      "Wireless information and energy transfer (WIET) is a prominent technology to prolong the lifetime of battery-charging wireless networks. In this paper, we exploit the benefit of massive MIMO for WIET under external interference, and propose the antenna partition for information decoding and energy harvesting. Considering the effects of the external interference, i.e., interfering the information reception and benefiting the energy harvesting, we analyze the tradeoff between the data rate and the harvested energy, and obtain the achievable rate-energy (R-E) region. Then, we propose a low-complexity receive antenna partition algorithm for WIET in massive MIMO systems with the consideration of interference mitigation. The algorithm maximizes the data rate while guaranteeing a minimum harvested energy. It is found that the SNR of the low-complexity algorithm is at least an approximable half of the optimal SNR. Simulation results verify our theoretical claims and show the effectiveness of the proposed low-complexity antenna partition algorithm.\n",
      "Data center interconnected by flexi-grid optical networks is a promising scenario to meet the high burstiness and high bandwidth requirement of data center application, because flexi-grid optical networks can allocate spectral resources for applications in a dynamic, tunable and efficient control manner. Meanwhile, as centralized control architecture, the software-defined networking (SDN) enabled by OpenFlow protocol can provide maximum flexibility for the networks and make a unified control over various resources for the joint optimization of data center and network resources. Time factor is firstly introduced into SDN-based control architecture for flexi-grid optical networks supporting data center application. A traffic model considering time factor is proposed, and a requirement parameter, i.e., bandwidth-period product is adopted for the service requirement measurement. Then, time-sensitive software-defined networking (Ts-SDN)-based control architecture is designed with OpenFlow protocol extension. A novel deadline-driven PCE algorithm is proposed for the deadline-driven service under Ts-SDN-based control architecture, which can complete data center selection, path computation and bandwidth resource allocation. Finally, simulation results shows that our proposed Ts-SDN control architecture and deadline-driven PCE algorithm can improve the application and network performance to a large extent in blocking probability.\n",
      "Cloud computing is a style of computing in which dynamically scalable and other virtualized resources are provided as a service over the Internet. The energy consumption and makespan associated with the resources allocated should be taken into account. This paper proposes an improved clonal selection algorithm based on time cost and energy consumption models in cloud computing environment. We have analyzed the performance of our approach using the CloudSim toolkit. The experimental results show that our approach has immense potential as it offers significant improvement in the aspects of response time and makespan, demonstrates high potential for the improvement in energy efficiency of the data center, and can effectively meet the service level agreement requested by the users.\n",
      "Software Defined Network (SDN) may significantly enhance network and service management by enabling separated control and data planes. The centralized OpenFlow controller with a global vision of network states offers a promising approach to realizing flow-based admission control for supporting Quality of Service (QoS) provisioning in SDN. However, per-flow process brings in challenges to scalability of OpenFlow-based SDN. Flow aggregation has been explored as an effective method to address this issue. In this paper, we investigate admission control with flow aggregation for QoS provisioning in SDN. Specifically we propose a model for admission control with flow aggregation and develop the analysis techniques for determining the required amounts of bandwidth and buffer space at OpenFlow-enabled switches for meeting performance requirements in delay and packet loss. Network calculus is applied in our modeling and analysis; which makes our method applicable to general OpenFlow-based SDNs with various implementations. Numerical experiment results are also provided to evaluate effectiveness of the developed modeling and analysis techniques.\n",
      "Spatial data mining presents new challenges due to the large size of spatial data, the complexity of spatial data types, and the special nature of spatial access methods. Most research in this area has focused on efficient query processing of static data. This paper introduces an active spatial data mining approach that extends the current spatial data mining algorithms to efficiently support user-defined triggers on dynamically evolving spatial data. To exploit the locality of the effect of an update and the nature of spatial data, we employ a hierarchical structure with associated statistical information at the various levels of the hierarchy and decompose the user-defined trigger into a set of subtriggers associated with cells in the hierarchy. Updates are suspended in the hierarchy until their cumulative effect might cause the trigger to fire. It is shown that this approach achieves three orders of magnitude improvement over the naive approach that reevaluate the condition over the database for each update, while both approaches produce the same result without any delay. Moreover, this scheme can support incremental query processing as well.\n",
      "Knowledge unit (KU) is the smallest integral knowledge object in a given domain. Knowledge unit relation recognition is to discover implicit relations among KUs, which is a crucial problem in information extraction. This paper proposes a knowledge unit relation recognition framework based on Markov Logic Networks, which combines probabilistic graphical models and first-order logic by attaching a weight to each first-order formula. The framework is composed principally of structure learning, artificial add or delete formulas, weight learning and inferring. According to the semantic analysis of KUs and their relations, ground predicate set is first extracted. Next, the ground predicate set is inputted into structure learning module to achieve weight formula set. Then, in order to overcome limitations of structure learning, the weight rule set is added or deleted by human. The new weight formula set is turned into weight learning module to acquire the last weight formula set. Finally, knowledge unit relations are recognized by inferring module with the last weight formula set. Experiments on the four data sets related to computer domain show the utility of this approach. The time complexity of structure learning is also analyzed\n",
      "Wikipedia has become one of the best sources for creating and sharing a massive volume of human knowledge. Much effort has been devoted to generating and enriching the structured data by  automatic  information extraction from unstructured text in Wikipedia. Most, if not all, of the existing work share the same paradigm, that is, starting with information extraction over the unstructured text data, followed by supervised machine learning. Although remarkable progresses have been made, this paradigm has its own limitations in terms of effectiveness, scalability as well as the high labeling cost.   We present WiiCluster, a scalable platform for automatically generating infobox for articles in Wikipedia. The heart of our system is an effective  cluster-then-label  algorithm over a rich set of semi-structured data in Wikipedia articles:  linked entities . It is totally unsupervised and thus does not require any human label. It is effective in generating semantically meaningful summarization for Wikipedia articles. We further propose a cluster-reuse algorithm to scale up our system. Overall, our WiiCluster is able to generate nearly 10 million new facts. We also develop a web-based platform to demonstrate WiiCluster, which enables the users to access and browse the generated knowledge.\n",
      "Cascade is a program static analysis tool developed at New York University. Cascade takes as input a program and a control file. The control file specifies one or more assertions to be checked together with restrictions on program behaviors. The tool generates verification conditions for the specified assertions and checks them using an SMT solver which either produces a proof or gives a concrete trace showing how an assertion can fail. Version 2.0 supports the majority of standard C features except for floating point. It can be used to verify both memory safety as well as user-defined assertions. In this paper, we describe the Cascade system including some of its distinguishing features such as its support for different memory models trading off precision for scalability and its ability to reason about linked data structures.\n",
      "Purpose – Extant methods of product weakness detection usually depend on time-consuming questionnaire with high artificial involvement, so the efficiency and accuracy are not satisfied. The purpose of this paper is to propose an opinion-aware analytical framework – PRODWeakFinder – to expect to detect product weaknesses through sentiment analysis in an effective way. Design/methodology/approach – PRODWeakFinder detects product weakness by considering both comparative and non-comparative evaluations in online reviews. For comparative evaluation, an aspect-oriented comparison network is built, and the authority is assessed for each node by network analysis. For non-comparative evaluation, sentiment score is calculated through sentiment analysis. The composite score of aspects is calculated by combing the two types of evaluations. Findings – The experiments show that the comparative authority score and the non-comparative sentiment score are not highly correlated. It also shows that PRODWeakFinder outperform...\n",
      "Recent progress in biology and computer science have generated many complicated networks, most of which can be modeled as large and dense graphs. Developing effective and efficient subgraph match methods over these graphs is urgent, meaningful and necessary. Although some excellent exploratory approaches have been proposed these years, they show poor performances when the graphs are large and dense. This paper presents a novel Subgraph Query technique Based on Clique feature, called SQBC, which integrates the carefully designed clique encoding with the existing vertex encoding [40] as the basic index unit to reduce the search space. Furthermore, SQBC optimizes the subgraph isomorphism test based on clique features. Extensive experiments over biological networks, RDF dataset and synthetic graphs have shown that SQBC outperforms the most popular competitors both in effectiveness and efficiency especially when the data graphs are large and dense.\n",
      "Scalability is an important performance metric of parallel computing, but the traditional scalability metrics only try to reflect the scalability for parallel computing from one side, which makes it difficult to fully measure its overall performance. This paper studies scalability metrics intensively and completely. From lots of performance parameters of parallel computing, a group of key ones is chosen and normalized. Further the area of Kiviat graph is used to characterize the overall performance of parallel computing. Thereby a novel scalability metric about iso-area of performance for parallel computing is proposed and the relationship between the new metric and the traditional ones is analyzed. Finally the novel metric is applied to address the scalability of the matrix multiplication Cannon’s algorithm under LogP model. The proposed metric is significant to improve parallel computing architecture and to tune parallel algorithm design.\n",
      "Networked predictive control (NPC) is an effective technique to compensate for transmission delays and packet dropout in networked control systems (NCS). By introducing an auxiliary error variable, in this brief, the NCS with NPC is transformed to a coupled switched system. Then, based on the switched system theory and the small gain theorem, a new stability criterion is developed under the restrictions of unstable sampling point rate. As a special case that all switched systems are Schur stable, the proposed criterion is equivalent to existing ones. The proposed stability criterion is much less conservative than existing ones, as shown from both theory and numerical simulations.\n",
      "This paper proposes a novel energy management strategy (EMS) of an onboard supercapacitor (SC) for subway applications with a permanent-magnet (PM) traction system. The studied subway train is modeled, and its control is developed, in which the flux-weakening operation is taken into account to minimize the copper loss. The proposed EMS tries to minimize the actual flux-weakening region. Compared with the conventional EMS, the proposed EMS is realized by increasing the dc bus voltage to minimize the demagnetizing d-axis current, hence copper loss, in the flux-weakening region instead of reducing the dc bus voltage drop during startup. An experimental platform using the hardware-in-the-loop (HIL) simulation method is developed, and experiments are carried out to verify the effectiveness of the proposed EMS.\n",
      "High-resolution imaging for squint azimuth-variant bistatic synthetic aperture radar system is a challenging task due to the existence of the spatial variance of range cell migration (RCM) and Doppler frequency modulation (FM) rate. To address this problem, azimuth nonlinear chirp scaling (ANLCS) is investigated in this letter. First, linear range walk is removed and then ANLCS is applied in the range frequency azimuth time domain to correct the azimuth-variant RCMs and to equalize the different FM rates. Taking the 2-D variance caused by the azimuth-variant configuration into consideration, a new perturbation function is derived based on the bistatic geometry. Using method of series reversion, a close form of range-azimuth coupling is obtained and corrected in the range Doppler domain by an interpolation-free operation. Incorporated with the secondary range compression, this method leads to a more accurate focusing for azimuth-variant bistatic configurations, even with high squints. Simulation results validate the effectiveness of the method.\n",
      "A 10-Gb/s serial link transceiver is demonstrated using double-edged pulsewidth modulation to overcome frequency-dependent losses in electrical interconnects. Time domain modulation is discussed as a means to enhance the spectral efficiency in channels with sharp frequency roll-off similar to multilevel voltage-domain modulation such as 4-PAM. The transmitter and receiver are high-speed programmable digital-to-time and time-to-digital converters that adapt to channel bandwidth characteristics with a timing resolution of 40 ps. This paper presents a low-jitter, phase rotation architecture for cycle-to-cycle transmit pulsewidth control. The transceiver includes an elastic buffer to move data between synchronous and plesiochronous clock domains and is implemented in 45-nm CMOS SOI. Transmitter and receiver functionality is demonstrated to 10 Gb/s at a BER of under 10 -12  and is compared against NRZ schemes at the same rate. The inductor-less transmitter and receiver active circuitry respectively occupy an area of 93 × 94 and 218 × 160 μm 2 , and consume a total 107 mW from a 1.2 V supply.\n",
      "A sparse coding based framework is proposed for human action recognition.The proposed CS-Mltp descriptor performs better than other descriptors on RGB videos.The proposed framework significantly outperforms the state-of-the-art algorithms.The feature- and classifier-level fusions of color and depth information are explored. The recently released low-cost Kinect opens up new opportunities to research in human action recognition, by providing both the color images and depth maps. However, how to exploit and fuse useful features from these various sources remains a very challenging problem. In this paper, we propose a novel and effective framework to largely improve the performance of human action recognition using both the RGB videos and depth maps. The key contribution is the proposition of the sparse coding-based temporal pyramid matching approach (ScTPM) for feature representation. Due to the pyramid structure and sparse representation of extracted features, temporal information is well kept and approximation error is reduced. In addition, a novel Center-Symmetric Motion Local Ternary Pattern (CS-Mltp) descriptor is proposed to capture spatial-temporal features from RGB videos at low computational cost. Using the ScTPM-represented 3D joint features and CS-Mltp features, both feature-level fusion and classifier-level fusion are explored that further improves the recognition accuracy. We evaluate the proposed feature extraction, representation, classification and fusion framework on two challenging human action datasets, MSR-Action3D and MSR-DailyActivity3D. Experimental results indicate that our approaches are repeatedly superior to state-of-the-art methods by 6 % and 7 % on the two datasets, respectively.\n",
      "The secure operation of autonomous vehicle networks in the presence of adversarial observation is examined, in the context of a canonical double-integrator-network (DIN) model. Specifically, we study the ability of a sentient adversary to estimate the full network's state, from noisy local measurements of vehicle motions. Algebraic, spectral, and graphical characterizations are provided, which indicate the critical role of the inter-vehicle communication topology and control scheme in achieving security.\n",
      "In spite of the truly remarkable diversity of models of time series, there is still an evident need to develop constructs whose accuracy and interpretability are carefully identified and reconciled subsequently leading to highly interpretable (human-centric) constructs. While a great deal of research has been devoted to the design of nonlinear numeric models of time series (with an evident objective to achieve high accuracy of prediction), an issue of interpretability (transparency) of models of time series becomes an evident and ongoing challenge. The user-friendliness of models of time series comes with an ability of humans to perceive and process abstract constructs rather than dealing with plain numeric entities. In perception of time series, information granules (which are regarded as realizations of interpretable entities) play a pivotal role. This gives rise to a concept of granular models of time series or granular time series, in brief. This study revisits generic concepts of information granules and elaborates on a fundamental way of forming information granules (both sets—intervals as well as fuzzy sets) through applying a principle of justifiable granularity encountered in granular computing. Information granules are discussed with regard to the granulation of time series in a certain predefined representation space (viz. a feature space) and granulation carried out in time. The granular representation and description of time series is then presented. We elaborate on the fundamental hierarchically organized layers of processing supporting the development and interpretation of granular time series, namely (a) formation of granular descriptors used in their visualization, (b) construction of linguistic descriptors used afterwards in the generation of (c) linguistic description of time series. The layer of the linguistic prediction models of time series exploiting the linguistic descriptors is outlined as well. A number of examples are offered throughout the entire paper with intent to illustrate the main functionalities of the essential layers of the granular models of time series.\n",
      "This paper proposes a model to calculate the average speed of transmission of intervehicle communication (IVC) messages in a general traffic stream on highways in the early stage of deploying distributed traffic information systems (DTIS). The model helps explain the relationship between average IVC message speed and traffic parameters such as equipped vehicle density, traffic flow speed, and traffic direction. Simulation results are used to verify the correctness of the model. This model needs a much shorter calculation time than a simulation. Moreover, the theoretical analysis helps provide more insightful explanations of the phenomenon for IVC performance analysis. The results of this paper would help people better understand the design criteria for DTIS.\n",
      "Market-driven spectrum auctions offer an efficient way to improve spectrum utilization by transferring unused or underused spectrum from its primary license holder to spectrum-deficient secondary users. Such a spectrum market exhibits strong locality in two aspects: 1) that spectrum is a local resource and can only be traded to users within the license area, and 2) that holders can partition the entire license areas and sell any pieces in the market. We design a spectrum double auction that incorporates such locality in spectrum markets, while keeping the auction economically robust and computationally efficient. Our designs are tailored to cases with and without the knowledge of bid distributions. Complementary simulation studies show that spectrum utilization can be significantly improved when distribution information is available. Therefore, an auctioneer can start from one design without any a priori information, and then switch to the other alternative after accumulating sufficient distribution knowledge. With minor modifications, our designs are also effective for a profit-driven auctioneer aiming to maximize the auction revenue.\n",
      "We study the multi-resource allocation problem in cloud computing systems where the resource pool is constructed from a large number of heterogeneous servers, representing different points in the configuration space of resources such as processing, memory, and storage. We design a multi-resource allocation mechanism, called DRFH, that generalizes the notion of Dominant Resource Fairness (DRF) from a single server to multiple heterogeneous servers. DRFH provides a number of highly desirable properties. With DRFH, no user prefers the allocation of another user; no one can improve its allocation without decreasing that of the others; and more importantly, no user has an incentive to lie about its resource demand. As a direct application, we design a simple heuristic that implements DRFH in real-world systems. Large-scale simulations driven by Google cluster traces show that DRFH significantly outperforms the traditional slot-based scheduler, leading to much higher resource utilization with substantially shorter job completion times.\n",
      "In order to fulfill users' insatiable interests in accessing Internet services and information wirelessly, one of the key optimization techniques is caching frequently accessed data items in a local cache. A strong consistency is implicitly assumed in most caching schemes but it may cause a long query delay. In this paper, we propose a consistency-sensitive cache invalidation scheme, called ConSens, based on the existing Invalidation Report (IR) and Updated IR (UIR) based cache invalidation frameworks. In the ConSens scheme, each user is able to set its own consistency level with a server independently. This user-defined cache consistency can support diverse consistency requirements of applications. We also propose both lazy request and opportunistic data access techniques to effectively balance the data accessibility and query delay. In addition, we enhance the IR-based cache invalidation mechanism and propose a multiple data transmission scheme, called MDT, to further reduce the query delay. Extensive performance evaluation studies show that the proposed strategies can effectively balance the data accessibility, reduce the query delay, and significantly increase the number of opportunistic accesses according to the user-defined consistencies.\n",
      "Massive computation power and storage capacity of cloud computing systems allow scientists to deploy data-intensive applications without the infrastructure investment, where large application datasets can be stored in the cloud. Based on the pay-as-you-go model, data placement strategies have been developed to cost-effectively store large volumes of generated datasets in the scientific cloud workflows. As promising as it is, this paradigm also introduces many new challenges for data security when the users outsource sensitive data for sharing on the cloud servers, which are not within the same trusted domain as the data owners. This challenge is further complicated by the security constraints on the potential sensitive data for the scientific workflows in the cloud. To effectively address this problem, we propose a security-aware intermediate data placement strategy. First, we build a security overhead model to reasonably measure the security overheads incurred by the sensitive data. Second, we develop a data placement strategy to dynamically place the intermediate data for the scientific workflows. Finally, our experimental results show that our strategy can effectively improve the intermediate data security while ensuring the data transfer time during the execution of scientific workflows.\n",
      "This paper is concerned with the problem of stabilization for positive switched delay systems. A hysteresis switching law is designed to stabilize positive switched systems with a time delay. On one hand, such switching laws can eliminate the chattering resulting from state-dependent switching laws. On the other hand, a constant time delay may sometimes lead to the instability of positive switched systems. Sufficient conditions on the existence of hysteresis switching laws are presented by making use of linear programming approach. The validity of the proposed approaches is illustrated by a numerical example.\n",
      "Recently, many researchers have used graph theory to study the aberrant brain functions in mental disorders. However, the characteristics of the brain functional network in attention deficit hyperactivity disorder (ADHD) are still largely unexplored. In this study, blood oxygen level-dependence (BOLD) functional magnetic resonance images (fMRI) were employed to construct brain functional networks in 57 children with ADHD and 59 healthy controls (HC). The results showed that both groups had similar global efficiency and the ADHD group had significantly decreased local efficiency compared with the HC. The between-group differences of degree values were found in the frontal cortex, temporal cortex and the default mode network (DMN). The ADHD group retained most of the hub regions found in the HC, but showed altered hub regions in the left posterior cingulate gyrus and the right lingual gyrus. This altered topological organization of the functional network might be associated with the underlying pathophysiology in ADHD.\n",
      "Pseudogenes have long been considered to be nonfunctional segments in the genome, but recent studies have provided evidence to support their novel regulatory roles in biological processes. With the growing interests in pseudogene research, scientists rely on RNA sequencing technology to estimate expression level of pseudogenes at different tissues or cell lines. The major challenge of RNASeq on pseudogene quantification falls in the high sequence similarity between pseudogenes and their homologous parents. Reads can be ambiguously aligned to multiple homologous regions. In this article, we present PseudoLasso, a genome-wide approach to accurately estimate the abundance of pseudogenes and their parents, and correctly align reads to their origins. Our approach focuses on learning read alignment behaviors, and leveraging this knowledge for abundance estimation and alignment correction. Compared to the read count estimates reported by TopHat2, PseudoLasso is able to provide estimates with a reduced error rate of 10-fold.\n",
      "Android has been a major target of malicious applications (malapps). How to detect and keep the malapps out of the app markets is an ongoing challenge. One of the central design points of Android security mechanism is permission control that restricts the access of apps to core facilities of devices. However, it imparts a significant responsibility to the app developers with regard to accurately specifying the requested permissions and to the users with regard to fully understanding the risk of granting certain combinations of permissions. Android permissions requested by an app depict the app’s behavioral patterns. In order to help understanding Android permissions, in this paper, we explore the permission-induced risk in Android apps on three levels in a systematic manner. First, we thoroughly analyze the risk of an individual permission and the risk of a group of collaborative permissions. We employ three feature ranking methods, namely, mutual information, correlation coefficient, and T-test to rank Android individual permissions with respect to their risk. We then use sequential forward selection as well as principal component analysis to identify risky permission subsets. Second, we evaluate the usefulness of risky permissions for malapp detection with support vector machine, decision trees, as well as random forest. Third, we in depth analyze the detection results and discuss the feasibility as well as the limitations of malapp detection based on permission requests. We evaluate our methods on a very large official app set consisting of 310 926 benign apps and 4868 real-world malapps and on a third-party app sets. The empirical results show that our malapp detectors built on risky permissions give satisfied performance (a detection rate as 94.62% with a false positive rate as 0.6%), catch the malapps’ essential patterns on violating permission access regulations, and are universally applicable to unknown malapps (detection rate as 74.03%).\n",
      "In this letter, a novel robust scheme with antenna array and global positioning system/strapdown inertial navigation system (GPS/SINS) is proposed for improving the anti-jamming capability of navigation receiver. The key idea is that the apriori knowledge for beamforming is supplied by satellite almanac and tightly coupled GPS/SINS. Then a multiple constrained minimum variance-spatial temporal adaptive processing (MCMV-STAP) beamforming algorithm is proposed for enhancing satellite signals and suppressing interferences. Finally, desired satellite signals without interferences are used to calculate satellite pseudoranges, which are introduced to the coupled GPS/SINS filter. Simulation results show that the anti-jamming capability of the proposed scheme is reliable and robust.\n",
      "Nose is one of the salient features in a human face, and its localization is important for face recognition, face pose recognition, 3D face reconstruction, etc. In this paper, a novel nose tip localization method is proposed, which is based on two-stage subclass discriminant analysis (SDA). At the first stage, some randomly selected image patches are used as negative samples for the training of SDA classifier, and nose is detected from the whole face image. The second stage refines nose tip position by using some nose context patches as negative samples for the training of SDA classifier. The proposed method detects nose from the whole face image and no a priori knowledge about the layout of face components is used. Experimental results on AR images show that the proposed method can achieve high nose tip localization rates, and is robust to changes of illumination and facial expression.\n",
      "In this work we consider a Transportation Location Routing Problem (TLRP) that can be seen as an extension of the two stage Location Routing Problem, in which the first stage corresponds to a transportation problem with truck capacity. Two objectives are considered in this research, reduction of distribution cost and balance of workloads for drivers in the routing stage. Here, we present a mathematical formulation for the bi-objective TLRP and propose a new representation for the TLRP based on priorities. This representation lets us manage the problem easily and reduces the computational effort, plus, it is suitable to be used with both local search based and evolutionary approaches. In order to demonstrate its efficiency, it was implemented in two metaheuristic solution algorithms based on the Scatter Tabu Search Procedure for Non-Linear Multiobjective Optimization (SSPMO) and on the Non-dominated Sorting Genetic Algorithm II (NSGA-II) strategies. Computational experiments showed efficient results in solution quality and computing time.\n",
      "In silicon testing, a Shmoo plot is commonly used to give us an insight into the silicon manufacturing development health. Shmoo plots and other silicon characterization data has high value, however, analysis of them is a time-consuming work. This paper establishes a machine learning based model to improve and automate the procedure in silicon data analysis for HVM test content development. Our experiment shows that the supervised learning model has good accuracy on VMIN estimation across various kinds of Shmoo issues (crack/sprinkle/ceiling). The accuracy attained is greatly improved over previous tools. The framework can be easily integrated into any automated tester software and would save time to market during first silicon characterization. Additionally, the methodology discussed in this work can be extended to the HVM test flow for silicon behavior.\n",
      "To upgrade the systems or fix the security issues, some physical machines (PMs) in data centers are required to undergo a maintenance process, which might disable the continuous services of the virtual machines (VMs) run on them for a few time slots. To reduce the waiting delay, one may migrate the VMs to other active PMs. However, it will incur extra migration cost, e.g., bandwidth or memory used to move data. To balance the tradeoff between delay and migration cost, we formulate a two-objective optimization problem, which minimizes both delay and migration cost according to a certain weightage, so as to decide whether the VMs should be migrated to other active PMs or should wait their own maintained PMs to be back. We first prove that the proposed problem is NP-hard. For a special case, where each VM requires the same size of resource, we show that the defined problem can be converted to minimum weighted bipartite matching problem in an auxiliary bipartite graph. A lower bound of the delay is derived for a specific setting. For the general case of the problem, we also design an efficient heuristic algorithm. Finally, simulation results demonstrate the effectiveness of the proposed scheme.\n",
      "Protein-DNA interactions are essential for many biological processes. However, the structural mechanisms underlying these interactions are not fully understood. DNA binding proteins can be classified into double-stranded DNA binding proteins (DSBs) and single-stranded DNA binding proteins (SSBs), and they take part in different biological functions. DSBs usually act as transcriptional factors to regulate the genes' expressions, while SSBs usually play roles in DNA replication, recombination, and repair, etc. Understanding the binding specificity of a DNA binding protein is helpful for the research of protein functions.\n",
      "Compressive sensing can minimize the collection of redundant data in the acquisition step. However, it requires a huge amount of storage and creates a tremendous computation burden due to the size of random measurement matrix in compressive sensing theory for big data collection. The separable compressive sensing theory uses two-dimensional separable random measurement matrixes instead of a huge size of random matrix to remedy sensing matrix storage and computation complexity. In this paper, we proposed an intelligent throat polyp detection with singular value decomposition and support vector machine algorithms based on the vowel/a:/and/i:/voices. We compared the detection effects of the proposed intelligent detection method between original voice signals and compressed signals which were collected by separable compressive sensing theory. The experimental results showed that the matrix size of original vowel voices signal could affect the correct rate of prediction. Also, the correct rate of prediction was stable under different random measurement matrix and different compressed ratio.\n",
      "The proliferation of sensor-equipped smartphones has enabled an increasing number of context-aware applications that provide personalized services based on users' contexts. However, most of these applications aggressively collect users sensing data without providing clear statements on the usage and disclosure strategies of such sensitive information, which raises severe privacy concerns and leads to some initial investigation on privacy preservation mechanisms design. While most prior studies have assumed static adversary models, we investigate the context dynamics and call attention to the existence of intelligent adversaries. In this paper, we first identify the context privacy problem with consideration of the context dynamics and malicious adversaries with capabilities of adjusting their attack- ing strategies, and then formulate the interactive competition between users and adversaries as a zero-sum stochastic game. In addition, we propose an efficient minimax learning algorithm to obtain the optimal defense strategy. Our evaluations on real smartphone context traces of 94 users validate the proposed algorithm.\n",
      "Since microblog service became information provider on web scale, research on microblog has begun to focus more on its content mining. Most research on microblog context is often based on topic models, such as: Latent Dirichlet Allocation(LDA) and its variations. However,there are some challenges in previous research. On one hand, the number of topics is fixed as a priori, but in real world, it is input by the users. On the other hand, it ignores the hierar- chical information of topics and cannot grow structurally as more data are ob- served. In this paper, we propose a semi-supervised hierarchical topic model, which aims to explore more reasonable topics in the data space by incorporating some constraints into the modeling process that are extracted automatically. The new method is denoted as constrained hierarchical Latent Dirichlet Allocation (constrained-hLDA). We conduct experiments on Sina microblog, and evaluate the performance in terms of clustering and empirical likelihood. The experimen- tal results show that constrained-hLDA has a significant improvement on the in- terpretability, and its predictive ability is also better than that of hLDA. In the information explosion era, social network not only contains relationships, but also much unstructured information such as context. Furthermore, how to effectively dig out latent topics and internal semantic structures from social network is an im- portant research issue. Early work on microblogs mainly focused on user relationship and community structure. (1) studied the topological and geographical properties of Twitter. Others work such as (2) studied user behaviors and geographic growth pat- terns of Twitter. Only little research on content analysis of microblog was proposed recently. (3) was mainly based on traditional text mining algorithms. (4) proposed MB- LDA by overall considering contactor relevance relation and document relevance re- lation of microblogs. In this paper, we propose a novel probabilistic generative model based on hLDA, called constrained-hLDA, which focuses on both text content and topic hierarchy.\n",
      "Mobile devices such as smartphones are widely deployed in the world, and many people use them to download/upload media such as video and pictures to remote servers. On the other hand, a mobile device has limited resources, and some media processing tasks must be migrated to the media cloud for further processing. However, a significant question is, can mobile users trust the media services provided by the media cloud service providers? Many traditional security approaches are proposed to secure the data exchange between mobile users and the media cloud. However, first, because multimedia such as video is large-sized data, and mobile devices have limited capability to process media data, it is important to design a lightweight security method; second, uploading and downloading multi-resolution images/videos make it difficult for the traditional security methods to ensure security for users of the media cloud. Third, the error-prone wireless environment can cause failure of security protection such as authentication. To address the above challenges, in this article, we propose to use both secure sharing and watermarking schemes to protect user's data in the media cloud. The secure sharing scheme allows users to upload multiple data pieces to different clouds, making it impossible to derive the whole information from any one cloud. In addition, the proposed scalable watermarking algorithm can be used for authentications between personal mobile users and the media cloud. Furthermore, we introduce a new solution to resist multimedia transmission errors through a joint design of watermarking and Reed- Solomon codes. Our studies show that the proposed approach not only achieves good security performance, but also can enhance media quality and reduce transmission overhead.\n",
      "In this paper, a new radiation pattern reconfigurable antenna is designed for mobile relay station (MRS) on vehicles to increase the link reliability and system flexibility in mobility. With only a single radio frequency (RF) chain, various beam patterns can be formed and selected electronically in a simple way. The design and corresponding beam switching mechanism are demonstrated through an experimental prototype with improved SINR performance, particularly at the cell boundary. Furthermore, its advantage in handover is confirmed through numerical simulations. The handover success rate increases significantly in high speed mobility.\n",
      "Motivation: Next-generation RNA sequencing offers an opportunity to investigate transcriptome in an unprecedented scale. Recent studies have revealed widespread alternative polyadenylation (polyA) in eukaryotes, leading to various mRNA isoforms differing in their 3′ untranslated regions (3′UTR), through which, the stability, localization and translation of mRNA can be regulated. However, very few, if any, methods and tools are available for directly analyzing this special alternative RNA processing event. Conventional methods rely on annotation of polyA sites; yet, such knowledge remains incomplete, and identification of polyA sites is still challenging. The goal of this article is to develop methods for detecting 3′UTR switching without any prior knowledge of polyA annotations.#R##N##R##N#Results: We propose a change-point model based on a likelihood ratio test for detecting 3′UTR switching. We develop a directional testing procedure for identifying dramatic shortening or lengthening events in 3′UTR, while controlling mixed directional false discovery rate at a nominal level. To our knowledge, this is the first approach to analyze 3′UTR switching directly without relying on any polyA annotations. Simulation studies and applications to two real datasets reveal that our proposed method is powerful, accurate and feasible for the analysis of next-generation RNA sequencing data.#R##N##R##N#Conclusions: The proposed method will fill a void among alternative RNA processing analysis tools for transcriptome studies. It can help to obtain additional insights from RNA sequencing data by understanding gene regulation mechanisms through the analysis of 3′UTR switching.#R##N##R##N#Availability and implementation: The software is implemented in Java and can be freely downloaded from http://utr.sourceforge.net/.#R##N##R##N#Contact: ude.tijn@iewihz or ude.nnepu.dem.liam@ehzgnoh#R##N##R##N#Supplementary information: Supplementary data are available at Bioinformatics online.\n",
      "Let r and k be positive integers. A graph G is r-equitably k-colorable if its vertex set can be partitioned into k independent sets, any two of which differ in size by at most r. The r-equitable chromatic threshold of a graph G, denoted by @g\"r\"=^*(G), is the minimum k such that G is r-equitably k^'-colorable for all k^'>=k. Let GxH denote the Kronecker product of graphs G and H. In this paper, we completely determine the exact value of @g\"r\"=^*(K\"mxK\"n) for general m,n and r. As a consequence, we show that for r>=2, if n>=1r-1(m+r)(m+2r-1) then K\"mxK\"n and its spanning supergraph K\"m\"(\"n\") have the same r-equitable colorability, and in particular @g\"r\"=^*(K\"mxK\"n)=@g\"r\"=^*(K\"m\"(\"n\")), where K\"m\"(\"n\") is the complete m-partite graph with n vertices in each part.\n",
      "Accurate focusing of highly squinted azimuth-variant bistatic synthetic aperture radar data is a difficult issue due to the relatively large range migration, sensibility of the higher order terms, and the inherent geometric variance. To accommodate for this problem, extended azimuth nonlinear chirp scaling algorithm is investigated in this letter. First, range-azimuth coupling is mitigated through a linear range walk correction operation, and then, bulk secondary range compression is implemented to compensate the residual range cell migration and cross-coupling terms. Following which, the characteristics of the azimuth-dependent quadratic and cubic phase terms are analyzed, and modified scaling coefficients are derived by adopting higher order approximation and incorporating the azimuth-dependent range offset caused by the inherent geometric configuration. Compared with traditional nonlinear chirp scaling method, large azimuth depth of focusing can be realized without changing the overall procedure. Simulation results validate the effectiveness of the proposed algorithm.\n",
      "Most of the quantization based watermarking algorithms are very sensitive to valumetric distortions, while these distortions are regarded as common processing in audio/video analysis. In recent years, watermarking methods which can resist this kind of distortions have attracted a lot of interests. But still many proposed methods can only deal with one certain kind of valumetric distortion as amplitude scaling, and fail in other kinds of valumetric distortions like constant change attack or gamma correction. In this paper, we propose a very simple method to tackle all the three kinds of valumetric distortions. A constant change invariant domain is first constructed by spread transform, in which the watermark is embedded using a certain amplitude scaling invariant based watermarking scheme. Several typical watermarking methods and attacks have been implemented in our experiments to demonstrate the effectiveness of the proposed method.\n",
      "Pan-sharpening is an approach that fuse low resolution multi-spectral (LRMS) images with a high spatial detail of panchromatic (PAN) image to obtain the high resolution multispectral (HRMS) images. In this paper, we present a compressed sensing-based pan-sharpening method that include joint data fidelity and blind blurring kernel estimation. The joint data fidelity contain following three fidelity terms: (1) the LRMS images could be the decimated form of the HRMS images by convolving a blurring kernel, (2) the gradient of HRMS images in the spectrum direction could be proximity to those of the LRMS images, (3) the high frequency part of linear combination of HRMS image bands is approximate to the corresponding parts of the PAN image. Different from other methods which simply apply average blurring kernel for pan-sharpening, a blind deconvolution algorithm is introduced to estimate the blurring kernel from different satellites respectively. We also include a novel anisotropic total variation (TV) prior term to better reconstruct the image edges. The alternating direction method of multipliers (ADMM) is used to solve the proposed model efficiently. Finally, a Pleiades satellite image is employed to demonstrate that the proposed method achieve effective and efficient results simultaneously compared with other existing methods.\n",
      "Methods for extracting quantitative information regarding nuclear morphology from histopathology images have been long used to aid pathologists in determining the degree of differentiation in numerous malignancies. Most methods currently in use, however, employ the naive Bayes approach to classify a set of nuclear measurements extracted from one patient. Hence, the statistical dependency between the samples (nuclear measurements) is often not directly taken into account. Here we describe a method that makes use of statistical dependency between samples in thyroid tissue to improve patient classification accuracies with respect to standard naive Bayes approaches. We report results in two sample diagnostic challenges.\n",
      "The problem of efficiently finding top-k frequent items has attracted much attention in recent yeras. Storage constraints in the processing node and intrinsic evloving feature of the data streams are two main challenges. In this paper, we propose a method to tackle these two challenges based on space-saving and gossip-based algorithms respectively. Our method is implemented on SAMOA, a scalable advanced massive online analysis machine learning framework. The experimental results show its effectiveness and scalability.\n",
      "Cross-media hashing, which conducts cross-media retrieval by embedding data from different modalities into a common low-dimensional hamming space, has attracted intensive attention in recent years. This is motivated by the facts a) the multi-modal data is widespread, e.g., the web images on Flickr are associated with tags, and b) hashing is an effective technique towards large-scale high-dimensional data processing, which is exactly the situation of cross-media retrieval. Inspired by recent advances in deep learning, we propose a cross-media hashing approach based on multi-modal neural networks. By restricting in the learning objective a) the hash codes for relevant cross-media data being similar, and b) the hash codes being discriminative for predicting the class labels, the learned Hamming space is expected to well capture the cross-media semantic relationships and to be semantically discriminative. The experiments on two real-world data sets show that our approach achieves superior cross-media retrieval performance compared with the state-of-the-art methods.\n",
      "In this research, a new type of manufacturing feature that is commonly observed in aircraft structural parts, known as ribs, is defined and implemented using the object-oriented software engineering approach. The rib feature type is defined as a set of constrained and adjacent faces of a part which are associated with a set of specific rib machining operations. Computerized numerical control CNC operation experience and the machining knowledge are leveraged by analysing typical geometry interactions when generating machining tool paths where such knowledge and experience are abstracted as rules of process planning. Then those abstracted machining process rules are implemented in a feature recognition algorithm on top of an existing and holistic attribute adjacency graph solution to extract seed faces, identify individual local rib elements and further cluster these newly identified local rib elements into groups for the ease of machining operations. Out of the potentially different combinations of local rib elements, those optimised cluster groups are merged into the top-level rib features. The enhanced recognition algorithm is presented in details. A pilot system has already been developed and applied for machining many advanced aircraft structural parts in a large aircraft manufacturer. Observations and conclusions are presented at the end.\n",
      "The surging deployment of WiFi hotspots in public places drives the blossoming of location-based services (LBSs) available. A recent measurement reveals that a large portion of the reported locations are either forged or superfluous, which calls attention to location authentication. However, existing authentication approaches breach user's location privacy, which is of wide concern of both individuals and governments. In this paper, we propose PriLA, a privacy-preserving location authentication protocol that facilitates location authentication without compromising user's location privacy in WiFi networks. PriLA exploits physical layer information, namely carrier frequency offset (CFO) and multipath profile, from user's frames. In particular, PriLA leverages CFO to secure wireless transmission between the mobile user and the access point (AP), and meanwhile authenticate the reported locations without leaking the exact location information based on the coarse-grained location proximity being extracted from user's multipath profile. Existing privacy preservation techniques on upper layers can be applied on top of PriLA to enable various applications. We have implemented PriLa on GNURadio/USRP platform and off-the-shelf Intel 5300 NIC. The experimental results demonstrate the practicality of CFO injection and accuracy of multipath profile based location authentication in a real-world environment.\n",
      "Similarity query is a fundamental problem in database, data mining and information retrieval research. Recently, querying incomplete data has attracted extensive attention as it poses new challenges to traditional querying techniques. The existing work on querying incomplete data addresses the problem where the data values on certain dimensions are unknown. However, in many real-life applications, such as data collected by a sensor network in a noisy environment, not only the data values but also the dimension information may be missing. In this work, we propose to investigate the problem of similarity search on dimension incomplete data. A probabilistic framework is developed to model this problem so that the users can find objects in the database that are similar to the query with probability guarantee. Missing dimension information poses great computational challenge, since all possible combinations of missing dimensions need to be examined when evaluating the similarity between the query and the data objects. We develop the lower and upper bounds of the probability that a data object is similar to the query. These bounds enable efficient filtering of irrelevant data objects without explicitly examining all missing dimension combinations. A probability triangle inequality is also employed to further prune the search space and speed up the query process. The proposed probabilistic framework and techniques can be applied to both whole and subsequence queries. Extensive experimental results on real-life data sets demonstrate the effectiveness and efficiency of our approach.\n",
      "This paper presents a low-complexity lossless screen content coding method on top of a well structured dictionary based solution DictSCC. Both methods are implemented on the emerging High-Efficiency Video Coding (HEVC) Range Extensions (RExt) to demonstrate the coding efficiency. Compared with the DictSCC our scheme further explores the 2D correlation between neighboring image blocks for performance improvement, and reduces the complexity significantly by constraining the dictionary buffer which will be more favored by the hardware implementation. Results demonstrate averaged 17%, 10% and 8% bit rate reduction over HEVC for screen content compression with respect to All Intra, Random Access and Low-Delay encoder configurations.\n",
      "Lattice reduction (LR) aided detectors mitigate the exponentially increasing complexity of large multiple-input, multiple-output (MIMO) systems while achieving near-optimal performance with low computational complexity. In this paper, a channel-adaptive complex-domain LR-aided K-best MIMO detector is presented that reduces the gap between the K-best sphere decoding (SD) detector and the maximum likelihood (ML) optimal MIMO detector. While maintaining BER performance, computational complexity is reduced by 50% over a conventional complex domain K-best SD detector by implementing a new on-demand complex-domain candidate symbol selection algorithm. Two tunable variables in the candidate selection process are introduced to enable both coarse-grained and fine-grained adaptation of computational complexity to channel conditions.\n",
      "Developers know that copy-pasting code (aka code cloning) is often a convenient shortcut to achieving a design goal, albeit one that carries risks to the code quality over time. However, deciding which, if any, clones should be eliminated within an existing system is a daunting task. Fixing a clone usually means performing an invasive refactoring, and not all clones may be worth the effort, cost, and risk that such a change entails. Furthermore, sometimes cloning fulfils a useful design role, and should not be refactored at al. And clone detection tools often return very large result sets, making it hard to choose which clones should be investigated and possibly removed. In this paper, we propose an automated approach to recommend clones for refactoring by training a decision tree-based classifier. We analyze more than 600 clone instances in three medium-to large-sized open source projects, and we collect features that are associated with the source code, the context, and the history of clone instances. Our approach achieves a precision of around 80% in recommending clone refactoring instances for each target system, and similarly good precision is achieved in cross-project evaluation. By recommending which clones are appropriate for refactoring, our approach allows for better resource allocation for refactoring itself after obtaining clone detection results, and can thus lead to improved clone management in practice.\n",
      "Sensing falsification is a key security problem in cooperative spectrum sensing for cognitive radio networks. Most previous approaches assume that malicious users only cheat in their sensing reports following a predefined rule. However, some malicious users usually act intelligently to strategically adjust their malicious behavior according to their objectives and the network's defense schemes. The existing schemes cannot resist the malicious behaviors of intelligent malicious users (IMUs) without long-term collection of information on their reputation. In this paper, we construct a moral hazard principal-agent framework and design an incentive compatible mechanism to thwart the malicious behaviors of rational and irrational IMUs. We find that neither spectrum sensing nor spectrum access alone can prevent the malicious behavior without any information on users' reputation. According to the analysis of malicious behavior resistance methods, we propose a joint spectrum sensing and access mechanism to optimally prevent the IMUs from sensing falsification. Our evaluation results show that the proposed mechanism achieves almost the same performance as the ideal case with perfect sensing.\n",
      "The network resource competition of today' data enters is extremely intense between long-lived elephant flows and latency-sensitive mice flows. Achieving both goals of high throughput and low latency respectively for the two types of flows requires compromise, which recent research has not successfully solved mainly due to the transfer of elephant and mice flows on shared links without any differentiation. However, current data enters usually adopt clos-based topology, e.g. Fat-tree/VL2, so there exist multiple shortest paths between any pair of source and destination. In this paper, we leverage on this observation to propose a flow scheduling scheme, Freeway, to adaptively partition the transmission paths into low latency paths and high throughput paths respectively for the two types of flows. An algorithm is proposed to dynamically adjust the number of the two types of paths according to the real-time traffic. And based on these separated transmission paths, we propose different flow type-specific scheduling and forwarding methods to make full utilization of the bandwidth. Our simulation results show that Freeway significantly reduces the delay of mice flow by 85.8% and achieves 9.2% higher throughput compared with Hedera.\n",
      "We describe the design, implementation, and evaluation of EMBERS, an automated, 24x7 continuous system for forecasting civil unrest across 10 countries of Latin America using open source indicators such as tweets, news sources, blogs, economic indicators, and other data sources. Unlike retrospective studies, EMBERS has been making forecasts into the future since Nov 2012 which have been (and continue to be) evaluated by an independent T&E team (MITRE). Of note, EMBERS has successfully forecast the June 2013 protests in Brazil and Feb 2014 violent protests in Venezuela. We outline the system architecture of EMBERS, individual models that leverage specific data sources, and a fusion and suppression engine that supports trading off specific evaluation criteria. EMBERS also provides an audit trail interface that enables the investigation of why specific predictions were made along with the data utilized for forecasting. Through numerous evaluations, we demonstrate the superiority of EMBERS over baserate methods and its capability to forecast significant societal happenings.\n",
      "Consensus tracking problem in multi-agent systems with both outage and partial loss of effectiveness types of actuator faults is investigated in this paper. By adopting the virtual actuator technique based on the obtained fault estimates, the effects of possible actuator faults can be effectively compensated in each individual agent. Based on this, a distributed control strategy is developed by using locally available information. It is shown that the tracking error of each fault-free agent can converge to zero and the tracking errors of the faulty agents can remain bounded if the fault estimates are accurate. Moreover, a sufficient condition for achieving bounded tracking errors for all the followers is derived in terms of the fault estimation error. The sliding mode observer is utilized to estimate the faults. Simulation results are given to show the effectiveness of the proposed approach.\n",
      "In this paper, we focus on local image tampering detection. For a JPEG image, the probability distributions of its DCT coefficients will be disturbed by tampering operation. The tampered region and the unchanged region have different distributions, which is an important clue for locating tampering. Based on the assumption of Laplacian distribution of unquantized ac DCT coefficients, these two distributions as well as the size of tampered region can be estimated so that the probability of each DCT block being tampered is obtained. More accurate localization results could be got when we consider the prior knowledge of common tampered regions. We also design three kinds of features that can distinguish truly tampered regions from the false ones to reduce false alarm. For a tampered image which is saved in lossless compressed format, we also propose the specialized approach, which employs the quantization noise of high-frequency DCT coefficient, to improve the tampering localization performance. Extensive experiments on large scale databases prove the effectiveness of our proposed method and demonstrate that our method is suitable for locating tampered regions with different scales.\n",
      "In academia, scientific research achievements would be inconceivable without academic collaboration and cooperation among researchers. Previous studies have discovered that productive scholars tend to be more collaborative. However, it is often difficult and time-consuming for researchers to find the most valuable collaborators (MVCs) from a large volume of big scholarly data. In this paper, we present MVCWalker, an innovative method that stands on the shoulders of random walk with restart (RWR) for recommending collaborators to scholars. Three academic factors, i.e., coauthor order, latest collaboration time, and times of collaboration, are exploited to define link importance in academic social networks for the sake of recommendation quality. We conducted extensive experiments on DBLP data set in order to compare MVCWalker to the basic model of RWR and the common neighbor-based model friend of friends in various aspects, including, e.g., the impact of critical parameters and academic factors. Our experimental results show that incorporating the above factors into random walk model can improve the precision, recall rate, and coverage rate of academic collaboration recommendations.\n",
      "Spectrum sensing is an essential process in cognitive radio networks. The majority of existing sensing approaches aim to detect the existence of a signal on a busy channel without differentiating whether a signal originates from a primary user or not. In this paper, we address this issue and propose to employ an Angle of Arrival (AoA) based sensing approach to effectively distinct a primary user's signal from a secondary user' signal. Multiple Signal Classification (MUSIC), a classical AoA algorithm, is selected due to easy implementation and high resolution. Unlike the previous works on AoA based sensing, we thoroughly investigate its sensing performance based on a practical model which captures typical characteristics in cognitive radio networks. Two performance metrics named false alarm probability and miss detection probability are theoretically analyzed. Closed-form analytical expressions are derived for both metrics. Extensive simulations are carried out under various scenarios to evaluate AoA based sensing approach.\n",
      "Elastic caching platforms (ECPs) play an important role in accelerating the performance of Web applications. Several cache strategies have been proposed for ECPs to manage data access and distributions while maintaining the service availability. In our earlier research, we have demonstrated that there is no “one-fits-all” strategy for heterogeneous scenarios and the selection of the optimal strategy is related with workload patterns, cluster size and the number of concurrent users. In this paper, we present a new reconfiguration framework named PRESC\\(^{2}\\). It applies machine learning approaches to determine an optimal cache strategy and supports online optimization of performance model through trace-driven simulation or semi-supervised classification. Besides, the authors also propose a robust cache entries synchronization algorithm and a new optimization mechanism to further lower the adaptation costs. In our experiments, we find that PRESC\\(^{2}\\) improves the elasticity of ECPs and brings big performance gains when compared with static configurations.\n",
      "The Hong Kong Metro System has been operating for more than 30 years. With the increase in population, some of the stations are crowded in most time of a day. To facilitate the design and the alteration works for a station, the crowding and passenger movement pattern, especially the escape pattern in case of fire, should be studied. This article illustrated an agent-based people movement model, and a case targeting at the people movement pattern and egress process in a metro station has been studied. The simulation results demonstrate that the model is useful for assisting building designers to evaluate different design alternatives and to support fire safety performance studies.\n",
      "Remote collaboration has become increasingly important and common in designers' working routine. It is critical for geographically distributed designers to accurately perceive and comprehend other remote team members' intentions and activities with a high level of awareness and presence as if they were working in the same room. More specifically, distributed cognition places emphasis on the social aspects of cognition and asserts that knowledge is distributed by placing memories, facts, or knowledge on objects, individuals, and tools in the environment they work. This paper proposes a new computer-mediated remote collaborative design system, TeleAR, to enhance the distributed cognition among remote designers by integrating Augmented Reality and telepresence technologies. This system can afford a high level externalization of shared resources, which includes gestures, design tools, design elements, and design materials. This paper further investigates how this system may affect designers' communication and collaboration with focus on distributed cognition and mutual awareness. It also explores the critical communication-related issue addressed in the proposed system, including common ground and social capitals, perspective invariance, trust and spatial faithfulness.\n",
      "Image blur is a common phenomenon in daily life. Due to the great challenge, image restoration fascinates researchers to find out the solutions. Considering different types of blur, we propose a framework to segment the partial blur from a single image and then restore the latent information. In general, some morphological technologies are applied to separate the blur area. Traditionally, blind deconvolution method is applied in underdetermined conditions. In this research, we marginalize the kernel estimation by separating the problem into two stages, both of which are combined with different useful priors. A criterion of ranking the blur degree of a partial blur image is also proposed at the end of this paper. Experimental results demonstrate the accuracy and superiority of our approach.\n",
      "According to the fact that cloud servers have different energy consumption on different running states, as well as the energy waste problem caused by the mismatching between cloud servers and cloud tasks, we carry out researches on the energy optimal method achieved by a priced timed automaton for the cloud computing center in this paper. The priced timed automaton is used to model the running behaviors of the cloud computing system. After introducing the matching matrix of cloud tasks and cloud resources as well as the power matrix of the running states of cloud servers, we design a generation algorithm for the cloud system automaton based on the generation rules and reduction rules given ahead. Then, we propose another algorithm to settle the minimum path energy consumption problem in the cloud system automaton, therefore obtaining an energy optimal solution and an energy optimal value for the cloud system. A case study and repeated experimental analyses manifest that our method is effective and feasible.\n",
      "Classification in large-scale data is a key problem in big data domain. The theory of compressive sensing enables the recovery of a sparse signal from a small set of linear, random projections which provides a compressive classification method operating directly on the compressed data without reconstructing for big data. In this paper, we collected the compressed vowel /a:/ and /i:/ voice signals using compressive sensing for throat polyp detection. The throat polyp prediction procedure based on wavelet packet transform and support vector machine intelligent algorithm was deduced. The experiments for throat polyp prediction with the proposed classification algorithm were carried out. The results showed that the correct rate of prediction was stable under different number of samples and different random measurement matrices.\n",
      "In this paper we analyze characteristics of the ground reaction force (GRF) experienced by the legs of the quadruped robot during stance phase in walk gait, in particular, when the height of center of gravity (COG) of the quadruped robot is changeable. We also build the dynamics model of the quadruped robot. Two dynamics equations during swing phase and stance phase are established, respectively. Additionally, we design a controller to adjust the height of COG of the quadruped robot. The controller uses the central pattern generator (CPG) model to generate basic rhythmic motion, and utilizes the discrete tracking differentiator (TD) to implement the transition between two different rhythmic medium values of the CPG. The combination of the CPG model and the discrete TD enables the quadruped robot to adjust the height of COG according to the environment. The ground reaction peak force and the joint torque of the quadruped robot increase with the reduction of the height of COG. Finally, we give a simulation example and the results, including an analysis of the vertical reaction force and the joint torque of the quadruped robot.\n",
      "Mapping reads to a reference sequence is a common step when analyzing allele effects in high-throughput sequencing data. The choice of reference is critical because its effect on quantitative sequence analysis is non-negligible. Recent studies suggest aligning to a single standard reference sequence, as is common practice, can lead to an underlying bias depending on the genetic distances of the target sequences from the reference. To avoid this bias, researchers have resorted to using modified reference sequences. Even with this improvement, various limitations and problems remain unsolved, which include reduced mapping ratios, shifts in read mappings and the selection of which variants to include to remove biases. To address these issues, we propose a novel and generic multi-alignment pipeline. Our pipeline integrates the genomic variations from known or suspected founders into separate reference sequences and performs alignments to each one. By mapping reads to multiple reference sequences and merging them afterward, we are able to rescue more reads and diminish the bias caused by using a single common reference. Moreover, the genomic origin of each read is determined and annotated during the merging process, providing a better source of information to assess differential expression than simple allele queries at known variant positions. Using RNAseq of a diallel cross, we compare our pipeline with the single-reference pipeline and demonstrate our advantages of more aligned reads and a higher percentage of reads with assigned origins. Database URL: http://csbio.unc.edu/CCstatus/index.py?run¼Pseudo\n",
      "The Internet of Things (IoT) paradigm connects everyday objects to the Internet and enables a multitude of applications with the real world data collected from those objects. In the city environment, real world data sources include fixed installations of sensor networks by city authorities as well as mobile sources, such as citizens' smartphones, taxis and buses equipped with sensors. This kind of data varies not only along the temporal but also the spatial axis. For handling such frequently updated, time-stamped and structured data from a large number of heterogeneous sources, this paper presents a data-centric framework that offers a structured substrate for abstracting heterogeneous sensing sources. More importantly, it enables the collection, storage and discovery of observation and measurement data from both static and mobile sensing sources.\n",
      "Series elastic actuation (SEA) has been widely used for output force/torque regulation, however, the nonlinearity introduced by its torsion spring hysteresis will greatly affect the quality of feedback control. Thus an experimental procedure is conducted for the modeling and identification of spring hysteresis effect, based on which a compensated control scheme is given. Experiment results show that the compensated control can improve the control performance compared with ordinary linear controller.\n",
      "This paper concentrates on the gait transition of a quadruped robot. Animals move with different gaits to adapt to the change of terrains or just to save energy. A biomimetic robot should achieve this process in a smooth, continuous and energy efficient way. We propose a gait transition controller that can achieve needed phase difference between every two legs based on the Hopf oscillator. The quadruped robot has achieved biomimetic gait transitions from walk to trot and trot to gallop based on the proposed controller, and the two gait transitions of the opposite direction are the same by changing the control variables reversely. All of these processes have been verified on a simulated quadruped robot. The robot can achieve a normal velocity value for the walk, trot and gallop gait respectively. Both the walk to trot and trot to gallop gait transitions take only one second. We analyze the stability and some problems existing in the simulation and two gait transitions.\n",
      "In this paper, we propose a new image retrieval method based on Sectored Contour to Centroid Triangulation (SCTCT) using distinctive shape feature, named Arc Difference Rate (ADR). We utilized Support Vector Machine (SVM) method as an extraction tool to extract suspicious tumor area as binary object image from the breast MRI. Therefore extracted 100 binary object images are used as test cases in the experimental study. The results from proposed method show the improvement in finding correct matches compare to the traditional SCTCT.\n",
      "Based on the in-depth analysis of negotiation characteristics in MCD, a novel negotiation method for MCD is proposed.An effective negotiation strategy to resolve the conflicts among multiple disciplines is given.A new negotiation model is given by introducing concepts of concession, satisfaction degree and strategy power exponent.Two types of satisfaction degree functions and the mapping method among multiple domains are given.Negotiation process and system framework are designed to assist multiple disciplines rapidly reach a compromise solution. The collaborative design of a complicated mechanical product often involves conflicting multidisciplinary objectives, thus one key problem is conflict resolution and coordination among the different disciplines. Since the characteristics such as cooperative competition, professional dependence, compromise, overall utility and so on exist in multidisciplinary collaborative design (MCD), an effective way to gradually eliminate the conflicts among the multiple disciplines and reach an agreement is the negotiation by which a compromise solution that satisfies all parties is got. By comprehensively analyzing the characteristics in MCD and considering the benefit equilibrium among discipline individuals and team, a negotiation strategy is presented, which maximize the union satisfaction degree of system overall objective under the premise of ensuring the higher satisfaction degree level of each discipline's local objective. A design action of a discipline is abstractly expressed as a concession in the negotiation strategy, and a negotiation model used for MCD is generated by establishing the relation between concession and satisfaction degree. By the relation between satisfaction degree and objective function, the mapping relationship between satisfaction degree domain and physical domain is built to get the design solution. A negotiation process is planned, and a negotiation system framework is designed to support the negotiation among multiple disciplines and assist the different disciplines rapidly reach a consistent compromise solution. A design example of automotive friction clutch is given to illustrate the proposed method.\n",
      "To construct various Virtual Environments (VEs) is essential for geographically dispersed users to real-time interact and roam in various virtual worlds(e.g., virtual city, massively multiuser online games). However, this operation has been hindered by the contradiction between explosively growing 3D contents and relatively slower increasing resources (e.g., bandwidth, CPU resource). This work thus designs a Peer-to-Peer (P2P) based scenes progressive downloading (i.e., P2P 3D streaming) scheme for constructing large scale VE (named as PSVE) efficiently. Main work of this paper is elaborated as follows: determination of downloading priority for all Level of Detail (LoD) pieces of models, discovery and selection a node to distribute required scenes by two strategies respectively. Simulations based on real network conditions are implemented to evaluate the performance of PSVE. The simulation results demonstrate that our work has achieved high performance in terms of streaming quality, overhead and stability, our presented scheme could realize real-time walkthrough in a large scale P2P-DVE even with high churn of nodes.\n",
      "Telecommunication network infrastructures both stationary and ad hoc, play an important role in maintaining the stability of society worldwide. The protection of these critical infrastructures and their supporting structures become highly challenged due to its complexity. The understanding of interdependency of these infrastructures is the essential step to protect these infrastructures from destruction and attacks. This paper presents a critical infrastructure detection model to discover the interdependency based on the theories from social networks and new telecommunication pathways while this study transforms social theory into computational constructions. The procedure and solution of protecting critical infrastructures are discussed and computational results from the proposed model are presented.\n",
      "Supervised distance metric learning plays a substantial role to the success of statistical classification and information retrieval. Although many related algorithms are proposed, it is still an open problem about incorporating both the geometric information (i.e., locality) and the label information (i.e., globality) in metric learning. In this paper, we propose a novel metric learning framework, called ''Dependence Maximization based Metric Learning'' (DMML), which can efficiently integrate these two sources of information into a unified structure as instances of convex programming without requiring balance weights. In DMML, the metric is trained by maximizing the dependence between data distributions in the reproducing kernel Hilbert spaces (RKHSs). Unlike learning in the existing information theoretic algorithms, however, DMML requires no estimation or assumption of data distributions. Under this proposed framework, we present two methods by employing different independence criteria respectively, i.e., Hilbert-Schmidt Independence Criterion and the generalized Distance Covariance. Comprehensive experimental results for classification, visualization and image retrieval demonstrate that DMML favorably outperforms state-of-the-art metric learning algorithms, meanwhile illustrate the respective advantages of these two proposed methods in the related applications.\n",
      "In this technical note, an adaptive failure compensation problem has been studied for a class of nonlinear uncertain systems subject to stochastic actuator failures and unknown parameters. The stochastic functions related to Markovian variables have been introduced to denote the failure scaling factors for each actuators which is much more practical and challenging. Firstly, by taking into account of the Markovian variables existing in the system, some preliminary knowledges have been established. Then, by employing backstepping strategy, an adaptive failure compensation control scheme has been proposed, which ensures the boundedness in probability of all the closed-loop signals in the presence of stochastic actuator failures. A simulation example is presented to show the effectiveness of the proposed scheme.\n",
      "Coherent joint transmission (CJT) algorithm has been extensively investigated in multi-cell cooperative downlink transmission due to its huge potential performance gains. Comparing with the existing CJT algorithm based on the multi-user eign-mode transmission, an enhanced CJT (eCJT) algorithm is proposed in this paper to further enhance the performance of cellular network in terms of spectral efficiency, especially for the cell edge users. Theoretical analysis show that utilizing successive transmit channel optimization and selection, eCJT outperforms traditional CJT in multi-cell coordinated joint transmission. System level simulation results demonstrate that eCJT algorithm outperforms traditional CJT algorithm in both cell average performance and cell edge performance. Taking the performance of traditional CJT algorithm as the baseline, eCJT algorithm can achieve over 30% spectral efficiency gain in cell edge under ITU UMi and 3GPP Case1-3D scenario, respectively.\n",
      "A situational awareness system is essential to provide accurate understanding of power system dynamics, such that proper actions can be taken in real time in response to system disturbances and to avoid cascading blackouts. Event analysis has been an important component in any situational awareness system. However, most state-of-the-art techniques can only handle single event analysis. This paper tackles the challenging problem of multiple event detection and recognition. We propose a new conceptual framework, referred to as event unmixing, where we consider real-world events mixtures of more than one constituent root event. This concept is a key enabler for analysis of events to go beyond what are immediately detectable in a system, providing high-resolution data understanding at a finer scale. We interpret the event formation process from a linear mixing perspective and propose an innovative nonnegative sparse event unmixing (NSEU) algorithm for multiple event separation and temporal localization. The proposed framework has been evaluated using both PSS/E simulated cases and real event cases collected from the frequency disturbance recorders (FDRs) of the Frequency Monitoring Network (FNET). The experimental results demonstrate that the framework is reliable to detect and recognize multiple cascading events as well as their time of occurrence with high accuracy.\n",
      "Popular techniques used in today's Web search engines and digital libraries for retrieving and ranking scientific publications have foundations in modern information retrieval. Information and users in the scientific research communities have their own characteristics, however, they have not been sufficiently exploited in existing retrieval and ranking methods. We present a semantic search engine, IRIS2, which represents the semantic entities and their relations using ontologies and knowledge bases. It utilises a ranking method based on the \"rational research\" model, which restores an elegant idea that a researcher does rational research in an academic environment. We explain in detail the design and implementation of the IRIS2 prototype and compare its retrieving and ranking performance with existing methods.\n",
      "The characteristics of cognitive radio networks have huge impacts on the end-to-end performance of the transmission control protocol TCP for secondary users. Thus, the existing TCP throughput expression, widely used in wired and wireless networks, is no longer suitable for cognitive radio networks. In this paper, we derive the transmission opportunity of secondary users, taking into account the dynamics of spectrum availability, the overhead and errors of spectrum sensing, as well as the interaction between TCP and lower layers. The amount of transmission opportunity is expressed in terms of effective data transmission time. On the basis of the analysis of the transmission opportunity, an expression of an effective TCP throughput is then derived. To evaluate this effective TCP throughput expression, two cross-layer optimization problems are formulated as application examples to maximize the transport layer effective throughput and energy utility, respectively. Simulation results show that our analysis on transmission opportunity is accurate, and the derived effective TCP throughput expression is more precise than existing ones. Copyright © 2012 John Wiley & Sons, Ltd.\n",
      "Device-to-device (D2D) communication is known as a promising way to cope with the growing mobile video traffic, which may suffer from the short duration caused by the limited energy supply. In this paper, we propose a practical D2D cooperation framework based on distributed optimization to extend the video transmission duration. In the multi-path multi-hop D2D communication scenario, we effectively schedule the routes and video traffic workloads to avoid low-battery D2D outages due to non-uniform energy consumption. Specifically, we formulate the D2D cooperation as a consensus problem among the network operator and cooperative devices, and then solve it in a flexible distributed fashion. Numerical results show that the proposed framework could reach the optimality quickly in time division duplex communication systems and significantly increase the duration of the cooperative video transmission.\n",
      "In this paper, we propose a new compressive sensing-based compression and recovery ultra-wideband UWB communication system. Compared with the conventional UWB system, we can jointly estimate the channel and compress the data, which can also simplify the design of hardware. No information about the transmitted signal is required in advance as long as the channel follows autoregressive process. As an application example, real-world UWB signal is collected and processed to evaluate the performance of our proposed system. The compression procedure is so simple that we just multiply random Gaussian or Bernoulli matrix with the original data to capture all the information we want. Simulation results show that the data could be perfectly recovered if the compression ratio does not exceed 2.5:1 when Bernoulli matrix is chosen as the sensing matrix. Copyright © 2012 John Wiley & Sons, Ltd.\n",
      "The network autocorrelation model has become an increasingly popular tool for conducting social network analysis. More and more researchers, however, have documented evidence of a systematic negative bias in the estimation of the network effect (ρ). In this paper, we take a different approach to the problem by investigating conditions under which, despite the underestimation bias, a network effect can still be detected by the network autocorrelation model. Using simulations, we find that moderately-sized network effects (e.g., ρ = .3) are still often detectable in modest-sized networks (i.e., 40 or more nodes). Analyses reveal that statistical power is primarily a nonlinear function of network effect size (ρ) and network size (N), although both of these factors can interact with network density and network structure to impair power under certain rare conditions. We conclude by discussing implications of these findings and guidelines for users of the autocorrelation model.\n",
      "This paper presents an efficient method for securely broadcasting a message to multiple recipients in a heterogeneous environment where each recipient is allowed to choose his or her preferred secure encryption scheme independently of other recipients' choices. Previous work pertinent to this direction of research, namely multi-recipient encryption scheme (MRES), generally requires all recipients adhere to the same public key encapsulation mechanism (KEM) for the sake of delivering promised savings in computation and bandwidth via randomness reuse. Our work eliminates the requirement of using the same KEM by all recipients, whereby removing a practical barrier to the adoption of MRES in real world applications. A second advantage is the method's capability to cope with a dynamically changing group of recipients where old recipients may be deleted and new recipients may be added, while ensuring the security of messages shared in future. Additional features of our method include decryption by a sender, anonymity of recipients and stateful key encapsulation which significantly reduces computational costs for securely transmitting or sharing new messages. All these attributes would be useful in building applications for secure data sharing in a cloud computing environment.\n",
      "CAPTCHA is now commonly used as standard security technology to tell computers and humans apart. The most widely deployed CAPTCHAs are text-based schemes. In this paper, we document how we have broken such a text-based scheme which uses the ”connecting characters together (CCT)” principle. CAPTCHAs of this type can be classified into three types: CAPTCHA with overlap but no noise arcs; CAPTCHA with noise arcs but no overlap; and CAPTCHA without noise arcs and overlap. Yahoo!, Baidu CAPTCHA and reCAPTCHA were selected as representatives of the three types. The CCT CAPTCHA is effectively resistant to segmentation and recognition in the early attacks. In contrast to early works that recognized the text after segmentation, we combined recognition with segmentation to break the CCT CAPTCHA. Our method segments the text by extracting the recognized characters. The experiments show that our extraction and segmentation attack on Yahoo! CAPTCHA achieved a success rate of 78% and an overall (individual character recognition with OCR) success rate of 55%. The segmentation and recognition success rate of Baidu CAPTCHA was 34%. On reCAPTCHA we achieved 34% success rate.\n",
      "Motivation: RNA-Seq technique has been demonstrated as a revolutionary means for exploring transcriptome because it provides deep coverage and base pair-level resolution. RNA-Seq quantification is proven to be an efficient alternative to Microarray technique in gene expression study, and it is a critical component in RNA-Seq differential expression analysis. Most existing RNA-Seq quantification tools require the alignments of fragments to either a genome or a transcriptome, entailing a time-consuming and intricate alignment step. To improve the performance of RNA-Seq quantification, an alignment-free method, Sailfish, has been recently proposed to quantify transcript abundances using all k-mers in the transcriptome, demonstrating the feasibility of designing an efficient alignment-free method for transcriptome quantification. Even though Sailfish is substantially faster than alternative alignment-dependent methods such as Cufflinks, using all k-mers in the transcriptome quantification impedes the scalability of the method.#R##N##R##N#Results: We propose a novel RNA-Seq quantification method, RNA-Skim, which partitions the transcriptome into disjoint transcript clusters based on sequence similarity, and introduces the notion of sig-mers, which are a special type of k-mers uniquely associated with each cluster. We demonstrate that the sig-mer counts within a cluster are sufficient for estimating transcript abundances with accuracy comparable with any state-of-the-art method. This enables RNA-Skim to perform transcript quantification on each cluster independently, reducing a complex optimization problem into smaller optimization tasks that can be run in parallel. As a result, RNA-Skim uses  100 speedup over the state-of-the-art alignment-based methods, while delivering comparable or higher accuracy.#R##N##R##N#Availability and implementation: The software is available at http://www.csbio.unc.edu/rs.#R##N##R##N#Contact: ude.alcu.sc@gnawiew#R##N##R##N#Supplementary information: Supplementary data are available at Bioinformatics online.\n",
      "ZigBee hierarchical tree routing protocol (HRP; ZigBee Alliance, San Ramon, CA, USA) provides a simple but reliable topology. However, the transmission routes are not always efficient, and the links are fixed after they were determined during the network initialization. In this paper, we propose an adaptive routing optimization and energy-balancing algorithm in ZigBee hierarchical networks. In our routing algorithm, the parent node could adaptively maintain its child's links for lower network load, and all the information needed can be obtained from a neighbour table to avoid introducing extra communication overhead. Such algorithm makes ZigBee's hierarchical topology to adaptively maintain and optimize the routing paths during its lifetime, and an address reassignment mechanism is also introduced to ensure that our algorithm follows ZigBee specification. In addition, an energy-balancing algorithm is also proposed to reduce the power cost of low-battery device. Simulation results show that our routing scheme has better performance with lower average transmission hops and network load, and our energy-balancing algorithm could reduce the power consumption of low-battery device.\n",
      "Server farms generally consume an enormous amount of energy, which not only increases the running cost but also simultaneously enhances their greenhouse gas emissions. One way to improve the energy efficiency in server farms is dynamical powering on/off servers. However, this method suffers from many setup time to turn the servers back on, which would have a negative impact on a job's response time and waste yet additional energy. This situation is further exacerbated by the unreliability of the servers, which has become the norm in today's server farms. In this paper, we investigate the impact of dynamical powering on/off servers on energy and performance in a typical server farm environment. Prior work has analyzed similar models for a single server, but no analytical results are known for multiservers. We mainly use the matrix geometric method to analyze this model, and system performance measures are explicitly developed in terms of computable forms. An energy-performance trade-off model is derived to determine the optimal management policy for the server farms. Finally, we discuss some extensions of our model to show its robustness and to point out avenues for future research. Numerical examples are provided at several points throughout the paper to illustrate the correctness of our analysis results and to validate the optimization approach. Copyright © 2014 John Wiley & Sons, Ltd.\n",
      "Supervised metric learning plays a substantial role in statistical classification. Conventional metric learning algorithms have limited utility when the training data and testing data are drawn from related but different domains (i.e., source domain and target domain). Although this issue has got some progress in feature-based transfer learning, most of the work in this area suffers from non-trivial optimization and pays little attention to preserving the discriminating information. In this paper, we propose a novel metric learning algorithm to transfer knowledge from the source domain to the target domain in an information-theoretic setting, where a shared Mahalanobis distance across two domains is learnt by combining three goals together: 1) reducing the distribution difference between different domains; 2) preserving the geometry of target domain data; 3) aligning the geometry of source domain data with its label information. Based on this combination, the learnt Mahalanobis distance effectively transfers the discriminating power and propagates standard classifiers across these two domains. More importantly, our proposed method has closed-form solution and can be efficiently optimized. Experiments in two real-world applications demonstrate the effectiveness of our proposed method.\n",
      "Recently there has been increasing interest in deep neural network due to its powerful represent ability in several successful applications such as speech recognition and image classification. In this paper, we propose a general nonlinear embedding framework based on deep neural network which can be utilized to implement a family of dimensionality reduction algorithms. The objective function of our framework consists of two terms: 1) an embedding term which transforms the input to a low-dimensional representation with a multilayer network, and 2) a regularization term which computes the reconstruction error of the original input by unrolling the multilayer network to a deep auto encoder. We adopt a layer-by-layer pretraining procedure to obtain good initial weights for the network, and then minimize the objective function by back propagating derivatives of the two terms. To evaluate the proposed framework, we perform face recognition and digit classification experiments. The experiments demonstrate that the proposed framework achieves better results than the state-of-the-art algorithms. The success of our framework further verifies deep neural network's advantages in representation learning.\n",
      "In this paper, the problem of direction of arrival (DOA) estimation for monostatic multiple-input multiple-output (MIMO) radar is addressed, and a sparse representation scheme for DOA estimation is proposed. Firstly, the reduced-dimensional transformation matrix and SVD-technique are utilized to reduce the computational complexity of the sparse signal reconstruction. Then the coefficients of the reduced-dimensional Capon (RD-Capon) spectrum are exploited to design a weight matrix for reweighting l\"1 norm constraint minimization to enhance the sparsity of the solution, and the DOA can be estimated by finding the non-zero rows in the recovered matrix. The angle estimation performance of the proposed method is better than RD-ESPRIT and RD-Capon algorithms. Furthermore, the proposed method works well for coherence targets without any decorrelation procedure, and has low sensitivity to the priori information of the target number. Simulation results verify the effectiveness of the proposed method.\n",
      "Keyword search over XML data has attracted a lot of research efforts in the last decade, where one of the fundamental research problems is how to efficiently answer a given keyword query w.r.t. a certain query semantics. We found that the key factor resulting in the inefficiency for existing methods is that they all heavily suffer from the common-ancestor-repetition problem. In this paper, we propose a novel form of inverted list, namely the IDList; the IDList for keyword $$k$$ consists of ordered nodes that directly or indirectly contain $$k$$ . We then show that finding keyword query results based on the smallest lowest common ancestor and exclusive lowest common ancestor semantics can be reduced to ordered set intersection problem, which has been heavily optimized due to its application in areas such as information retrieval and database systems. We propose several algorithms that exploit set intersection in different directions and with or without using additional indexes. We further propose several algorithms that are based on hash search to simplify the operation of finding common nodes from all involved IDLists. We have conducted an extensive set of experiments using many state-of-the-art algorithms and several large-scale datasets. The results demonstrate that our proposed methods outperform existing methods by up to two orders of magnitude in many cases.\n",
      "This paper studies the 1-error dictionary search problem in external memory. The input is a set D of strings whose characters are drawn from a constant-size alphabet. Given a string q, a query reports the ids of all strings in D that are within 1 edit distance from q. We give a structure occupying O(n/B) blocks that answers a query in    $O(1 + \\frac{m}{wB} + \\frac{k}{B})$    I/Os, where n is the total length of all strings in D, m is the length of q, k is the number of ids reported, w is the size of a machine word, and B is the number of words in a block.\n",
      "In practice, data are often found to violate functional depen- dencies, and are hence inconsistent. To resolve such violations, data are to be restored to a consistent state, known as \"repair\", while the number of possible repairs may be exponential. Previous works either consider optimal repair computation, to find one single repair that is (nearly) op- timal w.r.t. some cost model, or discuss repair sampling, to randomly generate a repair from the space of all possible repairs. This paper makes a first effort to investigate repair diversification problem, which aims at generating a set of repairs by minimizing their costs and maximizing their diversity. There are several motivating sce- narios where diversifying repairs is desirable. For example, in the recently proposed interactive repairing approach, repair diversification techniques can be employed to generate some representative repairs that are likely to occur (small cost), and at the same time, that are dissimilar to each other (high diversity). Repair diversification significantly differs from optimal repair computing and repair sampling in its framework and techniques. (1) Based on two natural diversification objectives, we formulate two versions of repair diversification problem, both modeled as bi-criteria optimization problem, and prove the complexity of their related decision problems. (2) We develop algorithms for diversification problems. These algorithms embed repair computation into the framework of diversifica- tion, and hence find desirable repairs without searching the whole repair space. (3) We conduct extensive performance studies, to verify the effec- tiveness and efficiency of our algorithms.\n",
      "The vision of Self-Organizing Networks (SON) has been drawing considerable attention as a major axis for the development of future networks. As an essential functionality in SON, cell outage detection is developed to autonomously detect macrocells or femtocells that are inoperative and unable to provide service. Previous cell outage detection approaches have mainly focused on macrocells while the outage issue in the emerging femtocell networks is less discussed. However, due to the two-tier macro-femto network architecture and the small coverage nature of femtocells, it is challenging to enable outage detection function- ality in femtocell networks. Based on the observation that spatial correlations among users can be extracted to cope with these challenges, this paper proposes a Cooperative femtocell Outage Detection (COD) architecture which consists of a trigger stage and a detection stage. In the trigger stage, we design a trigger mech- anism that leverages correlation information extracted through collaborative filtering to efficiently trigger the detection procedure without inter-cell communications. In the detection stage, to im- prove detection accuracy, we introduce a sequential cooperative detection rule to process spatially and temporally correlated user statistics. Numerical studies for a variety of femtocell deploy- ments and configurations demonstrate that COD outperforms the existing scheme in both communication overhead and detection accuracy.\n",
      "Display Omitted We present a novel method for mining TCM clinical cases.The method could extract useful treatment patterns.Discovered patterns could be used to improve the function prediction of a prescription.The framework is helpful to summarize experiences of TCM doctors.Results on 3090 real world clinical cases show the effectiveness of our method. In Traditional Chinese Medicine (TCM), the prescription is the crystallization of clinical experience of doctors, which is the main way to cure diseases in China for thousands of years. Clinical cases, on the other hand, describe how doctors diagnose and prescribe. In this paper, we propose a framework which mines treatment patterns in TCM clinical cases by exploiting supervised topic model and TCM domain knowledge. The framework can reflect principle rules in TCM and improve function prediction of a new prescription. We evaluate our method on 3090 real world TCM clinical cases. The experiment validates the effectiveness of our method.\n",
      "We study the multi-resource allocation problem in cloud computing systems where the resource pool is constructed from a large number of heterogeneous servers, representing different points in the configuration space of resources such as processing, memory, and storage. We design a multi-resource allocation mechanism, called DRFH, that generalizes the notion of Dominant Resource Fairness (DRF) from a single server to multiple heterogeneous servers. DRFH provides a number of highly desirable properties. With DRFH, no user prefers the allocation of another user; no one can improve its allocation without decreasing that of the others; and more importantly, no coalition behavior of misreporting resource demands can benefit all its members. DRFH also ensures some level of service isolation among the users. As a direct application, we design a simple heuristic that implements DRFH in real-world systems. Large-scale simulations driven by Google cluster traces show that DRFH significantly outperforms the traditional slot-based scheduler, leading to much higher resource utilization with substantially shorter job completion times.\n",
      "Pedestrian dead reckoning (PDR) is an effective way for navigation coupled with GNSS (Global Navigation Satellite System) or weak GNSS signal environment like indoor scenario. However, indoor location with an accuracy of 1 to 2 meters determined by PDR based on MEMS-IMU is still very challenging. For one thing, heading estimation is an important problem in PDR because of the singularities. For another thing, walking distance estimation is also a critical problem for pedestrian walking with randomness. Based on the above two problems, this paper proposed axis-exchanged compensation and gait parameters analysis algorithm to improve the navigation accuracy. In detail, an axis-exchanged compensation factored quaternion algorithm is put forward first to overcome the singularities in heading estimation without increasing the amount of computation. Besides, real-time heading is updated by R-adaptive Kalman filter. Moreover, gait parameters analysis algorithm can be divided into two steps: cadence detection and step length estimation. Thus, a method of cadence classification and interval symmetry is proposed to detect the cadence accurately. Furthermore, a step length model adjusted by cadence is established for step length estimation. Compared to the traditional PDR navigation, experimental results showed that the error of navigation reduces 32.6%.\n",
      "This paper studies the decentralized optimal control of discrete-time system with inputdelay, where a large number of agents with the identical decoupling dynamical equations and the coupling cost function through the mean field are considered. The decentralized and centralized optimal controllers are proposed by the optimal tracking control of LQR problem with delay. They are proved that the optimal controllers and the optimal cost function of the centralized and decentralized solutions are equivalent for the optimal control problem. An illustrative example is given to show the efficiency of the decentralized optimal controllers.\n",
      "Rapid urbanization in many large cities in China makes metro station an integral part of metropolitan people's daily life. High density of crowds in metro stations would cause serious congestion problems and pose threats to pedestrian safety. Because of the heterogeneous and complex properties of pedestrians, traditional approaches face difficulties in predicting future pedestrian flow patterns. The use of agent-based simulation approach makes it possible to naturally reproduce various pedestrian behaviors in different scenarios. This paper presented an agent-based microscopic pedestrian simulation model—CityFlow, which was proved to be flexible in revealing most important pedestrian behaviors in metro stations by several simulation cases. The model applications can provide implications in evaluation of design proposals of metro facilities.\n",
      "In 2010, Gentry and Halevi presented the first FHE implementation. FHE allows the evaluation of arbitrary functions directly on encrypted data on untrusted servers. However, even for the small setting with 2048 dimensions, the authors reported a performance of 1.8 s for a single bit encryption and 32 s for recryption on a high-end server. Much of the latency is due to computationally intensive multi-million-bit modular multiplications. In this paper, we introduce two optimizations coupled with a novel precomputation technique. In the first optimization called partial FFT, we adopt Strassen’s FFT-based multiplication algorithm along with Barret reduction to speedup modular multiplications. For the encrypt primitive, we employ a window-based evaluation technique along with a modest degree of precomputation. In the full FFT optimization, we delay modular reductions and change the window algorithm, which allows us to carry out the bulk of computations in the frequency domain. We manage to eliminate all FFT conversion except the final inverse transformation drastically reducing the computation latency for all FHE primitives. We implemented the GH FHE scheme on two GPUs to further speedup the operations. Our experimental results with small parameter setting show speedups of 174, 7.6, and 13.5 times for encryption, decryption, and recryption, respectively, when compared to the Gentry–Halevi implementation. The speedup is enhanced in the medium setting. However, in the large setting, memory becomes the bottleneck and the speedup is somewhat diminished.\n",
      "This paper focuses on the development of an online high-precision probabilistic localization approach for the miniature underwater robots equipped with limited computational capacities and low-cost sensing devices. The localization system takes Monte Carlo localization (MCL) as the main framework and utilizes the onboard camera and low-cost inertial measurement unit (IMU) for information acquisition to provide a decimeter-level precision with 5-Hz refreshing rate in a small space with several artificial landmarks. Specifically, a novel underwater image processing algorithm is introduced to improve the underwater image quality; two key parameters, including a distance factor and an angle factor, are finally calculated to serve as the criteria to MCL. Meanwhile, the accurate orientation and rough odometry of the robot are acquired by onboard IMU. Moreover, a Kalman filter is adopted to filter the key information extracted from the sensors' data processing. In principle, when visual and inertial cues are both obtained, visual information with higher reliability has the priority to be used in the algorithm, which finally results in rapid convergence to the real pose of the robot. A series of relevant experiments are systematically conducted on the robotic fish, which prove that the online localization algorithm herein is highly accurate, robust, and practical for the miniature underwater robots with limited computational resources.\n",
      "Eigen-beamforming is an optimal beamforming scheme for full-dimension multi-input multi-output (FD-MIMO). However, it suffers from high complexity of the singular value decomposition (SVD) of the 3D channel matrix. In this paper, we propose a low complexity dual layer beamforming scheme to exploit both the horizontal and vertical degrees of freedom. The proposed scheme first segments the channel matrix into several sub-channel matrices. Then, by selecting the vertical and horizontal beamforming from the sub-channel matrices respectively, the proposed scheme can reduce the complexity, while achieving the performance approximate to the optimal eigen-beamforming scheme. Moreover, to enable practical implementation, a hybrid codebook design is proposed which exploits the structure of the dual layer beamforming to reduce feedback overhead. According to the different correlation characteristics in horizontal and vertical direction, we design different codebooks for the horizontal and vertical beamforming. Simulation results demonstrate that the proposed scheme can effectively improve the spectral efficiency of FD-MIMO systems with limited feedback, especially for the cell edge users.\n",
      "The continuing advances in the storage and transmission abilities of user equipment have made it possible to share videos through device-to-device communications, which may be an efficient way to enhance the capacity of cellular network to provide wireless video services. In adaptive D2D video streaming, user experience is greatly influenced by the quality and fluency of the video, which is affected by the D2D link’s quality. Additionally, the quality of D2D links relies on the resource allocation scheme for D2D pairs. To improve the quality of experience in D2D video streaming, we propose a QoE-aware resource allocation scheme for adaptive D2D video streaming. The QoE-aware resource allocation scheme has the ability to cater to the user experience in adaptive video steaming while considering the co-channel interference derived from frequency reuse in D2D communications. Specifically, a dynamic network scheduling problem is formulated and solved, with the objective of maximizing the video quality while maintaining the long-term stable performance of fluency during video playback. Extensive numerical results demonstrate that the proposed QoE-aware resource allocation scheme outperforms the QoE-oblivious resource allocation scheme.\n",
      "Private property refers to something carrying private information, or expensive items; these items are very important for companies or individuals. The proposed private property protection system uses adaptive sensing technology to protect private materials in real time. In this paper, we propose an adaptive sensing private property authentication scheme which can be applied in the cloud computing. Considering a relatively safe room with a remote reader, there are several valuable items in the room. Each itemis labeled with a unique tamper-evident adaptive sensor and the reader can simultaneously read a plurality of sensors. Encrypted information of items, sensors, and readers is stored in the cloud. The reader reads sensors and uploads the collected data to the cloud for further processing in real time. The proposed scheme is under the cloud environment to protect the user privacy and prevent synchronized attacks. Compared with some traditional schemes, our scheme is economical, practical, and easy to be expanded. Furthermore, it pays attention to privacy protection with real-time monitoring.\n",
      "Localization is emerging as a fundamental component in wireless sensor network and is widely used in the field of environmental monitoring, national and military defense, transportation monitoring, and so on. Current localization methods, however, focus on how to improve accuracy without considering the robustness. Thus, the error will increase rapidly when nodes density and SNR (signal to noise ratio) have changed dramatically. This paper introduces CTLL, Cell-Based Transfer Learning Method for Localization in WSNs, a new way for localization which is robust to the variances of nodes density and SNR. The method combines samples transfer learning and SVR (Support Vector Regression) regression model to get a better performance of localization. Unlike past work, which considers that the nodes density and SNR are invariable, our design applies regional division and transfer learning to adapt to the variances of nodes density and SNR. We evaluate the performance of our method both on simulation and realistic deployment. The results show that our method increases accuracy and provides high robustness under a low cost.\n",
      "Over years, virtual backbone has attracted lots of attentions as a promising approach to deal with the broadcasting storm problem in wireless networks. One popular way to construct a quality virtual backbone is to solve the minimum connected dominating set problem. However, a virtual backbone computed in this way is not resilient against topology change since the induced graph by the connected dominating set is one-vertex-connected. As a result, the minimum k-connected m-dominating set problem is introduced to construct a fault-tolerant virtual backbone. Currently, the best known approximation algorithm for the problem in unit disk graph assumes k ≤ 3 and m ≥ 1 and its performance ratio is 280 when k = m = 3. In this paper, we use a classical result from graph theory, Tutte decomposition, to design a new approximation algorithm for the problem in unit disk graph for k ≤ 3 and m ≥ 3. In particular, the algorithm features with much simpler structure and much smaller performance ratio, e.g. nearly 66 when k = m = 3. We also conduct simulation to evaluate the performance of our algorithm.\n",
      "Traditionally, jamming to the wireless system is a fatal threat to the security of home area networks HANs, which impedes the two-way data transmission between electric devices and the smart meter, and thus deteriorates the reliability of the in-home communication of Smart Grid. On the basis of this consideration, this paper incorporates the power line system into the HAN and proposes a hybrid architecture of orthogonal frequency-division multiplexing-based wireless communication and power line communication for the Smart Grid security application. With this new solution, the channel diversity of the HAN is realized, and the communication reliability is still guaranteed even when the wireless channel suffers from jamming. Simulation results validate the feasibility of the proposed hybrid architecture, and furthermore, as a receiver diversity scheme, selection combining is preferred to maximum ratio combining. Copyright © 2013 John Wiley & Sons, Ltd.\n",
      "Relation learning is a fundamental operation in many computer vision tasks. Recently, high-order Boltzmann machine and its variants have exhibited the great power of modelling various data relation. However, most of them are unsupervised learning models which are not very discriminative and thus cannot server as a standalone solution to relation learning tasks. In this paper, we explore supervised learning algorithms and propose a new model named Conditional High-order Boltzmann Machine (CHBM), which can be directly used as a bilinear classifier to assign similarity scores for pairwise images. Then, to better deal with complex data relation, we propose a gated version of CHBM which untangles factors of variation by exploiting a set of latent variables to gate classification. We perform four-order tensor factorization for parameter reduction, and present two efficient supervised learning algorithms from the perspectives of being generative and discriminative, respectively. The experimental results of image transformation visualization, binary-way classification and face verification demonstrate that, by performing supervised learning, our models can greatly improve the performance.\n",
      "Existing semantically secure public-key searchable encryption schemes take search time linear with the total number of the ciphertexts. This makes retrieval from large-scale databases prohibitive. To alleviate this problem, this paper proposes searchable public-key ciphertexts with hidden structures (SPCHS) for keyword search as fast as possible without sacrificing semantic security of the encrypted keywords. In SPCHS, all keyword-searchable ciphertexts are structured by hidden relations, and with the search trapdoor corresponding to a keyword, the minimum information of the relations is disclosed to a search algorithm as the guidance to find all matching ciphertexts efficiently. We construct an SPCHS scheme from scratch in which the ciphertexts have a hidden star-like structure. We prove our scheme to be semantically secure in the random oracle (RO) model. The search complexity of our scheme is dependent on the actual number of the ciphertexts containing the queried keyword, rather than the number of all ciphertexts. Finally, we present a generic SPCHS construction from anonymous identity-based encryption and collision-free full-identity malleable identity-based key encapsulation mechanism (IBKEM) with anonymity. We illustrate two collision-free full-identity malleable IBKEM instances, which are semantically secure and anonymous, respectively, in the RO and standard models. The latter instance enables us to construct an SPCHS scheme with semantic security in the standard model.\n",
      "In this paper, we describe  FairMesh , which is the first attempt at mitigating the unfairness  arising from physical layer capture (PLC)  in 802.11 mesh networks. In the presence of PLC, which is surprisingly common in practical mesh networks, existing state-of-art solutions either fail to correctly identify the sender that needs to be throttled or are too aggressive in reducing the sending rate. FairMesh is able to accurately detect unfairness quickly and employs a simple   $CW_{min}$       adjustment algorithm to achieve approximate max-min fairness. Our key insight is that the nodes that cause an unfair situation to arise and can act to remedy it are often distinct from the ones that can accurately assess the degree of unfairness. To the best of our knowledge, we are the first to decouple the detection and assessment of unfairness from the remedial action. A key strength of our approach is its  simplicity , which makes it amenable for deployment in practical 802.11 mesh networks to allow an arbitrary number of flows to operate concurrently without modifications to the 802.11 MAC. We show via simulation and with experiments on a 20-node outdoor 802.11 wireless mesh testbed that FairMesh has many desirable properties. First, it is fully distributed and has negligible control overhead. Second, it achieves approximate max-min fairness, and can be modified to support a different notion of fairness (e.g., proportional fairness). Third, it can handle multiple (more than two) competing links and can scale up to mesh networks with tens of nodes. Fourth, it remains efficient under high data rates and high loss rates. Finally, FairMesh interacts well with TCP and maintains good fairness when a multi-hop flow competes with a single-hop flow.\n",
      "More and more application workflows are computed in cloud and most of them can be expressed by Directed Acyclic Graph DAG. As Cloud resource providers, they should guarantee as many as possible DAGs be accomplished within their deadline when they face the overstep request of computer resource. In this paper, we define the urgency of DAG and introduce the MTMD Maximize Throughput of Multi-DAG with Deadline algorithm to improve the ratio of DAGs which can be accomplished within deadline. The urgency of DAG is changing among execution and determine the execution order of tasks. We can detect DAGs which will exceed the deadline by this algorithm and abandon these DAGs timely. Based on the MTMD algorithm, we put forward the CFS Cost Fairness Scheduling algorithm to reduce the unfairness of cost between different DAGs. The simulation results show that the MTMD algorithm outperforms three other algorithms and the CFS algorithm reduces the cost of all DAGs by 12.1i¾ź% on average and reduces the unfairness among DAGs by 54.5i¾ź% on average.\n",
      "Recently, deep learning techniques have enjoyed success in various multimedia applications, such as image classification and multi-modal data analysis. Two key factors behind deep learning's remarkable achievement are the immense computing power and the availability of massive training datasets, which enable us to train large models to capture complex regularities of the data. There are two challenges to overcome before deep learning can be widely adopted in multimedia and other applications. One is usability, namely the implementation of different models and training algorithms must be done by non-experts without much effort. The other is scalability, that is the deep learning system must be able to provision for a huge demand of computing resources for training large models with massive datasets. To address these two challenges, in this paper, we design a distributed deep learning platform called SINGA which has an intuitive programming model and good scalability. Our experience with developing and training deep learning models for real-life multimedia applications in SINGA shows that the platform is both usable and scalable.\n",
      "In this paper, we consider the delay-optimal fronthaul allocation problem for cloud radio access networks (CRANs). The stochastic optimization problem is formulated as an infinite horizon average cost Markov decision process. To deal with the curse of dimensionality, we derive a closed-form approximate priority function and the associated error bound using perturbation analysis. Based on the closed-form approximate priority function, we propose a low-complexity delay-optimal fronthaul allocation algorithm solving the per-stage optimization problem. The proposed solution is further shown to be asymptotically optimal for sufficiently small cross link path gains. Finally, the proposed fronthaul allocation algorithm is compared with various baselines through simulations, and it is shown that significant performance gain can be achieved.\n",
      "Existing scientific workflow tools, created by computer scientists, require that domain scientists meticulously design their multi-step experiments before analyzing data. However, this is oftentimes contradictory to a domain scientist's routine of conducting research and exploration. This paper presents a novel way to resolve this dispute, in the context of service-oriented science. After scrutinizing how Earth scientists conduct data analytics research in their daily work, a provenance model is developed to record their activities. Reverse-engineering the provenance, a technology is developed to automatically generate workflows for scientists to review and revise, supported by a Petri nets-based workflow verification instrument. In addition, dataset is proposed to be treated as first-class citizen to drive the knowledge sharing and recommendation. A data-centric repository infrastructure is established to catch richer provenance to further facilitate collaboration in the science community. In this way, we aim to revolutionize computer-supported Earth science.\n",
      "As the premier international forum for data science, data mining, knowledge discovery and big data, the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) brings together researchers and practitioners from academia, industry, and government to share their ideas, research results and experiences. Partnered with Bloomberg, it celebrated its 20th years in 2014 with the theme “Data Science for Social Good”. The breadth of topics covered in the 2014 research program is truly comprehensive and nicely balanced among social and information networks, data mining for social good, graph mining, statistical techniques for big data, topic modeling, recommender systems, data streams, scalable methods, Web mining, clustering, feature selection, applications to health care and medicine, public safety, advertising, social analytics, personalization, workforce analytics, health, and many more.\n",
      "We present optimization algorithm for relay (RLY) selection and, source (SRC) and RLY power allocations in a cooperative multicast millimeter-wave (mmWave) system with non-line-of-sight (non-LoS) sensing. We show that there is a significant benefit to the system outage performance by allocating powers for the SRC and the RLYs selected from the candidates availing of line-of-sight (LoS) paths. Specifically, we consider the joint optimization of RLY selection, and, SRC and RLY power allocations to minimize the largest outage probability among all the common-multicast group (co-MGroup) users. The joint optimization problem is non-convex and the complexity of finding the optimal solution is extremely high. Using the dual-step iterative optimization (DIO) algorithm, the joint problem is decomposed into a non-convex RLY selection, and, a convex SRC and RLY power allocation problem. By exploiting the search method and the Lagrange multipliers algorithm, we present efficient algorithms that yield the optimal solutions for both RLY selection (non-convex) and, SRC and RLY power allocation problems. The simulation results indicate significant improvements both on outage probability and power consumption performances over the traditional two-stage multicasting.\n",
      "Specific to the structured operating environment of multi-rows pipe- line, the mode of alternated crawling motion with double claws is employed for kinematic and motion planning. Firstly, designing a 5-DOF robot prototype, and building its kinematical model by employing DENAVIT - HARTENBERG Method for forward and inverse kinematical validation; secondly, decomposing the motions in the structured pipeline environment into three typical motions: single pipe crawling, switch between pipes and between rows of pipes, then determining key gestures in the process of crawling, and then performing inter- polation of joint space trajectory; finally, performing kinematics simulation analysis based on Adams simulation environment software. The result indicates that: the robot prototype can fulfill the crawling motion trajectory along pipe, between pipes, and between rows of pipes; the joint will sustain more force under over-restrained conditions.\n",
      "We study the problem of angle estimation for non-circular sources in bistatic multiple-input multiple-output (MIMO) radar system and propose a low-complexity ESPRIT---Root-MUSIC algorithm for joint direction of departure (DOD) and direction of arrival (DOA) estimation. By taking the non-circularity characteristics of the impinging signals, the received data can be extended, which corresponds to double the number of elements in MIMO radar. Then a novel signal subspace, based on the multistage wiener filter, can be determined directly from the extended data, which does not involve the estimation of covariance matrix and its eigenvalue decomposition (EVD), thus implying that the proposed method is computationally efficient. Finally, the DOD and DOA are estimated by ESPRIT and Root-MUSIC, respectively. Compared with EVD-based algorithms, the proposed method provides the comparable angle estimation performance with lower computational complexity. Some simulation results verify the validity of the proposed method.\n",
      "This paper considers the containment control of networked autonomous underwater vehicles guided by multiple dynamic leaders over a directed network. Each vehicle is subject to model uncertainty and unknown time-varying ocean disturbances. A new predictor-based neural dynamic surface control design approach is presented to develop the adaptive containment controllers, under which the trajectories of vehicles converge to the convex hull spanned by those of the leaders. Specifically, iterative neural updating laws, based on prediction errors, are constructed, which enable the accurate identification of the unknown dynamics for each vehicle, not only in steady state but also in transient state. Furthermore, this result is extended to the output-feedback case where only the position-yaw information can be measured. A neural observer is developed to recover the unmeasured velocity information. Based on the observed velocities of neighboring vehicles, distributed output-feedback containment controllers are devised, under which the containment can be achieved regardless of model uncertainty, unknown ocean disturbances, and unmeasured velocity information. For both cases, Lyapunov-Krasovskii functionals are used to prove the uniform ultimate boundedness of the closed-loop error signals. Comparative studies are given to show the performance improvement of the proposed methods.\n",
      "In this paper, we propose diagonal precoder designs of spatial modulation (SM) by minimizing symbol error rate (SER) upper bound. A general precoder design method applicable for any signal-to-noise ratio (SNR) is first proposed. We then carry out asymptotic analysis of the precoder design in the regimes of high and low SNR. In high SNR regime, our proposed precoder design problem is found to be a non-convex quadratically constrained quadratic program (QCQP) problem that can be solved by semi-definite relaxation (SDR) technique. In low SNR regime, we find the closed form expression of the optimal solution that is equivalent to best antenna selection. Simulation results illustrate that our proposed algorithm can significantly enhance the SER performance and has a better performance than other algorithms.\n",
      "We present a novel hierarchical MRFs optimization method for dense and deformable motion extraction in dynamic scenes. In particular, this hierarchical MRFs structure consists of two layers, the segmentation and the correspondence layer. Firstly, dynamic RGB-D foreground data is segmented through a pixel-level MRF in the segmentation layer. Subsequently, the extracted foreground data is transformed into a 3D point-level MRF in the correspondence layer. A new surface descriptor named deformable color and shape histogram is proposed. It is combined with photometric and geometric features to represent a deformable surface. Finally, the dynamic scene motion is retrieved from correspondences established in the image sequence. Discrete optimization schemes are used for the binary classification and multi-labeling problems. We provide an RGB-D dataset of dynamic scenes, which involves different motion patterns and surface properties of foreground objects. The effectiveness and efficiency of our proposed approach for high accurate foreground segmentation and motion extraction is validated in experiments.\n",
      "In this paper, a real-valued covariance vector sparsity-inducing method for direction of arrival (DOA) estimation is proposed in monostatic multiple-input multiple-output (MIMO) radar. Exploiting the special configuration of monostatic MIMO radar, low-dimensional real-valued received data can be obtained by using the reduced-dimensional transformation and unitary transformation technique. Then, based on the Khatri–Rao product, a real-valued sparse representation framework of the covariance vector is formulated to estimate DOA. Compared to the existing sparsity-inducing DOA estimation methods, the proposed method provides better angle estimation performance and lower computational complexity. Simulation results verify the effectiveness and advantage of the proposed method.\n",
      "Communication is challenging for underwater robots. This paper presents the first research into developing an underwater electric current communication system and integrating the system into a small robotic fish. It is notable for its potential for a group of underwater robots communicating within a short distance range in conditions where optical and acoustic methods would meet difficulty. The working principle of the electric current communication is explained by a simplified electric dipole model. After that, systematic design of the electric current communication system is proposed for underwater robots. Communication experiments with the robotic fish demonstrate the effectiveness of the developed electric current communication system. The experimental results show that a remote control system can communicate underwater with our robotic fish over a distance of three meters by use of electric current communication.\n",
      "Efficiently processing massive data is a big issue in high-speed network intrusion detection, as network traffic has become increasingly large and complex. In this work, instead of constructing a large number of features from massive network traffic, the authors aim to select the most important features and use them to detect intrusions in a fast and effective manner. The authors first employed several techniques, that is, information gain (IG), wrapper with Bayesian networks (BN) and Decision trees (C4.5), to select important subsets of features for network intrusion detection based on KDD'99 data. The authors then validate the feature selection schemes in a real network test bed to detect distributed denial-of-service attacks. The feature selection schemes are extensively evaluated based on the two data sets. The empirical results demonstrate that with only the most important 10 features selected from all the original 41 features, the attack detection accuracy almost remains the same or even becomes better based on both BN and C4.5 classifiers. Constructing fewer features can also improve the efficiency of network intrusion detection.\n",
      "In this paper, we consider the probing order and stopping problem arising from the identification of spectrum holes in multi-channel cognitive radio networks, in which a secondary user (SU) seeks to maximize the probability of finding an available channel while minimizing the related probing cost within a long time horizon. This problem can be casted into a restless multi-armed bandit problem, which is proved to be PSPACE-hard. The key point of this problem is the trade-off between  exploitation , in which the SU stops probing once an available channel is identified, and  exploration , in which the SU continues to probe new channels even after identifying an available channel in order to learn the system state to reduce probing cost in the future. To strike a desirable balance between the two conflicting objectives, we develop a heuristic channel probing policy, termed the    $\\nu$  -step lookahead policy, in which the SU makes its decision based on the prediction of system state within the future    $\\nu$   steps, with    $\\nu$   being a tunable parameter. We conduct an analytical study on the structure of the proposed    $\\nu$  -step lookahead policy and demonstrate how the policy can be implemented with linear complexity with respect to the number of channels in the system via a detailed analysis on the 1-step lookahead policy. Numerical experiments between the    $\\nu$  -step lookahead policy and myopic probing policy on two representative network scenarios demonstrate the effectiveness of the proposed    $\\nu$  -step lookahead policy.\n",
      "In this paper, we consider the dynamic power control for delay-aware D2D communications. The stochastic optimization problem is formulated as an infinite horizon average cost Markov decision process. To deal with the curse of dimensionality, we utilize the interference filtering property of the CSMA-like MAC protocol and derive a closed-form approximate priority function and the associated error bound using perturbation analysis. Based on the closed-form approximate priority function, we propose a low-complexity power control algorithm solving the per-stage optimization problem. The proposed solution is further shown to be asymptotically optimal for a sufficiently large carrier sensing distance. Finally, the proposed power control scheme is compared with various baselines through simulations, and it is shown that significant performance gain can be achieved.\n",
      "Shape descriptors have been identified as important features in distinguishing malignant masses from benign masses. Thus, an effective morphological irregularity measure could provide a helpful reference to indicate the likelihood of malignancy of breast masses. In this paper, a new Fourier-Transform-based measure of irregularity--Fourier Irregularity Index (F2), is proposed to provide reliable malignant/benign tumor/mass classification. The proposed measure has been evaluated on 418 breast masses, including 190 malignant masses and 218 benign lesions identified by radiologists on film mammograms. The results show the proposed measure has better performance than other approaches, such as Compactness Index (CI), Fractal Dimension (FD) and the Fourier-descriptor-based shape Factor (FF). Furthermore, these mentioned measures are paired to investigate the possibility of performance improvement. The results showed the combination of F2 and CI further enhances the performance in indicating the likelihood of malignancy of breast masses.\n",
      "In this paper, by analysing the characteristics of node assignments, network structures and the data transmission in the smart grid, we propose and evaluate a self-healing hierarchical architecture SHA for the ZigBee networks in the smart grid application. The SHA could maintain the network structure based on the link quality indication LQI and the modified neighbour table information to optimise the parent child relationship from the viewpoint of overall network. A backup loop scheme is also introduced to enhance the robustness. A backup link is calculated for each node based on the network address information. The simulation results show that the SHA achieves better performance by obtaining lower average hops, lower transmitter power and less bit error rates. For the backup loop scheme, the result shows it has a greater packet delivery ratio when network topology is damaged.\n",
      "The vertex arboricity $$\\rho (G)$$?(G) of a graph $$G$$G is the minimum number of colors to color $$G$$G such that each color class induces a forest. The list vertex arboricity $$\\rho _l(G)$$?l(G) is the list-coloring version of this concept. Zhen and Wu conjectured that $$\\rho _l(G)=\\rho (G)$$?l(G)=?(G) whenever $$|V(G)|\\le 3\\rho (G)$$|V(G)|≤3?(G). In this paper, we prove the weaker version of the conjecture obtained by replacing $$3\\rho (G)$$3?(G) with $$\\frac{5}{2}\\rho (G)+\\frac{1}{2}$$52?(G)+12.\n",
      "Sensing falsification is a key security threat in cooperative spectrum sensing in cognitive radio networks. Intelligent malicious users (IMUs) adjust their malicious behaviors according to their objectives and the network’s defense schemes. Without long-term collection of information on users’ reputation, the existing schemes fail to thwart such malicious behaviors. In this paper, we construct a joint spectrum sensing and access framework to thwart the malicious behaviors of both rational and irrational IMUs. Lack of reputation information makes the malicious behavior resistance degrade performance since the honest users may be misjudged as IMUs. Based on the moral hazard principal-agent model, we design an incentive compatible mechanism to provide a moderate punishment to IMUs. Our findings show that neither spectrum sensing nor spectrum access alone can prevent malicious behaviors without any information on users’ reputation. According to the different properties of malicious behavior resistance by spectrum sensing and spectrum access, we employ joint spectrum sensing and access to optimally prevent the IMUs sensing falsification. The proposed malicious behavior resistance mechanism is shown to achieve almost the same performance as the ideal case with truthful sensing.\n",
      "The proliferation of WiFi hotspots in public places enables ubiquitous Internet access. These public WiFi hotspots usually serve scores of mobile devices and suffer from extremely poor performance in terms of low good put and severe delay. In this paper, we first study the traffic characteristics in public WiFi networks, and demonstrate that the main causes of such poor performance are media access control (MAC) inefficiency and downlink-uplink traffic asymmetry. To cope with these issues, we call attention to transmission carpool, which facilitates an access point (AP) to send multiple frames for different mobile stations (STAs) in a single transmission. It reduces contention and conveys more frames in each channel access. As such, each downlink transmission carries more payload and thus improves efficiency and solves traffic asymmetry simultaneously.\n",
      "We study efficient query processing for approximate string queries, which find strings within a string collection whose edit distances to the query strings are within the given thresholds. Existing methods typically hinge on the property that globally  similar  strings must share at least certain number of  identical  substrings or subsequences. They become ineffective when there are burst errors or when the number of errors is large. In this paper, we explore the opposite paradigm focusing on finding out the  differences  of database strings to the query string. We propose a new filtering method, called  local filtering , based on the idea that two strings exhibiting substantial local  dissimilarities  must be globally  dissimilar . We propose the concept of (positional) local distance to quantify the minimum amount of errors a query fragment contributes to the edit distance between the query and a data string. It also leads to effective pruning rules and can speed up verification via early termination. We devise a family of indexing methods based on the idea of precomputing (positional) local distances for all possible combinations of query fragments and edit distance thresholds. Based on careful analyses of subtle relationships among local distances, novel techniques are proposed to drastically reduce the amount of enumeration with no or little impact on the pruning power. Efficient query processing methods exploiting the new index and bit-parallelism are also proposed. Experimental results on real datasets show that our local filtering-based methods can achieve substantial speedup compared with state-of-the-art methods, and they are robust against factors such as dataset characteristics and large edit distance thresholds.\n",
      "This study proposes a hybrid method, which combines community finding (CF) of complex network with case-based reasoning (CBR) for industrial application. The process of case base construction, case retrieval and case reuse is designed in this paper. A developed application software based on the proposed approach has been successfully applied to the control center of a copper-molybdenum ore concentration plant in China. The figure displays an overall structural chart of the application. The overflow particle size of cyclone is one of the most significant performance indices in the process of ore grinding. Given the measuring difficulty under the current industrial conditions, a hybrid method, which combines community finding (CF) of a complex network with case-based reasoning (CBR) is proposed in this study. The CF method with a new evaluation criterion for the vertex combination degree is designed to select the typical cases from the constructed communities, and a k-nearest neighbors (k-NN) based strategy with multi-similarity threshold is proposed in the case retrieval process. To verify the effectiveness of the proposed method, a number of comparative simulations by using the real-world data coming from a copper-molybdenum concentration plant are carried out, and the results indicate that the proposed method can provide a good measure quality for the industrial application.\n",
      "In this paper, a single-stage LED driver is proposed for a street lighting system. Two boost circuits that share a single inductor are formed by integrating the switches of a half-bridge    $LLC$   resonant converter. Both boost circuits operate in the boundary conduction mode, which realizes a power-factor correction function. Because the input voltage of the LED driver is divided by two capacitors, the bus voltage is considerably reduced to almost the input peak voltage, rendering the novel single-stage LED driver to work suitably under high-input-voltage conditions. The soft-switching characteristics of the half-bridge    $LLC$   resonant circuit are not affected by the integration of the switches; thus, the converter has a low cost and a high efficiency. A 100-W prototype was developed, and the efficiency was determined to be as high as 91.1% in a full-load state under a 220-V ac input.\n",
      "Music identification via audio fingerprinting has been an active research field in recent years. In the real-world environment, music queries are often deformed by various interferences which typically include signal distortions and time-frequency misalignments caused by time stretching, pitch shifting, etc. Therefore, robustness plays a crucial role in music identification technique. In this paper, we propose to use scale invariant feature transform (SIFT) local descriptors computed from a spectrogram image as sub-fingerprints for music identification. Experiments show that these sub-fingerprints exhibit strong robustness against serious time stretching and pitch shifting simultaneously. In addition, a locality sensitive hashing (LSH)-based nearest sub-fingerprint retrieval method and a matching determination mechanism are applied for robust sub-fingerprint matching, which makes the identification efficient and precise. Finally, as an auxiliary function, we demonstrate that by comparing the time-frequency locations of corresponding SIFT keypoints, the factor of time stretching and pitch shifting that music queries might have experienced can be accurately estimated.\n",
      "This paper introduces a 2-D index map coding of the palette mode in screen content coding extension of the High-Efficiency Video Coding (HEVC SCC) standard to further improve the compression performance. In contrast to the current 1-D search using RUN to represent the length of matched string, we bring the block width and height to describe the arbitrary rectangle shape. We also use the block vector displacement to signal the matched block distance efficiently. By enlarging the search range from current coding tree unit (CTU) to a small neighbor CTU window (i.e., 3x5 CTUs), it provides the coding efficiency comparable to the case that full-frame intra block copy is used. It is more practical to use the local search window in real life considering the trade-off between the coding efficiency and implementation cost.\n",
      "In many scientific computing applications, sparse Cholesky factorization is used to solve large sparse linear equations in distributed environment. GPU computing is a new way to solve the problem. However, sparse Cholesky factorization on GPU is hardly to achieve excellent performance due to the structure irregularity of matrix and the low GPU resource utilization. A hybrid CPU-GPU implementation of sparse Cholesky factorization is proposed based on multifrontal method. A large sparse coefficient matrix is decomposed into a series of small dense matrices (frontal matrices) in the method, and then multiple GEMM (General Matrix-matrix Multiplication) operations are computed. GEMMs are the main operations in sparse Cholesky factorization, but they are hardly to perform better in parallel on GPU. In order to improve the performance, the scheme of multiple task queues is adopted when performing multiple GEMMs parallelized with multifrontal method; all GEMM tasks are scheduled dynamically on GPU and CPU based on computation scales for load balance and computing-time reduction. Experimental results show that the approach can outperform the implementations of BLAS and cuBLAS, achieving up to 3.15× and 1.98× speedup, respectively.\n",
      "According to the recent rule released by Health and Human Services (HHS), healthcare data can be outsourced to cloud computing services for medical studies. A major concern about outsourcing healthcare data is its associated privacy issues. However, previous solutions have focused on cryptographic techniques which introduce significant cost when applied to healthcare data with high-dimensional sensitive attributes. To address these challenges, we propose a privacy-preserving framework to transit insensitive data to commercial public cloud and the rest to trusted private cloud. Under the framework, we design two protocols to provide personalized privacy protections and defend against potential collusion between the public cloud service provider and the data users. We derive provable privacy guarantees and bounded data distortion to validate the proposed protocols. Extensive experiments over real-world datasets are conducted to demonstrate that the proposed protocols maintain high usability and scale well to large datasets.\n",
      "The Chinese Government and citizens face enormous challenges of disaster management as widespread devastation, economic damages, and loss of human lives caused by increasing natural disasters. Disaster management requires a complicated iterative process that includes disaster monitoring, early detection, forecasting, loss assessment, and efficient analysis of disaster reduction. Each task typically involves the use of technologists and multiple geospatial information resources, including sensors, data sources, models, geo-tools, software packages, and computing resources. However, most existing disaster management systems operate in a typical passive data-centric mode, where resources cannot be fully utilized. This impediment is partially being addressed by the increasingly complex application requirements and the growing availability of diverse resources. In this paper, we summarize and analyze the practical problems experienced by the National Disaster Reduction Application System of China. To address t...\n",
      "Mobile opportunistic network (MON) is a new paradigm which exploits node contacts to forward data, enabling numerous and impressive applications. The data copy spraying scheme is a challenging problem in MON, due to the mobility of nodes and lack of global knowledge, it hence captures great interests from research communities. Traditional algorithms allocate data copies with node's statistical information and neglect the temporal contact feature, resulting in a poor delivery performance. We propose AS, an adaptive data copy spraying scheme in MON. AS adjusts the number of copies dynamically based on the temporal contact feature among nodes. Theoretical analysis verifies that AS achieves a lower mean delivery delay than SprayWait, one of the state-of-the-art works. Simulation results show that AS improves the packet delivery ratio simultaneously.\n",
      "In cognitive radio networks (CRNs), collaborative sensing has been considered an attractive means to improving spectrum sensing performance. However, privacy issues arise when multiple service providers (SPs) collaborate on learning the spectrum availabilities. Specifically, sharing sensing data may enable malicious SPs or secondary users (SUs) to geolocate an SU using existing localization techniques. These malicious entities could be untrusted SPs/SUs or external attackers that compromise SPs/SUs. To incentivize SUs to contribute their sensing data, the privacy of each SU should be guaranteed. In this paper, we propose a privacy preservation framework called   PrimCos   for multi-SP collaborative sensing, which addresses several competing challenges not yet considered in the literature, that is, being compatible with general collaborative sensing schemes, providing privacy guarantee for each SU and ensuring worst case privacy protection under collusion. Both analytical and numerical results show that the proposed framework provides privacy protection for each SU with controllable impact on the sensing performance under different types of attacks.\n",
      "Purpose – The paper aims to present a new thought for design of a thrown robot based on flexible structures. The aim of the design is to reduce the weight and improve the anti-impact capability for mini thrown robot. Design/methodology/approach – A mass-spring wheeled robot model is proposed and an impact analysis is given in this paper. Some principia were derived for configuration design and material choice to get a light and robust thrown reconnaissance robot. Based on the theoretical analysis, flexible elements like flexure hinges or rubber shell were utilized to build two generation of robots that both showed excellent performances of anti-impact ability. Findings – A second-generation thrown robot (2,050 g) was developed, which could survive dropping from the height of 6 m more than 10 times without apparent damage. Originality/value – The method based on the flexible structure provides the thrown robot with high survivability from impact, as well as light weight. It can be used in the design of the...\n",
      "This paper investigates the problem of platoon control with sensor range limitation. A nonlinear vehicular platoon model is established, in which the sensing range constraint described by a piecewise nonlinear function is involved. Then a robust nonlinear control design method is proposed based on a disturbance observer and the backstepping technique. The results are obtained in the context of both individual vehicle stability and platoon string stability analysis, which can lead to substantially enhanced platoon control performance with a guaranteed level of attenuation of the disturbance caused by lead vehicle acceleration and wind gust. The effectiveness of the method has been shown by numerical simulations and experiments carried out with Arduino cars.\n",
      "In social network, users generated multi-typed entities and complex interactive relations. The relational data mining is a hot research in social computing. Co-clustering algorithms have been proposed to mine underlying structure of different entities in heterogeneous social network. However, the real heterogeneous relational data are very sparse. In this paper, we propose a fast High-order Sparse Non-negative Matrix Factorization algorithm to co-cluster heterogeneous sparse relational data based on Correlation Matrix(HSNMF-CM), which is built by the correlation relations of small entities. In HSNMF-CM, the sparseness and size of matrix are reduced simultaneously. Under the sparse constraint, the block coordinate descent algorithms are used to accelerate the convergence rate of the matrix factorization. We assess the performance of the HSNMF-CM on two social data sets. The results show that our algorithm outperforms state-of-the-art algorithms on accuracy and convergence speed, and possesses a high scalability on large-scale heterogeneous relational data sets.\n",
      "Large data centers are usually built to support increasing computational and data storage demand of growing global business and industry, which consume an enormous amount of energy, at a huge cost to both business and the environment. However, much of that energy is wasted to maintain excess service capacity during periods of low load. In this paper, we investigate the problem of \"right-sizing\" data center for energy-efficiency through virtualization which allows consolidation of workloads into smaller number of servers while dynamically powering off the idle ones. In view of the dynamic nature of data centers, we propose a stochastic model based on Queueing theory to capture the main characteristics. Solving this model, we notice that there exists a tradeoff between the energy consumption and performance. We hereby develop a BFGS based algorithm to optimize the tradeoff by searching for the optimal system parameter values for the data center operators to \"right-size\" the data centers. We implement our Stochastic Right-sizing Model (SRM) and deploy it in the real-world cloud data center. Experiments with two real-world workload traces show that SRM can significantly reduce the energy consumption while maintaining high performance. A Stochastic Right-sizing Model(SRM) based on Queueing theory is proposed.A BFGS based algorithm is proposed to achieve energy-efficiency.SRM is implemented with open-source cloud platform OpenStack.\n",
      "Image matting refers to the problem of accurately extracting the foreground object from an image. The trimap containing labels of known foreground, known background and unknown has been broadly used to reduce solution space of the problem. The matte of an unknown pixel can be solved using samples from its neighbor known foreground and known background. However, the existing methods of color-based sampling may fail when foreground and background share similar colors, meanwhile, automatically generated trimap may not properly cover foreground boundary. In this paper, we propose a novel matting method using depth-assisted sampling and adaptive trimap generation. We use depth to assist color for improving sample selection and generate trimap based on color distribution of local unknown regions to make it cover foreground boundary adaptively. Our experiments show the effectiveness of proposed method.\n",
      "We investigate the emulation controller design approach for nonlinear networked control systems (NCS) with FlexRay. FlexRay is a deterministic communication protocol which is increasingly used in the automotive industry as it provides a high bandwidth and allows for safety critical applications. It is characterized by pre-set communication cycles that are subdivided into static and dynamic segments; the data transmissions are scheduled by different rules depending on the segment. We propose for the first time a hybrid model of NCS with FlexRay for this purpose. We show, under reasonable assumptions, that the asymptotic stability property ensured by the controller in the absence of communication constraints is preserved when the latter is implemented over FlexRay with sufficiently frequent data transmission. In particular, we assume that on each communication segment, the data transmissions are governed by uniformly globally exponentially stable protocols. This covers the case when the round-robin protocol is implemented on the static segment and the try-once-discard protocol is implemented on the dynamic segment. We provide explicit maximum allowable transmission interval bounds that guarantee stability.\n",
      "Sensor-equipped smartphones as well as wearable devices have undoubtedly become the predominant source of user-generated data in mobile networks. The proliferation of user-generated data has created a plethora of opportunities for personalized services based on the states of users and their surrounding environments. Those personalized services, although improving users’ perceived quality of experience, have raised severe privacy concerns, as most of these applications aggressively collect users’ personal data without providing clear statements on the usage and disclosure policies of such sensitive information. In order to sustain personalized services with long-term privacy preservation, disruptive paradigms are required. We envision that context awareness is a key pillar to providing long-term quality of protection (QoP) for individual privacy. In particular, users transit between different contexts, including mobility modes and social activities, and these contexts are temporally or logically correlated, which can be leveraged by adversaries to compromise users’ privacy. In addition, users may have different QoP preferences in different contexts. With these salient features in mind, this article investigates context-aware QoP mechanism designs for personalized services in mobile networks. We discuss possible attacks and propose corresponding countermeasures. In particular, we develop a QoP framework that exploits context awareness to achieve better trade-offs between service quality and privacy protection in long-term services. Finally, we provide some implications for future context-aware QoP mechanism designs by conducting a case study on smartphone traces.\n",
      "The requirement of a large number of sizeable capacitors in traditional modular multilevel converter (MMC) submodules is seen as a major barrier hampering its widespread use in medium-voltage applications. To reduce the value of capacitance, this paper presents a modified MMC (M-MMC) topology, which offers inherent alleviated capacitor voltage fluctuations compared with traditional MMC. Its operating principle, modulation, and capacitor voltage balancing strategies are described in detail. Moreover, a novel capacitor voltage fluctuation suppression scheme is also proposed to further reduce the second-order capacitor voltage ripples. Finally, both a three-phase prototype of M-MMC and traditional MMC have been built in the laboratory. The effectiveness of the proposed M-MMC topology and control methods is experimentally verified by comparing with the traditional MMC.\n",
      "The problem of food safety is a critical issue in recent years. To address the issue, the technologies of the Internet of Things are used to offer the possibilities to easily track the processes in the production, storage, transportation, sale, and even using phases of foods. This paper, therefore, introduces the design of an electronic pedigree system for food safety, which uses electronic pedigrees to enhance the safety of food supply. The system implements an extension of the pedigree standard of EPCglobal, and offers a more trustworthily tracking service to monitor and supervise the production and supply of food. We discuss the key issues of the design, and implement a prototype to evaluate the feasibility of the design. Finally, we analyze the trustworthiness assurance and security of our electronic pedigree system.\n",
      "In wireless device-to-device (D2D) networks, devices are reluctant to forward packets because of limited energy and possible delays for their own data. The incentive mechanisms that motivate devices to constitute direct communication for wireless multimedia quality optimality in D2D systems have been overlooked in the past. In this paper, we propose a new low-complexity distributed game-theoretic source selection and power control scheme that enhances the multimedia transmission quality with latency constraints. This approach has two major contributions. First, the proposed approach optimally selects the most beneficial source devices by analyzing the interactions between the base station's (BS's) rewarding strategies (denoted by price) and the devices' contributing behaviors (denoted by transmission power) using a Stackelberg game model. Second, optimal transmission power is adjusted for each selected source device in D2D networks by deriving Stackelberg equilibrium, wherein the BS and the device both achieve maximum utility. Computer simulations demonstrate that significant improvement in D2D multimedia transmission quality can be obtained by deploying the proposed scheme.\n",
      "In this paper, we propose a novel global object descriptor, so-called Viewpoint oriented Color-Shape Histogram (VCSH), which combines 3D object's color and shape features. The descriptor is efficiently used in a real-time textured/textureless object recognition and 6D pose estimation system, while also applied for object localization in a coherent semantic map. We build the object model first by registering from multi-view color point clouds, and generate partial-view object color point clouds from different synthetic viewpoints. Thereafter, the extracted color and shape features are correlated as a VCSH to represent the corresponding object patch data. For object recognition, the object can be identified and its initial pose is estimated through matching within our built database. Afterwards the object pose can be optimized by utilizing an iterative closest point strategy. Therefore, all the objects in the observed area are finally recognized and their corresponding accurate poses are retrieved. We validate our approach through a large number of experiments, including daily complex scenarios and indoor semantic mapping. Our method is proven to be efficient by guaranteeing high object recognition rate, accurate pose estimation result as well as exhibiting the capability of dealing with environmental illumination changes.\n",
      "Multimodal learning has been mostly studied by assuming that multiple label assignments are independent of each other and all the modalities are available. In this paper, we consider a more general problem where the labels contain dependency relationships and some modalities are likely to be missing. To this end, we propose a multi-label conditional restricted Boltzmann machine (ML-CRBM), which handles modality completion , fusion, and multi-label prediction in a unified framework. The proposed model is able to generate missing modalities based on observed ones, by explicitly modelling and sampling their conditional distributions. After that, it can discriminatively fuse multiple modalities to obtain shared representations under the supervision of class labels. To consider the co-occurrence of the labels, the proposed model formulates the multi-label prediction as a max-margin-based multi-task learning problem. Model parameters can be jointly learned by seeking a balance between being generative for modality generation and being discriminative for label prediction. We perform a series of experiments in terms of classification, visualization, and retrieval, and the experimental results clearly demonstrate the effectiveness of our method.\n",
      "The influence of social interactions among mobile devices and network components in wireless networks has attracted substantial attention due to its potential impact on resource allocation of spectrum and power in particular. We present an organized social graphical view on resource allocation and then extend to multi-objective resource allocation of wireless networks. We subsequently consider taking advantage of multi-dimensional resources, including radio resource, user behavior, and content characteristics, such that we can successfully integrate caching capability, interest similarity, and content popularity and distribution into wireless network design. As an illustration, device-to-device communications is utilized to form pairs and clusters of mobile devices regarding optimal resource matching via a bipartite graph. This socially enabled methodology highlights new potential to design wireless networks and 5G mobile communications.\n",
      "Infectious disease epidemics such as influenza and Ebola pose a serious threat to global public health. It is crucial to characterize the disease and the evolution of the ongoing epidemic efficiently and accurately. Computational epidemiology can model the disease progress and underlying contact network, but suffers from the lack of real-time and fine-grained surveillance data. Social media, on the other hand, provides timely and detailed disease surveillance, but is insensible to the underlying contact network and disease model. This paper proposes a novel semi-supervised deep learning framework that integrates the strengths of computational epidemiology and social media mining techniques. Specifically, this framework learns the social media users' health states and intervention actions in real time, which are regularized by the underlying disease model and contact network. Conversely, the learned knowledge from social media can be fed into computational epidemic model to improve the efficiency and accuracy of disease diffusion modeling. We propose an online optimization algorithm to substantialize the above interactive learning process iteratively to achieve a consistent stage of the integration. The extensive experimental results demonstrated that our approach can effectively characterize the spatiotemporal disease diffusion, outperforming competing methods by a substantial margin on multiple metrics.\n",
      "Some pioneer WiFi signal based human activity recognition systems have been proposed. Their key limitation lies in the lack of a model that can quantitatively correlate CSI dynamics and human activities. In this paper, we propose CARM, a CSI based human Activity Recognition and Monitoring system. CARM has two theoretical underpinnings: a CSI-speed model, which quantifies the correlation between CSI value dynamics and human movement speeds, and a CSI-activity model, which quantifies the correlation between the movement speeds of different human body parts and a specific human activity. By these two models, we quantitatively build the correlation between CSI value dynamics and a specific human activity. CARM uses this correlation as the profiling mechanism and recognizes a given activity by matching it to the best-fit profile. We implemented CARM using commercial WiFi devices and evaluated it in several different environments. Our results show that CARM achieves an average accuracy of greater than 96%.\n",
      "Idea Graph is a core component of Idea Discovery, which discovers idea by converting unstructured data into a scenario graph. Since the scenario graph is complex and heterogeneous, the layouts generated by general graph layout algorithms can't well support human cognition. To tackle this issue, a novel graph layout algorithm named CiFDAL is proposed, which is a hybrid of circular layout algorithm and Force-Directed Algorithm (FDA). Overall, it places clusters and key nodes on two nested circles, and different hierarchies distinguish their different importance. Specifically, it uses FDA to ensure the related nodes being closer in distance. Additionally, it improves the classical random initialized FDA by adopting a better initial position set and iterative node-position exchange technique. Experimental results demonstrate the superiority of our algorithm by comparing with several benchmarks implemented in a famous visualization tool - GraphViz.\n",
      "Novel advance reservation (AR) requests, such as user data replication and grid computing, which are different from immediate reservation requests, allow certain initial-delay during setting up, as long as the resources are allocated before a preset deadline. Empowered by the optical orthogonal frequency-division multiplexing (O-OFDM) technology, the elastic optical networks (EON) support channels operating at heterogeneous line rates by allocating spectral resources in a flexible and dynamic manner. However, in a dynamic traffic scenario, dynamic path setup and teardown operation will inevitably lead to spectral fragmentation. In this paper, to effectively allocate the spectrum in a suitable duration along a better route, we introduce a notion of available time-spectrum consecutiveness (TSC) to describe the spectrum fragmentations along frequency axis and time axis. Based on TSC, three time-aware routing and spectrum assignment (RSA) algorithms are proposed. The novel algorithms retain the TSC as much as possible when establishing a lightpath and reduce the spectrum fragmentation. Simulation results indicate three proposed algorithms achieve lower blocking probability and higher spectrum efficiency.\n",
      "Infrastructure-as-a-Service clouds offer diverse pricing options, including on-demand and reserved instances with various discounts to attract different cloud users. A practical problem facing cloud users is how to minimize their costs by choosing among different pricing options based on their own demands. In this paper, we propose a new cloud brokerage service that reserves a large pool of instances from cloud providers and serves users with price discounts. The broker optimally exploits both pricing benefits of long-term instance reservations and multiplexing gains. We propose dynamic strategies for the broker to make instance reservations with the objective of minimizing its service cost. These strategies leverage dynamic programming and approximation algorithms to rapidly handle large volumes of demand. Our extensive simulations driven by large-scale Google cluster-usage traces have shown that significant price discounts can be realized via the broker. Index Terms—Cloud computing, cloud brokerage, cost management, instance reservation, approximation algorithm.\n",
      "Since a vehicle logo is the clearest indicator of a vehicle manufacturer, most vehicle manufacturer recognition (VMR) methods are based on vehicle logo recognition. Logo recognition can be still a challenge due to difficulties in precisely segmenting the vehicle logo in an image and the requirement for robustness against various imaging situations simultaneously. In this paper, a convolutional neural network (CNN) system has been proposed for VMR that removes the requirement for precise logo detection and segmentation. In addition, an efficient pretraining strategy has been introduced to reduce the high computational cost of kernel training in CNN-based systems to enable improved real-world applications. A data set containing 11 500 logo images belonging to 10 manufacturers, with 10 000 for training and 1500 for testing, is generated and employed to assess the suitability of the proposed system. An average accuracy of 99.07% is obtained, demonstrating the high classification potential and robustness against various poor imaging situations.\n",
      "In multivariate systems, the causality relationships between any two different data variables and the corresponding path models are often unknown. In this paper, the identification of bidirectional path models of a bivariate system is investigated by extending the augmented UD identification (AUDI) algorithm proposed by Niu   (1992) which can simultaneously identify the order and parameters for open-loop systems with unclear physical meanings of the even columns in the data matrix. To extract more information than the AUDI algorithm for identification of bidirectional path models, we develop a novel approach based on construction of the interleave data vector and UD factorization of the data matrix. The odd and even columns of the resulting data matrix correspond to the parameters of the forward and backward path models, respectively. Moreover, the information contained in the data matrix can be evaluated to determine the causality between the two data variables. The ARMAX process with white noise is first considered. The results are then extended to the case with colored noise. Simulation results are presented to show the effectiveness of our proposed methods.\n",
      "A scalable platform supporting large-scale sensor service discovery.A discovery approach more resilient to dynamicity of sensor services and environment.Accurate computation of the geographical features of gateways and sensor services.Superior performance over existing methods for response time and throughput. The Internet of Things enables human beings to better interact with and understand their surrounding environments by extending computational capabilities to the physical world. A critical driving force behind this is the rapid development and wide deployment of wireless sensor networks, which continuously produce a large amount of real-world data for many application domains. Similar to many other large-scale distributed technologies, interoperability and scalability are the prominent and persistent challenges. The proposal of sensor-as-a-service aims to address these challenges; however, to our knowledge, there are no concrete implementations of techniques to support the idea, in particular, large-scale, distributed sensor service discovery. Based on the distinctive characteristics of the sensor services, we develop a scalable discovery architecture using geospatial indexing techniques and semantic service technologies. We perform extensive experimental studies to verify the performance of the proposed method and its applicability to large-scale, distributed sensor service discovery.\n",
      "Map data are widely used in mobile services, but most maps might not be complete. Updating the map automatically is an important problem because road networks are frequently changed with the development of the city. This paper studies the problem of recovering missing road segments via GPS trajectories, especially low sampled data. Our approach takes the GPS noise into consideration and proposes an effective self-adaptive algorithm. Besides, we propose theoretical models behind all the important parameters to enable self-adaptive parameter setting. To the best of our knowledge, this is the first work that addresses the parameter setting issue successfully to make sure our approach is free of parameter-tuning. In addition, we also propose a quantitative evaluation method for map updating problem. The result shows our algorithm has a much better performance than the existing approaches.\n",
      "In this paper, the risk-sensitive filtering problem with time-varying delay is investigated. The problem is transformed into Krein space as an equivalent optimisation problem. The observations with time-varying delays are restructured as ones with multiple constant delays by defining a binary variable model with respect to the arrival process of observations, containing the same state information as the original. Finally, the reorganised innovation analysis approach in Krein space allows the solution to the proposed risk-sensitive filtering in terms of the solutions to Riccati and matrix difference equations.\n",
      "We present a hierarchical grid-based, globally optimal tracking-by-detection approach to track an unknown number of targets in complex and dense scenarios, particularly addressing the challenges of complex interaction and mutual occlusion. Frame-by-frame detection is performed by hierarchical likelihood grids, matching shape templates through a fast oriented distance transform. To allow recovery from misdetections, common heuristics such as nonmaxima suppression within observations is eschewed. Within a discretized state-space, the data association problem is formulated as a grid-based network flow model, resulting in a convex problem casted into an integer linear programming form, giving a global optimal solution. In addition, we show how a behavior cue (body orientation) can be integrated into our association affinity model, providing valuable hints for resolving ambiguities between crossing trajectories. Unlike traditional motion-based approaches, we estimate body orientation by a hybrid methodology, which combines the merits of motion-based and 3D appearance-based orientation estimation, thus being capable of dealing also with still-standing or slowly moving targets. The performance of our method is demonstrated through experiments on a large variety of benchmark video sequences, including both indoor and outdoor scenarios.\n",
      "In the age of big data, the emitter parameter measurement data is generally characteristic of uncertainty in the form of normally-distributed intervals, enormous size and continuous growth. However, existing interval-valued data analysis methods generally assume a uniform distribution instead and are unable to adapt to the rapid growth of volume. To address the above problems, we have brought forward an incremental distributed weighted class discriminant analysis method on interval-valued emitter parameters. Extensive experiments indicate that our method is able to cope with these new characteristics effectively.\n",
      "Network clustering is an important problem thathas recently drawn a lot of attentions. Most existing workfocuses on clustering nodes within a single network. In manyapplications, however, there exist multiple related networks, inwhich each network may be constructed from a different domainand instances in one domain may be related to instances in otherdomains. In this paper, we propose a robust algorithm, MCA, formulti-network clustering that takes into account cross-domain relationshipsbetween instances. MCA has several advantages overthe existing single network clustering methods. First, it is ableto detect associations between clusters from different domains, which, however, is not addressed by any existing methods. Second, it achieves more consistent clustering results on multiple networksby leveraging the duality between clustering individual networksand inferring cross-network cluster alignment. Finally, it providesa multi-network clustering solution that is more robust to noiseand errors. We perform extensive experiments on a variety ofreal and synthetic networks to demonstrate the effectiveness andefficiency of MCA.\n",
      "Fixed channelization configuration in today's wireless devices falls inefficient in the presence of growing data traffic and heterogeneous devices. In this regard, a number of fairly recent studies have provided spectrum adaptation capabilities for current wireless devices, however, they are limited to inband adaptation or incur substantial coordination overhead. The target of this paper is to fill the gaps in spectrum adaptation by overcoming these limitations. We propose Seer, a frame-level wideband spectrum adaptation system which consists of two major components: i) a specially-constructed preamble that can be detected by receivers with arbitrary RF bands, and ii) a spectrum detection algorithm that identifies the intended transmission band in the context of multiple asynchronous senders by exploiting the preamble's temporal and spectral properties. Seer can be realized on commodity radios, and can be easily integrated into devices running different PHY/MAC protocols. We have prototyped Seer on the GNURadio/USRP platform and tested it under various environments. Furthermore, our evaluation using 1.6GHz spectrum measurements shows that Seer largely improves system throughput over fixed channel configuration and state-of-the-art spectrum adaptation approaches.\n",
      "Traditional anomaly detection on microblogging mostly focuses on individual anomalous users or messages. Since anomalous users employ advanced intelligent means, the anomaly detection is greatly poor in performance. In this paper, we propose an innovative framework of anomaly detection based on bipartite graph and co-clustering. A bipartite graph between users and messages is built to model the homogeneous and heterogeneous interactions. The proposed co-clustering algorithm based on nonnegative matrix tri-factorization can detect anomalous users and messages simultaneously. The homogeneous relations modeled by the bipartite graph are used as constraints to improve the accuracy of the co-clustering algorithm. Experimental results show that the proposed scheme can detect individual and group anomalies with high accuracy on a Sina Weibo dataset.\n",
      "Due to the deployment limitations of global positioning system (GPS), a pseudo geometric broadcast approach is investigated in resource constrained wireless sensor networks (WSNs). Without relying on the GPS support, a set of nodes closely located in a strategic position is judiciously searched and identified as a forwarding node. In this paper, we first propose both enhanced ad hoc broadcast protocol (AHBP) with target forwarding nodes (EBP(|N ƒ |)) and node distribution sensitive broadcast (NDS) to maximize the network coverage while minimizing the number of broadcasts. Second, we revisit prior AHBP and approximating neighbor nodes based broadcast protocol (Approx) for investigation in terms of network coverage. For performance comparison study, we modify the AHBP and broadcast protocol for sensor networks (BPS), denoted as BPS*. We conduct extensive simulation experiments using OMNeT++ and analyze the performance of protocols. Extensive simulation results indicate that the proposed EBP(|N ƒ |) and NDS protocols achieve competitive performance and can be a viable approach in WSNs.\n",
      "With the rapid growth of multimedia data, it is very desirable to effectively and efficiently search objects of interest across different modalities from large scale databases. Cross-modal hashing provides a very promising way to address such problem. In this paper, we propose a two-step cross-modal hashing approach to obtain compact hash codes and learn hash functions from multimodal data. Our approach decomposes the cross-modal hashing problem into two steps: generating hash code and learning hash function. In the first step, we obtain the hash codes for all modalities of data via a joint multi-modal graph, which takes into consideration both the intra-modality and inter-modality similarity. In the second step, learning hashing function is formulated as a binary classification problem. We train binary classifiers to predict the hash code for any data object unseen before. Experimental results on two cross-modal datasets show the effectiveness of our proposed approach.\n",
      "A software module named flash translation layer (FTL) running in the controller of a flash SSD exposes the linear flash memory to the system as a block storage device. The effectiveness of an FTL significantly impacts the performance and durability of a flash SSD. In this research, we propose a new FTL called PCFTL (Plane-Centric FTL), which fully exploits plane-level parallelism supported by modern flash SSDs. Its basic idea is to allocate updates onto the same plane where their associated original data resides on so that the write distribution among planes is balanced. Furthermore, it utilizes fast intra-plane copy-back operations to transfer valid pages of a victim block when a garbage collection occurs. We largely extend a validated simulation environment called SSDsim to implement PCFTL. Comprehensive experiments using realistic enterprise-scale workloads are performed to evaluate its performance with respect to mean response time and durability in terms of standard deviation of writes per plane. Experimental results demonstrate that compared with the well-known DFTL, PCFTL improves performance and durability by up to 47 and 80 percent, respectively. Compared with its earlier version (called DLOOP), PCFTL enhances durability by up to 74 percent while delivering a similar I/O performance.\n",
      "Inspired by the green communication trend of next-generation wireless networks, we propose an opportunistic wireless information and energy transfer relaying scheme for sustainable cooperative relaying, in which the relay nodes are powered only by their independent harvested energy to forward the desired information to the destination. Thus there exists an inherent tradeoff between the information decoding (ID) and the energy harvesting (EH) for forwarding. We first analyze the tradeoff and formulate an optimization problem on joint EH/ID receive mode selection and transmit power control for the relay nodes. By Lagrangian optimality, we obtain the optimal solution in opportunistic relay and power control. Due to the NP-hard property of the EH/ID mode selection, we propose a low-complexity algorithm by relaxation for EH/ID mode selection. The simulation results show that the performance of the low-complexity algorithm is close to the optimal solution, and outperforms the baselines.\n",
      "While e-commerce has grown substantially over last several years, more and more people are utilizing this popular channel to purchase products and services. Thus the ability to predict user demographics, including gender, age and location has important applications in advertising, personalization, and recommendation. In this paper, we aim to automatically predict the users' genders based on their product viewing logs. Our study is based on a dataset from PAKDD'15 data mining competition. We propose an architecture for gender prediction, which consists of the \"machine learning model\" and the \"label updating function\". The experimental results show that our proposed method significantly outperform baseline methods. A detailed analysis of features provides an entertaining insight into behavior variation on female and male users.\n",
      "The hull stress monitoring system is to measure and display the ship motions and real-time stresses by strain gauge and accelerometer sensor networks. The statistical parameters such as standard deviation of measurements through hull stress sensor network are important for analysis of hull structure status. Due to the large amount of measurement data, it is difficult to acquire the standard deviation directly. Nested array is a sparse sampling algorithm which can keep the statistical property of the original data. This paper presents an algorithm for standard deviation computed of hull stress data based on nested array. From the experimental results, it can be seen that the provided algorithm can achieve higher accurate distribution of standard deviation with much less samples. This proves that the nested array sampling could be used in statistical computing for hull stress data.\n",
      "This paper presents an approach for real-time multivehicle tracking and counting under fisheye camera based on simple feature points tracking, grouping and association. Different from traditional cameras, the main challenge under fisheye cameras is that the objects being tracked suffer from severe distortion and perspective effects in even adjacent frames. As a result, the points can be stably matched by a point tracker are much fewer, and the points even lose tracking completely quite occasionally. Firstly, to preserve points discrimination in dynamic grouping, we propose an approach based on motion similarity and neighbor weighted grafting to transfers motion knowledge between long and short point trajectories. Moreover, to deal with cases such as points losing tracking completely or incorrect points grouping, we also propose a concept of points \"identity-appearance\" that integrates constrained motion for association between vehicle track lets and segmented point groups. Our approach also overcomes several common challenges in traffic surveillance such as stopping vehicles, pedestrians and counting of linked (partially occluded) vehicles. Finally, extensive experimental results are provided on challenging fisheye image sequences to demonstrate the robustness and effectiveness of the approach.\n",
      "Modeling of a new domain can be challenging due to scarce data and high-dimensionality. Transfer learning aims to integrate data of the new domain with knowledge about some related old domains, to model the new domain better. This article studies transfer learning for degenerate biological systems. Degeneracy refers to the phenomenon that structurally different elements of the system perform the same/similar function or yield the same/similar output. Degeneracy exists in various biological systems and contributes to the heterogeneity, complexity, and robustness of the systems. Modeling of degenerate biological systems is challenging and models enabling transfer learning in such systems have been little studied. In this article, we propose a predictive model that integrates transfer learning and degeneracy under a Bayesian framework. Theoretical properties of the proposed model are studied. Finally, we present an application of modeling the predictive relationship between transcription factors and gene exp...\n",
      "For piecewise planar scene modeling, many challenging issues still persist, in particular, how to generate sufficient candidate planes and how to assign an optimal plane for each spatial patch. To address these issues, we present a novel multi-view piecewise planar stereo method for the complete reconstruction. In our method, reconstruction is formulated as an energy-based plane labeling problem, where photo-consistency and geometric constraints are incorporated to a unified super pixel-level MRF (Markov Random Field) framework. To enhance the efficacy of the plane inference and optimization, an effective multi-direction plane sweeping with much restricted search space is carried out to generate sufficient and reliable candidate planes. Experiments show that our method can effectively handle many challenging factors (e.g. Slant surfaces, texture less regions) and achieve satisfactory results.\n",
      "Most cloud workflow scheduling algorithms assume that resources are charged under an ideal pay-as-you-go model, which may not be the case in real production cloud systems. Currently, most IaaS cloud providers charge users on billing cycle basis. If a resource is terminated before one billing cycle, the payment is still rounded upi¾?to one cycle. To address this problem, we firstly formalized it using bin-package method. Then, we propose a DAG schedule compaction algorithm of IC-$$\\star $$-SC, which compacts schedules generated by already exist algorithms to reduce resource requirement. Based on the compaction idea, we also propose a deadline constrained DAG scheduling algorithm of IC-SC. We compare our algorithms with state-of-the-art algorithms of IC-PCP and IC-PCPD2, and use 2 well-known scientific workflow applications for evaluation. Experimental results show that our algorithms reduce monetary cost drastically.\n",
      "Much of today's online social network (OSN) system relies on advertising for financial support. To improve the effectiveness of advertising, online advertisers tend to leverage influential users to deliver ads. Most of existing efforts on online advertising have focused on single-shot scenarios or assume static OSN models, while they overlook the fact that actions of advertising affect users' behaviors. In this paper, we investigate the behaviors of Sina Weibo users over three months, and make the observation that advertising affects the behaviors of the user's followers, which in turn has an impact on the effectiveness of future advertising. Based on this observation, we propose TiSA, a time-dependent advertising framework, which considers the future impact of advertising. Under this framework, the advertiser and the user make their decisions based on their instant utilities as well as future utilities. We also devise a learning algorithm with provable convergence to obtain the optimal policies. Evaluations using three month traces of 975 Sina Weibo users have been conducted, and the results validate the effectiveness of the proposed framework by showing that the utilities of all entities are significantly improved compared with traditional systems.\n",
      "As the HPC community moves into the exascale computing era, application energy is becoming as large of a concern as performance. Optimizing for energy will be essential in the effort to overcome the limited power envelope. Existing efforts to optimize energy in applications employ Dynamic Frequency and Voltage Scaling (DVFS) to maximize energy savings in less compute-intensive regions or non-critical execution paths. However, we found that DVFS has high power state switching overhead, preventing its use when a more fine-grained technique is necessary. In this work, we take advantage of the low transition overhead of CPU clock modulation and apply it to fine-grained Open MP parallel loops. The energy behavior of Open MP parallel regions is first characterized by changing the effective frequency using clock modulation. The clock modulation setting that achieves the best energy efficiency is then determined for each region. Finally, different CPU clock modulation settings are applied to the different loops within the same application. The resulting multi-frequency execution of Open MP applications achieves better energy-delay trade-off than any single frequency setting. In the best case scenario, the multi-frequency approach achieved 8.6% energy savings with less than 1.5% execution time increase. Concurrency throttling (i.e., Reducing the number of hardware threads used by an application) saves more energy and can be combined with CPU clock modulation. Using both, we see savings of 21% energy and improvement of energy-delay product (EDP) by 16%.\n",
      "Components in cold-standby state are usually assumed to be as good as new when they are activated. However, even in a standby environment, the components will suffer from performance degradation. This article presents a study of a redundancy allocation problem (RAP) for cold-standby systems with degrading components. The objective of the RAP is to determine an optimal design configuration of components to maximize system reliability subject to system resource constraints (e.g. cost, weight). As in most cases, it is not possible to obtain a closed-form expression for this problem, and hence, an approximated objective function is presented. A genetic algorithm with dual mutation is developed to solve such a constrained optimization problem. Finally, a numerical example is given to illustrate the proposed solution methodology.\n",
      "In this paper, a novel instrumental variable (IV) based identification method is proposed for closed-loop systems in the presence of coloured noise. The key technique lies in constructing an interleaved information matrix with respect to a multiple model structure formulated for the bi-directional paths. Then by utilizing UD factorization, all the parameter estimates for both forward and backward path models with orders possibly from zero to n , as well as the corresponding minimum loss function values, can be obtained simultaneously. Simulation results are provided to show the effectiveness of the proposed method.\n",
      "A string-similarity measure quantifies the similarity between two text strings for approximate string matching or comparison. For example, the strings “Sam” and “Samuel” can be considered to be similar. Most existing work that computes the similarity of two strings only considers syntactic similarities, for example, number of common words or  q -grams. While this is indeed an indicator of similarity, there are many important cases where syntactically-different strings can represent the same real-world object. For example, “Bill” is a short form of “William,” and “Database Management Systems” can be abbreviated as “DBMS.” Given a collection of predefined synonyms, the purpose of this article is to explore such existing knowledge to effectively evaluate the similarity between two strings and efficiently perform similarity searches and joins, thereby boosting the quality of approximate string matching.   In particular, we first present an expansion-based framework to measure string similarities efficiently while considering synonyms. We then study efficient algorithms for similarity searches and joins by proposing two novel indexes, called SI-trees and QP-trees, which combine signature-filtering and length-filtering strategies. In order to improve the efficiency of our algorithms, we develop an estimator to estimate the size of candidates to enable an online selection of signature filters. This estimator provides strong low-error, high-confidence guarantees while requiring only logarithmic space and time costs, thus making our method attractive both in theory and in practice. Finally, the experimental results from a comprehensive study of the algorithms with three real datasets verify the effectiveness and efficiency of our approaches.\n",
      "In natural language processing and information retrieval, the bag of words representation is used to implicitly represent the meaning of the text. Implicit semantics, however, are insufficient in supporting text or natural language based interfaces, which are adopted by an increasing number of applications. Indeed, in applications ranging from automatic ontology construction to question answering, explicit representation of semantics is starting to play a more prominent role. In this paper, we introduce the task of conceptual labeling (CL), which aims at generating a minimum set of conceptual labels that best summarize a bag of words. We draw the labels from a data driven semantic network that contains millions of highly connected concepts. The semantic network provides meaning to the concepts, and in turn, it provides meaning to the bag of words through the conceptual labels we generate. To achieve our goal, we use an information theoretic approach to trade-off the semantic coverage of a bag of words against the minimality of the output labels. Specifically, we use Minimum Description Length (MDL) as the criteria in selecting the best concepts. Our extensive experimental results demonstrate the effectiveness of our approach in representing the explicit semantics of a bag of words.\n",
      "M.T. would like to thank Prof. Pakming Hui for stimulating discussions. This work was partially supported by the National Natural Science Foundation of China (Grant No. 11105025) and China Postdoctoral Science Special Foundation (Grant No. 2012T50711). Y. Do was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (NRF-2013R1A1A2010067). Y.C.L. was supported by AFOSR under Grant No. FA9550-10-1-0083. G.W. Lee was supported by the Korea Meteorological Administration Research and Development Program under Grant CATER 2012-2072.\n",
      "In this paper, we try to design a collaborative caching scheme in an edge Content Centric Networking (CCN). Specifically, we first decompose the arbitrary network into clusters based on dominating set. Then, we place the content heterogeneously within each cluster based on the content popularity results and resort to a dynamic request routing. Simulation results show that the proposed outperforms the existing caching mechanisms in CCN.\n",
      "Today's Android-powered smartphones are equipped with various embedded sensors, such as the motion sensors, the environmental sensors and the position sensors. Many functions in the third-party applications (apps) need to use these sensors. However, embedded sensors may lead to security issues, as the third-party apps can access data from these sensors without claiming any permissions. It has been proven that embedded sensors can be exploited by well designed malicious apps, resulting in leaking users' privacy. In this work, we are motivated to provide an up-to-date overview of sensor usage patterns in current apps by investigating what, why and how embedded sensors are used in all the apps collected from a complete app market. To fulfill this goal, We develop a tool called “SDFDroid” to identify the sensors' types and to generate the sensor data propagation paths in each app. We then cluster the apps to find out their sensor usage patterns based on their sensor data propagation paths. We apply our method on AppChina, a widely used Chinese Android app market. Extensive experiments are conducted and the experimental results show that most apps implement their sensor related functions by using the third-party libraries. We further study the sensor usage in the third-party libraries. Our results show the accelerometer sensor is the most frequently used sensor. Though many third-party libraries use no more than four types of sensors, there are some third-party libraries register all the types of sensors recklessly. These results show the need for better regulating the sensor usage in Android apps.\n",
      "Self-localization is one of the key technologies in the wireless sensor networks (WSN). Some traditional self-localization algorithms can provide a reasonable positioning accuracy only in a uniform and dense network, while for a nonuniform network the performance is not acceptable. In this paper, we presented a novel grid-based linear least squares (LLS) self-localization algorithm. The proposed algorithm uses the grid method to screen the anchors based on the distribution characteristic of a nonuniform network. Furthermore, by taking into consideration the quasi-uniform distribution of anchors in the area, we select suitable anchors to assist the localization. Simulation results demonstrate that the proposed algorithm can greatly enhance the localization accuracy of the anonymous nodes and impose less computation burden compared to traditional Trilateration and Multilateration.\n",
      "Online 3D point cloud classification and scene understanding are crucial tasks for Unmanned Ground Vehicles (UGVs) equipped with multiple laser scanners. Due to the poor performance of traditional 2D image representation model for 3D point clouds, a novel Optimal Bearing Angle (OBA) model is therefore proposed to overcome the limitations of texture information losing and image blurring caused by the UGV's on-the-fly navigation. With the result of 3DSLIC based super-pixel segmentation in OBA images, the center of points belonging to each segmented OBA image patch is assigned as CRF graph node, so that a simplified CRF graph structure is constructed for online contextual classification of 3D laser points in urban environments. Moreover, total 29-dimensional features are extracted from both the raw 3D laser points and the corresponding OBA images. A large number of urban scenes selected from both DUT2 dataset and KAIST dataset are used as testing data in our experiments, and the results show the validity and performance of the proposed method.\n",
      "Public clouds become essential for many organizations to run their applications because they provide huge financial benefits and great flexibility. However, it is very challenging to accurately evaluate the performance and cost of applications without actual deployment on the clouds. Existing cloud simulators are generally designed from the perspective of cloud service providers, thus they can be under-developed for answering questions for the perspective of cloud users. To solve this prediction and evaluation problem, we created a Public Cloud IaaS Simulator (PICS). PICS enables the cloud user to evaluate the cost and performance of public IaaS clouds along with such dimensions like VM and storage service, resource scaling, job scheduling, and diverse workload patterns. We extensively validated PICS by comparing its results with the data acquired from real public IaaS cloud using real cloud-applications. We show that PICS provides highly accurate simulation results (less than 5% of average errors) under a variety of use cases. Moreover, we evaluated PICS' sensitivity with imprecise simulation parameters. The results show that PICS still provides very reliable simulation results with imprecise simulation parameters and performance uncertainty.\n",
      "Nitsche method application in non-conforming plate is presented in the context of isogeometric analysis. Reissner-Mindlin plate theory is employed to build governing equation and stiffness matrix. We use this theory to solve the elasticity problems of various classical plate models, and compare the obtained results to those from single-patch models and the exact solutions in Kirchhoff theory. The solutions of problem involving the use of complex model are as well obtained using the same Reissner-Mindlin theory and compared to the results from finite element method. All models are built with NURBS (non-uniform rational B-spline) patches with non-conforming mesh along the common boundaries. The algorithms of knot insertion and order elevation are applied to enrich the basis functions of NURBS patches. The results of numerical examples show the accuracy, robustness and high convergence rate of this method.\n",
      "Data harvesting using mobile data ferries has recently emerged as a promising alternative to the traditional multi-hop transmission paradigm. The use of data ferries can significantly reduce energy consumption at sensor nodes and increase network lifetime. However, it usually incurs longer data delivery latency as the data ferry needs to travel through the network to collect data, during which some delay-sensitive data may become obsolete. Therefore, optimizing the trajectory of the data ferry with data delivery latency bound is important for this approach to be effective in practice. To address this problem, we formally define the time-constrained data harvesting problem, which seeks an optimal data harvesting path in a network to collect as much data as possible within a time duration. We first characterise the performance bound given by the optimal data harvesting algorithm and show that the optimal algorithm significantly outperforms the random algorithm, especially when network scales. Motivated by the theoretical analysis and proving the NP-completeness of the time-constrained data harvesting problem, we then devise polynomial-time approximation schemes (PTAS) and mathematically prove the output being a constant-factor approximation of the optimal solution.\n",
      "This paper presents a systemic methodology for identifying and analysing the stakeholders of an organisation at many different levels. The methodology is based on soft systems methodology and is applicable to all types of organisation, both for profit and non-profit. The methodology begins with the top-level objectives of the organisation, developed through debate and discussion, and breaks these down into the key activities needed to achieve them. A range of stakeholders are identified for each key activity. At the end, the functions and relationships of all the stakeholder groups can clearly be seen. The methodology is illustrated with an actual case study in Hunan University.\n",
      "With the development of cloud computing, the problem of scheduling workflow in cloud system attracts a large amount of attention. In general, the cloud workflow scheduling problem requires to consider a variety of optimization objectives with some constraints. Traditional workflow scheduling methods focus on single optimization goal like makespan and single constraint like deadline or budget. In this paper, we first make a unified formalization of the optimality problem of multi-constraint and multi-objective cloud workflow scheduling using pareto optimality theory. We also present a two-constraint and two-objective case study, considering deadline, budget constraints and energy consumption, reliability objectives. A general list scheduling algorithm and a tuning mechanism are designed to solve this problem. Through extensive experimental, it confirms the efficiency of the unified multi-constraint and multi-objective cloud workflow scheduling system.\n",
      "The applicability and performance of motion detection methods dramatically degrade with the increasing noise. In this paper, we propose a robust dictionary-based background subtraction approach, which formulates background modeling as a linear and sparse combination of atoms in a pre-learned dictionary. Motion detection is then implemented to compare the difference between sparse representations of the current frame and the background model. The projection of noise over the dictionary being irregular and random guarantees the adaptability of our approach. Experimental results on synthetic and real noisy videos demonstrate the robustness of the proposed approach compared to other methods.\n",
      "Citation recommendation is an interesting and significant research area as it solves the information overload in academia by automatically suggesting relevant references for a research paper. Recently, with the rapid proliferation of information technology, research papers are rapidly published in various conferences and journals. This makes citation recommendation a highly important and challenging discipline. In this paper, we propose a novel citation recommendation method that uses only easily obtained citation relations as source data. The rationale underlying this method is that, if two citing papers are significantly co-occurring with the same citing paper(s), they should be similar to some extent. Based on the above rationale, an association mining technique is employed to obtain the paper representation of each citing paper from the citation context. Then, these paper representations are pairwise compared to compute similarities between the citing papers for collaborative filtering. We evaluate our proposed method through two relevant real-world data sets. Our experimental results demonstrate that the proposed method significantly outperforms the baseline method in terms of precision, recall, and F1, as well as mean average precision and mean reciprocal rank, which are metrics related to the rank information in the recommendation list.\n",
      "Inspired by geckos, we built a new configuration for climbing robot, the GPL model with one passive waist and four active axil legs. In this paper, further research was made to analysis its motion. We found the anomalies line between supporting feet and its effect on robot motion. A step further, the principia of configuration design and gait planning were proposed based on dynamic analysis. The prototype was developed and the climbing forces experiment was carried out. We testified the rationality of GPL model and waist trajectory planning. Also, energy analysis was proven by the driving forces measured on the supporting feet. In our experiment, the deceleration force during the robot climbing was only provided by the gravity, indicating a potential space for speed improvement without significant energy waste.\n",
      "We investigate the problem of efficiently supporting location-aware Publish/Subscribe (Pub/Sub for short), which is essential in many applications such as location-based recommendation and advertising, thanks to the proliferation of geo-equipped devices and the ensuing location-based social media applications. In a location-aware Pub/Sub system (e.g., an e-coupon system), subscribers can register their interest as spatial-keyword subscriptions (e.g., interest in nearby iphone discount); each incoming geo-textual message (e.g., geo-tagged e-coupon) will be delivered to all the relevant subscribers immediately. While there are several prior approaches aiming at providing efficient processing techniques for this problem, their approaches belong to spatial-prioritized indexing method which cannot well exploit the keyword distribution. In addition, their textual filtering techniques are built upon simple variants of traditional inverted indexes, which do not perform well for the textual constraint imposed by the problem. In this paper, we address the above limitations and provide a highly efficient solution based on a novel adaptive index, named AP-Tree. AP-Tree adaptively groups registered subscriptions using keyword and spatial partitions, guided by a cost model. AP-Tree also naturally indexes ordered keyword combinations. Furthermore, we show that our techniques can be extended to process moving spatial-keyword subscriptions, where subscribers can continuously update their locations. We present efficient algorithms to process both stationary and moving subscriptions, which can seamlessly and effectively integrate keyword and spatial partitions. Our extensive experiments demonstrate that AP-Tree and its variant AP $$^{+}$$+ -Tree can achieve up to an order of magnitude improvement on efficiency compared with prior state-of-the-art methods.\n",
      "The concept of sensing-as-a-service is proposed to enable a unified way of accessing and controlling sensing devices for many Internet of Things based applications. Existing techniques for Web service computing are not sufficient for this class of services that are exposed by resource-constrained devices. The vast number of distributed and redundantly deployed sensors necessitate specialised techniques for their discovery and ranking. Current research in this line mostly focuses on discovery, e.g., designing efficient searching methods by exploiting the geographical properties of sensing devices. The problem of ranking, which aims to prioritise semantically equivalent sensor services returned by the discovery process, has not been adequately studied. Existing methods mostly leverage the information directly associated with sensor services, such as detailed service descriptions or quality of service information. However, assuming the availability of such information for sensor services is often unrealistic. We propose a ranking strategy by estimating the cost of accessing sensor services. The computation is based on properties of the sensor nodes as well as the relevant contextual information extracted from the service access process. The evaluation results demonstrate not only the superior performance of the proposed method in terms of ranking quality measure, but also the potential for preserving the energy of the sensor nodes.\n",
      "The essential work of feature-specific opinion mining is centered on the product features. Previous related research work has often taken into account explicit features but ignored implicit features, However, implicit feature identification, which can help us better understand the reviews, is an essential aspect of feature-specific opinion mining. This paper is mainly centered on implicit feature identification in Chinese product reviews. We think that based on the explicit synonymous feature group and the sentences which contain explicit features, several Support Vector Machine (SVM) classifiers can be established to classify the non-explicit sentences. Nevertheless, instead of simply using traditional feature selection methods, we believe an explicit topic model in which each topic is pre-defined could perform better. In this paper, we first extend a popular topic modeling method, called Latent Dirichlet Allocation (LDA), to construct an explicit topic model. Then some types of prior knowledge, such as: must-links, cannot-links and relevance-based prior knowledge, are extracted and incorporated into the explicit topic model automatically. Experiments show that the explicit topic model, which incorporates pre-existing knowledge, outperforms traditional feature selection methods and other existing methods by a large margin and the identification task can be completed better.\n",
      "Who are the best targets to receive a call-for-paper or call-for-participation? What kind of topics should we propose for a workshop or a special issue of next year? Precisely predicting author's topic following behavior, i.e., publishing papers of a certain research topic in future, is essential to answer these questions. In this paper, we aim to model and predict author's topic following behavior in a heterogeneous information network. The heart of our methodology is to evaluate the author-author similarity through informative meta paths in the network. The models we propose in this paper can predict not only whether a given author will follow a certain topic but also the topic distribution over all publications in the next year. Extensive experimental evaluations justify that the prediction performance of our approach outperforms the existing approaches across various topics.\n",
      "Device-to-device (D2D) relaying in a store-carry-forward manner can efficiently expand the transmission range of D2D traffic offloading in cellular systems, which needs external incentives to promote the cooperation of relay nodes who are tend to be selfish. Different to the existing incentive mechanisms which usually adopt large enough incentives, we propose a moderate incentive-compatible data forwarding mechanism based on the Markov decision process (MDP) framework with the principal-agent model. The main idea of this mechanism is to dynamically adjust the payment to incentivize the relay nodes to forward the data with an appropriate radius such that the system utility is maximized. Due to the curse of dimensionality in solving MDP, we propose a greedy algorithm which considers the past information only and further prove its optimality. For discussing the implementation of the proposed solution, we propose an infrastructure-assisted D2D relaying protocol for cellular systems. Simulation results show that our proposed moderate incentive mechanism can achieve a better performance on system utility compared to existing incentive mechanisms.\n",
      "Over years, virtual backbone has attracted lots of attention as a promising approach to deal with the broadcasting storm problem in wireless networks. Frequently, the problem of a quality virtual backbone is formulated as a variation of the minimum connected dominating set problem. However, a virtual backbone computed in this way is not resilient against topology change since the induced graph by the connected dominating set is one-vertex-connected. As a result, the minimum   $k$  -connected   $m$  -dominating set problem is introduced to construct a fault-tolerant virtual backbone. Currently, the best known approximation algorithm for the problem in unit disk graph by Wang   assumes    $k \\leq 3$   and   $m \\geq 1$  , and its performance ratio is 280 when    $k = m = 3$  . In this paper, we use a classical result from graph theory, Tutte decomposition, to design a new approximation algorithm for the problem in unit disk graph for   $k \\leq 3$   and   $m \\geq 1$  . In particular, the algorithm features with (a) a drastically simple structure and (b) a much smaller performance ratio, which is nearly 62 when   $k = m = 3$  . We also conduct simulation to evaluate the performance of our algorithm.\n",
      "The staggering amount of streaming time series coming from the real world calls for more efficient and effective online modeling solution. For time series modeling, most existing works make some unrealistic assumptions such as the input data is of fixed length or well aligned, which requires extra effort on segmentation or normalization of the raw streaming data. Although some literature claim their approaches to be invariant to data length and misalignment, they are too time-consuming to model a streaming time series in an online manner. We propose a novel and more practical online modeling and classification scheme, DDE-MGM, which does not make any assumptions on the time series while maintaining high efficiency and state-of-the-art performance. The derivative delay embedding (DDE) is developed to incrementally transform time series to the embedding space, where the intrinsic characteristics of data is preserved as recursive patterns regardless of the stream length and misalignment. Then, a non-parametric Markov geographic model (MGM) is proposed to both model and classify the pattern in an online manner. Experimental results demonstrate the effectiveness and superior classification accuracy of the proposed DDE-MGM in an online setting as compared to the state-of-the-art.\n",
      "Task engagement is defined as loadings on energetic arousal (affect), task motivation, and concentration (cognition). It is usually challenging and expensive to label cognitive state data, and traditional computational models trained with limited label information for engagement assessment do not perform well because of overfitting. In this paper, we proposed two deep models (i.e., a deep classifier and a deep autoencoder) for engagement assessment with scarce label information. We recruited 15 pilots to conduct a 4-h flight simulation from Seattle to Chicago and recorded their electroencephalograph (EEG) signals during the simulation. Experts carefully examined the EEG signals and labeled 20 min of the EEG data for each pilot. The EEG signals were preprocessed and power spectral features were extracted. The deep models were pretrained by the unlabeled data and were fine-tuned by a different proportion of the labeled data (top 1%, 3%, 5%, 10%, 15%, and 20%) to learn new representations for engagement assessment. The models were then tested on the remaining labeled data. We compared performances of the new data representations with the original EEG features for engagement assessment. Experimental results show that the representations learned by the deep models yielded better accuracies for the six scenarios (77.09%, 80.45%, 83.32%, 85.74%, 85.78%, and 86.52%), based on different proportions of the labeled data for training, as compared with the corresponding accuracies (62.73%, 67.19%, 73.38%, 79.18%, 81.47%, and 84.92%) achieved by the original EEG features. Deep models are effective for engagement assessment especially when less label information was used for training.\n",
      "Incompatibility of image descriptor and ranking is always neglected in image retrieval. In this paper, manifold learning and Gestalt psychology theory are involved to solve the incompatibility problem. A new holistic descriptor called Perceptual Uniform Descriptor (PUD) based on Gestalt psychology is proposed, which combines color and gradient direction to imitate the human visual uniformity. PUD features in the same class images distributes on one manifold in most cases because PUD improves the visual uniformity of the traditional descriptors. Thus, we use manifold ranking and PUD to realize image retrieval. Experiments were carried out on five benchmark data sets, and the proposed method can greatly improve the accuracy of image retrieval. Our experimental results in the Ukbench and Corel-1K datasets demonstrated that N-S score reached to 3.58 (HSV 3.4) and mAP to 81.77% (ODBTC 77.9%) respectively by utilizing PUD which has only 280 dimension. The results are higher than other holistic image descriptors (even some local ones) and state-of-the-arts retrieval methods.\n",
      "Device-free gesture tracking is an enabling HCI mechanism for small wearable devices because fingers are too big to control the GUI elements on such small screens, and it is also an important HCI mechanism for medium-to-large size mobile devices because it allows users to provide input without blocking screen view. In this paper, we propose LLAP, a device-free gesture tracking scheme that can be deployed on existing mobile devices as software, without any hardware modification. We use speakers and microphones that already exist on most mobile devices to perform device-free tracking of a hand/finger. The key idea is to use acoustic phase to get fine-grained movement direction and movement distance measurements. LLAP first extracts the sound signal reflected by the moving hand/finger after removing the background sound signals that are relatively consistent over time. LLAP then measures the phase changes of the sound signals caused by hand/finger movements and then converts the phase changes into the distance of the movement. We implemented and evaluated LLAP using commercial-off-the-shelf mobile phones. For 1-D hand movement and 2-D drawing in the air, LLAP has a tracking accuracy of 3.5 mm and 4.6 mm, respectively. Using gesture traces tracked by LLAP, we can recognize the characters and short words drawn in the air with an accuracy of 92.3% and 91.2%, respectively.\n",
      "In this paper, a novel sparsity-aware direction of arrival (DOA) estimation scheme for a noncircular source is proposed in multiple-input multiple-output (MIMO) radar. In the proposed method, the reduced-dimensional transformation technique is adopted to eliminate the redundant elements. Then, exploiting the noncircularity of signals, a joint sparsity-aware scheme based on the reweighted l1 norm penalty is formulated for DOA estimation, in which the diagonal elements of the weight matrix are the coefficients of the noncircular MUSIC-like (NC MUSIC-like) spectrum. Compared to the existing l1 norm penalty-based methods, the proposed scheme provides higher angular resolution and better DOA estimation performance. Results from numerical experiments are used to show the effectiveness of our proposed method.\n",
      "With the rising of Cloud computing, evolution have occurred not only in datacenter, but also in software development, deployment, maintain and usage. How to build cloud platform for traditional software, and how to deliver cloud service to users are central research fields which will have a huge impact. In recent years, the development of microservice and container technology make software paradigm evolve towards Cloudware in cloud environment. Cloudware, which is based on service and supported by cloud platform, is an important method to cloudalize traditional software. It is also a significant way for software development, deployment, maintenance and usage in future cloud environment. Furthermore, it creates a completely new thought for software in cloud platform. In this paper, we proposed a new Cloudware PaaS platform based on microservice architecture and light weighted container technology. We can directly deploy traditional software which provides services to users by browser in this platform without any modification. By utilizing the microservice architecture, this platform has the characteristics of scalability, auto-deployment, disaster recovery and elastic configuration.\n",
      "Developing group recommender systems (GRSs) is a vital requirement in many online service systems to provide recommendations in contexts in which a group of users are involved. Unfortunately, GRSs cannot be effectively supported using traditional individual recommendation techniques because it needs new models to reach an agreement to satisfy all the members of this group, given their conflicting preferences. Our goal is to generate recommendations by taking each group member's contribution into account through weighting members according to their degrees of importance. To achieve this goal, we first propose a member contribution score (MCS) model, which employs the separable non-negative matrix factorization technique on a group rating matrix, to analyze the degree of importance of each member. A Manhattan distance-based local average rating (MLA) model is then developed to refine predictions by addressing the fat tail problem. By integrating the MCS and MLA models, a member contribution-based group recommendation (MC-GR) approach is developed. Experiments show that our MC-GR approach achieves a significant improvement in the performance of group recommendations. Lastly, using the MC-GR approach, we develop a group recommender system called GroTo that can effectively recommend activities to web-based tourist groups. A new contribution-based two-phase group recommender model is proposed.A new model relies on separable non-negative matrix factorization over sub-matrix and item relevance calculation.A web-based group tourism recommender system prototype is represented.\n",
      "This paper is on designing a compact data structure for multi-set membership testing allowing fast set querying. Multi-set membership testing is a fundamental operation for computing systems and networking applications. Most existing schemes for multi-set membership testing are built upon Bloom filter, and fall short in either storage space cost or query speed. To address this issue, in this paper we propose Noisy Bloom Filter (NBF) and Error Corrected Noisy Bloom Filter (NBF-E) for multi-set membership testing. For theoretical analysis, we optimize their classification failure rate and false positive rate, and present criteria for selection between NBF and NBF-E. The key novelty of NBF and NBF-E is to store set ID information in a compact but noisy way that allows fast recording and querying, and use denoising method for querying. Especially, NBF-E incorporates asymmetric error-correcting coding technique into NBF to enhance the resilience of query results to noise by revealing and leveraging the asymmetric error nature of query results. To evaluate NBF and NBF-E in comparison with prior art, we conducted experiments using real-world network traces. The results show that NBF and NBF-E significantly advance the state-of-the-art on multi-set membership testing.\n",
      "In wireless sensor networks, data harvesting using mobile data ferries has recently emerged as a promising alternative to the traditional multi-hop communication paradigm. The use of data ferries can significantly reduce energy consumption at sensor nodes and increase network lifetime. However, it usually incurs long data delivery latency as the data ferry needs to travel through the network to collect data, during which some delay-sensitive data may become obsolete. Therefore, it is important to optimize the trajectory of the data ferry with data delivery latency bound for this approach to be effective in practice. To address this problem, we formally define the time-constrained data harvesting problem, which seeks an optimal data harvesting path in a network to collect as much data as possible within a time duration. We then investigate the formulated data harvesting problem in the generic   $m$  -dimensional context, of which the cases of   $m=1$  , 2, 3 are particularly pertinent. We first characterize the performance bound given by the optimal data harvesting algorithm and show that the optimal algorithm significantly outperforms the random algorithm, especially when network scales. However, we mathematically prove that finding the optimal data harvesting path is NP-hard. We therefore devise an approximation algorithm and mathematically prove the output being a constant-factor approximation of the optimal solution. Our experimental results also demonstrate that our approximation algorithm significantly outperforms the random algorithm in a wide range of network settings.\n",
      "A backprojection algorithm (BPA), a time domain synthetic aperture radar (SAR) imaging algorithm, can be widely used without modification in various imaging modes, like stripmap, spotlight, sliding spotlight, etc. However, BPA also suffers when used in azimuth multichannel SAR due to the nonuniform sampling problem caused by a nonideal pulse repetition frequency. In this letter, we propose a weighted BPA (WBPA) for azimuth multichannel SAR imaging. Derived from the filter-bank-based reconstruction method, WBPA adds a weighting procedure to BPA. Simulations and an airborne SAR data experiment demonstrate that WBPA can perfectly suppress the azimuth ambiguities under band-limited circumstances. WBPA extends the applicable scope of BPA.\n",
      "Artificial lateral line has been drawing an increasing attention recently for its potential applications in robotics. Experiments are usually conducted with a bioinspired robot in a controlled environment, where the sensing platform is held stationary or slowly driven with a simple linear motion. In this paper, we conduct a more practical and challenging study where the robot uses artificial lateral line to evaluate its linear velocity while freely swimming. We use onboard artificial lateral line to measure the pressure profiles over the surface of a robotic fish and employ onboard IMU (inertial measurement unit) to record the motion kinematics of the robot while freely swimming at various speeds. We find that 1) pressure changes are greatest on the head of the robot; 2) pressures increase along with the swimming speed and the oscillation amplitude of angular velocity of the robot. Therefore, we propose a nonlinear prediction model which incorporates distributed pressure and angular velocity to estimate the speed of the robot. Online speed evaluation experiment demonstrates the effectiveness and the accuracy of the proposed model.\n",
      "Device-to-device (D2D) communication is a recently emerged disruptive technology for enhancing the performance of current cellular systems. To successfully implement D2D communications underlaying cellular networks, resource allocation to D2D links is a critical issue, which is far from trivial due to the mutual interference between D2D users and cellular users. Most of the existing resource allocation research for D2D communications has primarily focused on the intracell scenario while leaving the intercell settings not considered. In this paper, we investigate the resource allocation issue for intercell scenarios where a D2D link is located in the overlapping area of two neighboring cells. Furthermore, we present three intercell D2D scenarios regarding the resource allocation problem. To address the problem, we develop a repeated game model under these scenarios. Distinct from existing works, we characterize the communication infrastructure, namely, base stations, as players competing resource allocation quota from D2D demand, and we define the utility of each player as the payoff from both cellular and D2D communications using radio resources. We also propose a resource allocation algorithm and protocol based on the Nash equilibrium derivations. Numerical results indicate that the developed model not only significantly enhances the system performance, including sum rate and sum rate gain, but also shed lights on resource configurations for intercell D2D scenarios.\n",
      "The Web of Things aims to make physical world objects and their data accessible through standard Web technologies to enable intelligent applications and sophisticated data analytics. Due to the amount and heterogeneity of the data, it is challenging to perform data analysis directly; especially when the data is captured from a large number of distributed sources. However, the size and scope of the data can be reduced and narrowed down with search techniques, so that only the most relevant and useful data items are selected according to the application requirements. Search is fundamental to the Web of Things while challenging by nature in this context, e.g., mobility of the objects, opportunistic presence and sensing, continuous data streams with changing spatial and temporal properties, efficient indexing for historical and real time data. The research community has developed numerous techniques and methods to tackle these problems as reported by a large body of literature in the last few years. A comprehensive investigation of the current and past studies is necessary to gain a clear view of the research landscape and to identify promising future directions. This survey reviews the state-of-the-art search methods for the Web of Things, which are classified according to three different viewpoints: basic principles, data/knowledge representation, and contents being searched. Experiences and lessons learned from the existing work and some EU research projects related to Web of Things are discussed, and an outlook to the future research is presented.\n",
      "This paper proposes a novel method named ScholarRank to evaluate the scientific impact of rising stars. Our proposed ScholarRank integrates the merits of both statistical indicators and influence calculation algorithms in heterogeneous academic networks. The ScholarRank method considers three factors, which are the citation counts of authors, the mutual influence among coauthors and the mutual reinforce process among different entities in heterogeneous academic networks. Through experiments on real datasets, we demonstrate that our ScholarRank can efficiently select more top ranking rising stars than other methods.\n",
      "This note is intended to investigate noise-to-state stability for random nonlinear systems with state-dependent switching. Under some mild and easily verified conditions, the existence of global solution to random switched systems can be proved. Based on a reasonable requirement for the random disturbance, the criteria on noise-to-state stability of random switched systems are presented by the aid of single Lyapunov function technique. The reasonability of the obtained results is illustrated by using a mechanical model in random vibration environment.\n",
      "Silicon microneedle arrays (MNAs) have been widely studied due to their potential in various transdermal applications. However, discrete MNAs, as a preferred choice to fabricate flexible penetrating devices that could adapt curved and elastic tissue, are rarely reported. Furthermore, the reported discrete MNAs have disadvantages lying in uniformity and height-pitch ratio. Therefore, an improved technique is developed to manufacture discrete MNA with tunable height-pitch ratio, which involves KOH-dicing-KOH process. The detailed process is sketched and simulated to illustrate the formation of microneedles. Furthermore, the undercutting of convex mask in two KOH etching steps are mathematically analyzed, in order to reveal the relationship between etching depth and mask dimension. Subsequently, fabrication results demonstrate KOH-dicing-KOH process. {321} facet is figured out as the surface of octagonal pyramid microneedle. MNAs with diverse height and pitch are also presented to identify the versatility of this approach. At last, the metallization is realized via successive electroplating.\n",
      "In this note, distributed adaptive controllers are developed for output consensus tracking of multiple linear systems with unknown parameters, uncertain subsystem interconnections and external disturbances. The subsystems are allowed to have non-identical dynamics and the same yet arbitrary order. It is assumed that only part of subsystems can have direct access to the time-varying trajectory information and the subsystem states are unmeasurable for local feedback control. In our design, the directed graph representing the information transmission status among subsystems is preprocessed by splitting it into a hierarchical structure. Then local adaptive controllers of subsystems in different layers can be computed in a sequential order and the difficulty on deriving mutually dependent local controls in previous consensus works are successfully overcome. In each subsystem, additional estimates are introduced to account for the unknown parameters and states in its neighbors' dynamics. Besides, only the information of local outputs and inputs need be collected from the neighboring subsystems. Certain robust terms are added in distributed adaptive laws to mitigate the effects of uncertain subsystem interactions and disturbances. It is proved that with our scheme, all closed-loop signals can be ensured bounded when the strengths of uncertain subsystem interconnections are sufficiently weak. The tracking errors for the entire group of subsystems will converge to a compact set and the transient tracking error performance can be adjusted by appropriately choosing design parameters.\n",
      "The purpose of the study was to explore the application of artificial neural network model in the auxiliary diagnosis of lung cancer and compare the effects of back-propagation (BP) neural network with Fisher discrimination model for lung cancer screening by the combined detections of four biomarkers of p16, RASSF1A and FHIT gene promoter methylation levels and the relative telomere length. Real-time quantitative methylation-specific PCR was used to detect the levels of three-gene promoter methylation, and real-time PCR method was applied to determine the relative telomere length. BP neural network and Fisher discrimination analysis were used to establish the discrimination diagnosis model. The levels of three-gene promoter methylation in patients with lung cancer were significantly higher than those of the normal controls. The values of Z(P) in two groups were 2.641 (0.008), 2.075 (0.038) and 3.044 (0.002), respectively. The relative telomere lengths of patients with lung cancer (0.93 ± 0.32) were significantly lower than those of the normal controls (1.16 ± 0.57), t = 4.072, P < 0.001. The areas under the ROC curve (AUC) and 95 % CI of prediction set from Fisher discrimination analysis and BP neural network were 0.670 (0.569–0.761) and 0.760 (0.664–0.840). The AUC of BP neural network was higher than that of Fisher discrimination analysis, and Z(P) was 0.76. Four biomarkers are associated with lung cancer. BP neural network model for the prediction of lung cancer is better than Fisher discrimination analysis, and it can provide an excellent and intelligent diagnosis tool for lung cancer.\n",
      "Multimedia broadcasting applications become increasingly popular in future user-centric 5G networks. However, traditional multimedia data broadcasting scheme from the base station to devices is hard to satisfy the highly diversified Quality of Experience (QoE) requirement from users' perspectives. In this paper we propose a user-centric multimedia rebroadcasting scheme leveraging the device-to- device (D2D) communication model with strict energy constraint. Our method first optimizes the transmission energy of each rebroadcasting user device at physical layer, by taking modulation rate scaling under power limitation. Then we propose a cross-layer optimal method to improve the multimedia QoE for user applications by jointly considering the physical layer parameters and different channel information factors. The simulation results show the proposed rebroadcasting resource allocation approach achieves significant multimedia quality gain by allocating optimal transmission power and modulation rate under strict energy limitation. Performance gain of QoE is further enhanced when the differences of channel information between transmitting and receiving devices are available and leveraged.\n",
      "User identification is an essential problem for security protection and data privacy preservation of wearable devices. With proper user identification, wearable devices can adopt personalized settings for different users, automatically label the corresponding data to protect user privacy, and help prevent illegal user spoofing attacks. Current user identification solutions proposed for wearable devices either rely on dedicated devices with high cost or require user intervention which is not convenient. In this work, we leverage the bio-vibrometry to enable a novel user identification solution for wearable devices in small-scale scenarios,  e.g. , household scenario. Unlike existing user identification solutions, our system only uses the low-cost sensors that are already available for most wearable devices. The key idea is that, when human body is exposed to a vibration excitation, the vibration response reflects the physical characteristics of user,  i.e. , the  mass ,  stiffness  and  damping . Meanwhile, due to users' biological diversity, such physical characteristics of different users are quite distinctive. Therefore, we can leverage the discrepancy in users' vibration responses as an identifier. Based on this idea, we propose VibID, which only uses a low-cost vibration motor and accelerometer to generate an unobtrusive vibration to users' arms and capture the corresponding responses. By examining the vibration patterns at different frequencies, VibID builds an ensemble machine learning model to recognize who is using the device. Extensive experiments are conducted on human subjects to demonstrate that our system is reliable in small-scale scenarios and robust to various confounding factors,  e.g. , arm position, muscle state, user mobility and wearing location. We also show that, in an uncontrolled scenario of 8 users, our system can still ensure a identification accuracy above 91%.\n",
      "Computer aided diagnosis systems are recently introduced to increase the accuracy of mammography interpretation. This paper introduces a new classification algorithm based on Fuzzy Gaussian Mixture Model (FGMM) by combining the power of Gaussian Mixture Model (GMM) and Fuzzy Logic System (FLS) for computer aided diagnosis system, to classify the detected regions in mammogram images into malignant or benign categories. The experimental results are obtained from a data set of 300 images taken from the Digital Database for Screening Mammography (DDSM, University of South Florida) for different classes. Confusion matrix analysis is used to measure the performance of the proposed FGMM system. The results show that the proposed FGMM classifier has achieved an overall Matthews Correlation Coefficient (MCC) classification quality of 86.16 %, with 93 % accuracy, 90 % sensitivity and 96 % specificity, and outperformed other classifiers in all aspects. The experimental results obtained from the developed classifier prove that the proposed technique will improve the diagnostic accuracy and reliability of radiologists’ image interpretation in the diagnosis of breast cancer. The resulting breast cancer Computer Aided Diagnosis (CAD) detection system is a promising tool to provide preliminary decision support information to physicians for further diagnosis.\n",
      "Recent years have witnessed the prevalence of wearable devices. Wearable devices are intelligent and multifunctional, but they rely heavily on batteries. This greatly limits their application scope, where replacement of battery or recharging is challenging or inconvenient. We note that wearable devices have the opportunity to harvest energy from human motion, as they are worn by the people as long as being functioning. In this study, we propose a battery-free sensing platform for wearable devices in the form-factor of shoes. It harvests the kinetic energy from walking or running to supply devices with power for sensing, processing and wireless communication, covering all the functionalities of commercial wearable devices. We achieve this goal by enabling the whole system running on the harvested energy from two feet. Each foot performs separate tasks and two feet are coordinated by ambient backscatter communication. We instantiate this idea by building a prototype, containing energy harvesting insoles, power management circuits and ambient backscatter module. Evaluation results demonstrate that the system can wake up shortly after several seconds' walk and have sufficient Bluetooth throughput for supporting many applications. We believe that our framework can stir a lot of useful applications that were infeasible previously.\n",
      "With the increasing demand for higher performance wireless local area networks (WLANs), channel bonding was first proposed in the IEEE 802.11n protocol to offer a higher data rate by combining two 20 MHz channels into one 40 MHz channel. Although much has been understood about channel bonding management, hardly any of these innovations have made it into today's IEEE 802.11 WLANs in a distributed manner. This paper presents the first step to fill the gap, by proposing a channel bonding management solution that can be readily implemented in today's commercial 802.11 devices. We conduct a measurement with off-the-shelf 802.11 devices in a real WLAN to characterize channel bonding, and then propose a channel bonding scheme based on adaptive channel clear assessment (CCA). We conduction evaluation under the typical IEEE 802.11 enterprise scenario, and the results show that our scheme improves the throughput by 37% and 46% compared to the traditional channel bonding scheme and the default CSMA/CA, respectively.\n",
      "Fault reconstruction has been widely applied for fault diagnosis recently, which, however, treats the fault data as a single subject without analyzing the specific effects of different variables. To address this problem, a faulty variable selection method is proposed which can extract more meaningful discriminant directions and probe into the specific faulty variables. By pairwise performing nested-loop Fisher discriminant analysis (NeLFDA) algorithm on normal and fault process data, some projection directions that are useful for classification between normal and fault cases are extracted. Then along these directions, an iterative faulty variable selection procedure is designed by evaluating ratio of variable contribution to the fault variations so that process variables that are significantly fault-relevant are identified and distinguished from those general variables. Based on variable selection results, fault reconstruction model is developed for faulty variables, which is used for fault diagnosis. Online fault diagnosis is then performed by dually checking the characteristics of fault samples for faulty variables and general variables. Its performance is illustrated with the industrial process data from the cut-made process of cigarette.\n",
      "In this paper, we study a regularized inverse filtering method for blind image deconvolution. The main idea is to make use of nonnegativity and support constraints, and to incorporate regularization terms to establish a convex programming model which aims to determine an inverse filter for image deconvolution. Because of the convexity of the proposed energy functional, the existence of the solution can be guaranteed. We employ the alternating direction method of multipliers to solve the resulting optimization problem. In this paper, we consider three possible regularization methods in the inverse filtering, namely total variation, nonlocal total variation, and framelet approaches. Experimental results of these regularization methods are reported to show that the performance of the proposed methods is better than the other testing methods for several testing images.\n",
      "Display Omitted We have developed a robust temporal information extraction algorithm for analyzing low quality texts.Our algorithm has minimal dependency on grammatical and syntactic information.A time expression tagger was built to parse a variety of time expression patterns.The proposed algorithm outperforms other approaches on medical product safety report narratives. The sheer volume of textual information that needs to be reviewed and analyzed in many clinical settings requires the automated retrieval of key clinical and temporal information. The existing natural language processing systems are often challenged by the low quality of clinical texts and do not demonstrate the required performance. In this study, we focus on medical product safety report narratives and investigate the association of the clinical events with appropriate time information. We developed a novel algorithm for tagging and extracting temporal information from the narratives, and associating it with related events. The proposed algorithm minimizes the performance dependency on text quality by relying only on shallow syntactic information and primitive properties of the extracted event and time entities. We demonstrated the effectiveness of the proposed algorithm by evaluating its tagging and time assignment capabilities on 140 randomly selected reports from the US Vaccine Adverse Event Reporting System (VAERS) and the FDA (Food and Drug Administration) Adverse Event Reporting System (FAERS). We compared the performance of our tagger with the SUTime and HeidelTime taggers, and our algorithm's event-time associations with the Temporal Awareness and Reasoning Systems for Question Interpretation (TARSQI). We further evaluated the ability of our algorithm to correctly identify the time information for the events in the 2012 Informatics for Integrating Biology and the Bedside (i2b2) Challenge corpus. For the time tagging task, our algorithm performed better than the SUTime and the HeidelTime taggers (F-measure in VAERS and FAERS: Our algorithm: 0.86 and 0.88, SUTime: 0.77 and 0.74, and HeidelTime 0.75 and 0.42, respectively). In the event-time association task, our algorithm assigned an inappropriate timestamp for 25% of the events, while the TARSQI toolkit demonstrated a considerably lower performance, assigning inappropriate timestamps in 61.5% of the same events. Our algorithm also supported the correct calculation of 69% of the event relations to the section time in the i2b2 testing set.\n",
      "In multiple-input–multiple-output broadcast channels, lattice reduction (LR) preprocessing technique can significantly improve the precoding performance. Among the existing LR algorithms, the fixed complexity Lenstra–Lenstra–Lovasz (fcLLL) algorithm applying limited number of LLL loops is suitable for the real-time communication system. However, fcLLL algorithm suffers from higher average complexity. Aiming at this problem, a computationally efficient fcLLL (CE-fcLLL) algorithm for LR-aided (LRA) precoding is developed in this study. First, the authors analyse the impact of fcLLL algorithm on the signal-to-noise ratio performance of LRA precoding by a power factor (PF) which is defined to measure the relation of reduced basis and transmit power of LRA precoding. Then, they propose a CE-fcLLL algorithm by designing a new LLL loop and introducing new early termination conditions to reduce redundant and inefficient LR operation in fcLLL algorithm. Finally, they define a PF loss factor to optimise the PF threshold and the number of LLL loops, which can lead to a performance-complexity tradeoff. Simulation results show that the proposed algorithm for LRA precoding can achieve better bit-error-rate performance than the fcLLL algorithm with remarkable complexity savings in the same upper complexity bound.\n",
      "In this paper, we study a wireless-powered cooperative relaying system which consists of a source node (SN), a relay node (RN) and a destination node (DN). The RN has no embedded power supply, thus it needs to harvest energy from the radio signal transmitted by a power beacon (PB) which is responsible for charging the RN before forwarding the information to the DN. In addition, we assume that the RN possesses a buffer and can temporarily store the information received from the SN. Based on this assumption, we propose an adaptive transmission scheme in which the system adaptively switches between two transmission modes, namely the SN information transmission (SN-IT) mode and the RN harvest-and-transmit (RN-HAT) mode. The optimal mode adaptation method that maximizes the throughput of the system is obtained for different system setups and the throughput of the system is obtained in closed-form expressions. Numerical results are provided to verify the analytical expressions. It is shown that the throughput of the wireless-powered cooperative relaying system can be improved by using the proposed adaptive transmission scheme.\n",
      "Discovery of periodic patterns in time series data has become an active research area with many applications. These patterns can be hierarchical in nature, where a higher level pattern may consist of repetitions of lower level patterns. Unfortunately, the presence of noise may prevent these higher level patterns from being recognized in the sense that two portions (of a data sequence) that support the same (high level) pattern may have different layouts of occurrences of basic symbols. There may not exist any common representation in terms of raw symbol combinations; and hence such (high level) patterns may not be expressed by any previous model (defined on raw symbols or symbol combinations) and would not be properly recognized by any existing method. In this paper, we propose a novel model, namely meta-pattern, to capture these high level patterns. As a more flexible model, the number of potential meta-patterns could be very large. A substantial difficulty is how to identify the proper pattern candidates. However the well-known a priori properly is not able to provide sufficient pruning power. A new property, namely component location, is identified and used to conduct candidate generation so that an efficient computation-based mining algorithm can be developed. We apply our algorithm to real and synthetic sequences and interesting patterns are discovered.\n",
      "In this paper, we investigate energy cooperation among sustainable cooperative relay nodes, which adopt decode- and-forward (DF) relaying. The relay nodes work on either information decoding (ID) mode or energy harvesting (EH) mode when receiving the signal from the source. Due to different harvested energy at the relay nodes, effective energy cooperation can benefit the energy efficiency, but suffer from practical energy loss when achieving energy cooperation. By solving the problem using Lagrangian duality, we obtain the optimal energy cooperation policy for the relay nodes and further discuss its two-level waterfilling structure. Then we derive the optimal and low-complexity EH/ID mode selecting algorithm with the two-level waterfilling energy cooperation policy. Finally, we compare the throughput between the systems with and without energy cooperation to show the benefit of energy cooperation.\n",
      "In this paper, we propose a novel channel impulse response (CIR) clustering algorithm using a sparsity-based method, which exploits the feature of CIR that power of multipath component (MPC) is exponentially decreasing with increasing delay. We first use a sparsity-based optimization to recover CIRs, which can be well solved by using reweighted L1 minimization. Then a heuristic approach is provided to identify clusters in the recovered CIRs, which leads to improved clustering accuracy in comparison to identifying clusters directly in the raw CIRs. The proposed algorithm incorporates the physical behaviors of MPCs into the clustering framework and enables applications with no prior knowledge of the clusters, such as number and initial locations of clusters. The results in this paper can be used to parameterize the CIR model of radio channels.\n",
      "Mobility can potentially increase communication opportunities due to the channel variation it causes. This paper studies a fundamental scheduling problem in a three-node mobile relay system in which data packets are sent from a source to a destination via a mobile relay. Our aim is to fully exploit the communication opportunities brought by relay mobility to minimize the energy consumption under a data throughput constraint. In particular, first, we apply a 2-D finite-state Markov chain (2D-FSMC) channel model to reflect both the large- and small-scale channel fading in the mobile environment. Then, the scheduling problem is formulated as a constrained Markov decision process (C-MDP) and is approximately solved through the Lagrange relaxation approach. Based on this, we proposed a probabilistic algorithm, called the opportunistic packet scheduling (OPS) algorithm, to further reduce energy consumption. The proposed OPS algorithm is proved to be optimal with polynomial complexity. Simulation results show that the proposed algorithm generally outperforms the conventional algorithms in terms of energy consumption. Furthermore, some observations are analyzed for the effect of mobility on the energy-efficient transmission.\n",
      "Mobile-relay-assisted forwarding in delay-tolerant networks (DTNs) can improve network capacity and packet delivery ratio but, at the same time, may significantly increase energy consumption at the system level. In this paper, we propose a distance-based energy-efficient opportunistic forwarding (DEEOF) framework for the broadcast transmission in mobile DTNs. DEEOF strikes a balance between energy consumption and network performance by maximizing the energy efficiency while maintaining a high packet delivery ratio. In the proposed algorithms, we define the metric of the forwarding equivalent energy efficiency distance (FEED) for broadcast transmission to quantify the transmission distances achieving the same energy efficiency at different time instances, or with different numbers of the relays in the source's transmission radius. Based on the concept of FEED, we propose two DEEOF algorithms for opportunistic broadcast forwarding, which makes the forwarding decision by comparing the current energy efficiency with the estimated future expectation and distribution, respectively. The performance improvement of the proposed DEEOF algorithms is also demonstrated by simulation, particularly for the cases in which the source has very limited battery reserves.\n",
      "Modeling the aging process of human face is important for cross-age face verification and recognition. In this paper, we introduce a recurrent face aging (RFA) framework based on a recurrent neural network which can identify the ages of people from 0 to 80. Due to the lack of labeled face data of the same person captured in a long range of ages, traditional face aging models usually split the ages into discrete groups and learn a one-step face feature transformation for each pair of adjacent age groups. However, those methods neglect the in-between evolving states between the adjacent age groups and the synthesized faces often suffer from severe ghosting artifacts. Since human face aging is a smooth progression, it is more appropriate to age the face by going through smooth transition states. In this way, the ghosting artifacts can be effectively eliminated and the intermediate aged faces between two discrete age groups can also be obtained. Towards this target, we employ a twolayer gated recurrent unit as the basic recurrent module whose bottom layer encodes a young face to a latent representation and the top layer decodes the representation to a corresponding older face. The experimental results demonstrate our proposed RFA provides better aging faces over other state-of-the-art age progression methods.\n",
      "NAND flash memory–based solid state disks (SSDs) have been widely used in enterprise servers. However, flash memory has limited write endurance, as a block becomes unreliable after a finite number of program/erase cycles. Existing wear-leveling techniques are essentially intradisk data distribution schemes, as they can only even wear out across the flash medium within a single SSD. When multiple SSDs are organized in an array manner in server applications, an interdisk wear-leveling technique, which can ensure a uniform wear-out distribution across SSDs, is much needed. In this article, we propose a novel SSD-array level wear-leveling strategy called SWANS (  S  moothing   W  ear   A  cross   N     S  SDs) for an SSD array structured in a RAID-0 format, which is frequently used in server applications. SWANS dynamically monitors and balances write distributions across SSDs in an intelligent way. Further, to evaluate its effectiveness, we build an SSD array simulator on top of a validated single SSD simulator. Next, SWANS is implemented in its array controller. Comprehensive experiments with real-world traces show that SWANS decreases the standard deviation of writes across SSDs on average by 16.7x. The gap in the total bytes written between the most written SSD and the least written SSD in an 8-SSD array shrinks at least 1.3x.\n",
      "Encryption of a digital image is very important especially in applications of body area networks (BANs) since the image may include a number of privacy. Past encryption methods have disadvantages of the small key space and low ability of resistance to attack. In this paper, we propose a new encryption algorithm based on discrete wavelet transform (DWT) and multichaos which has characteristics of the deterministic, pseudorandomness, and sensitivity of initial values. The image is first decomposed and spatial reconstructed by two-dimensional DWT and then is performed by multichaos matrices for space encryption. The experimental results indicate that the proposed algorithm has a large key space, high key sensitivity, and excellent ability of resistance to attack.\n",
      "In this paper, we propose an energy-efficient and low-delay scheduling EELDS scheme for interruptible preamble sampling-based medium access control MAC protocols e.g., BoX-MAC-2 and X-MAC in IEEE 802.15.4 wireless sensor network WSN, which considers energy efficiency while reducing delay and contention in the context of real-time data gathering and remote monitoring applications. It is built on MAC layer and exploits local information to coordinate the MAC schedules of nodes in a slowly-changing topology, just with light overhead. The scheme also considers the adaptation to data aggregation with real-time data flows. Therefore, our scheme allows for much longer network lifetime while satisfying the delay constraints. The implementation and simulation results show the performance of the proposed scheme.\n",
      "Cross-domain recommender systems are usually able to suggest items, which are not in the same domain, where users provided ratings. For this reason, cross-domain recommendation has attracted more and more attention in recent years. However, most studies propose to make cross-domain recommendation in the scenario, where there are common ratings between different domains. The scenario without common ratings is seldom considered. In this paper, we propose a novel method to solve the cross-domain recommendation problem in such a scenario. We first apply trust relations to the cross-domain scenario for predicting coarse ratings pertaining to cross-domain items. Then, we build a new rating matrix, including known ratings and predicted ratings of items from different domains, and transform a user-item matrix into an item–item association matrix. Finally, we compute the similarities of items belonging to different domains and use item-based collaborative filtering to generate recommendations. Through relevant experiments on a real-world data set, we compare our method to a trust-aware recommendation method and demonstrate its effectiveness in terms of prediction accuracy, recall, and coverage.\n",
      "Permutation codes are vector quantizers whose codewords are related by permutations and, in one variant, sign changes. Asymptotically, as the vector dimension grows, optimal Variant I permutation code design is identical to optimal entropy-constrained scalar quantizer (ECSQ) design. However, contradicting intuition and previously published assertions, there are finite block length permutation codes that perform better than the best ones with asymptotically large length; thus, there are Variant I permutation codes whose performances cannot be matched by any ECSQ. Along similar lines, a new asymptotic relation between Variant I and Variant II permutation codes is established but again demonstrated to not necessarily predict the performances of short codes. Simple expressions for permutation code performance are found for memoryless uniform and Laplacian sources. The uniform source yields the aforementioned counterexamples.\n",
      "In the hierarchical control paradigm of a smart grid cyber-physical system, decentralized local agents (LAs) can potentially be compromised by opportunistic attackers to manipulate electricity prices for illicit financial gains. In this paper, to address such opportunistic attacks, we propose a Dirichlet-based detection scheme, where a Dirichlet-based probabilistic model is built to assess the reputation levels of LAs. Initial reputation levels of the LAs are first trained using the proposed model, based on their historical operating observations. An adaptive detection algorithm with reputation incentive mechanism is then employed to detect opportunistic attackers. We demonstrate the utility of our proposed scheme using data collected from the IEEE 39-bus power system with the PowerWorld simulator.\n",
      "In a wireless powered communication network, where user equipments (UEs) harvest radio frequency energy from an access point (AP) and send data to the AP, there exists the near-far problem with respect to energy harvesting efficiency due to UEs’ random locations. In this paper, we introduce the concept of delay-aware energy balancing by minimizing the average transmission delay while taking into account the issue of unbalanced harvested energy distribution. In particular, we propose an adaptive harvest-then-cooperate protocol, where every UE first harvests the energy emitted by the AP and then sends data to the AP directly or via other UEs acting as relays in a time-division multiplexing manner. In this protocol, the AP selects the combination of transmission power and routing topology by matching load and energy distributions in the network while minimizing the average transmission delay. Furthermore, we develop a method generating scheduling schemes for this protocol to avoid data overflow in the UE relay. To determine the combination with minimum delay, we approximate the average delay as a Markov decision process and propose a low-complexity sample path-based algorithm to obtain a near-optimal solution. Simulation results demonstrate that the proposed protocol is able to balance the energy distribution while minimizing the transmission delay.\n",
      "In this paper, a novel screen content transcoding framework is presented to efficiently bridge the state-of-art High Efficiency Video Coding (HEVC) standard and its incoming screen content coding (SCC) extension currently pending finalization. The proposed scheme is implemented as an Intra-coding “pre-processing” module on top of official SCC test model software (SCM). Both Coding Unit (CU) statistical features (such as CU color quantity, CU pixel variance, CU edge directionality distribution, etc.) and decoded video side information (such as CU partitions, modes, residual, etc.) are jointly analyzed. Accordingly, fast CU mode decisions and CU partitions bypass / termination heuristics are designed. Compared with SCM-4.0 official release, the proposed fast transcoding scheme can achieve an average of 48% re-encoding complexity reduction over JCT-VC screen content testing sequences with less than 2.14% marginal BD-Rate increase under SCC common testing conditions for All-Intra (AI) configuration.\n",
      "Content-Centric Networking (CCN) proposals rethink the communication model around named data. In-network caching and multipath routing are regarded as two fundamental features to distinguish the CCN from the current host-centric IP network. In this paper, we tackle the problem of joint collaborative caching and multipath routing in CCN. We achieve this with an online and offline combination caching scheme based on a local content popularity statistic results. Besides, we place the content heterogeneously along a path and resort to a caching aware dynamic multipath routing in a coordination fashion. The proposed scheme can increase the content diversity and improve the caching utility with the aim of minimizing the user access delay. Simulation experiments have been performed to evaluate the proposed scheme. Simulation results show that the proposed scheme is effective and outperforms the existing caching mechanisms in CCN.\n",
      "The classic constant false alarm rate edge detector with a rectangle-shaped filter has been proven to be effective and widely used in polarimetric synthetic aperture radar (PolSAR) images. However, in practical use, the assumption of complex Wishart distribution is often not respected, particularly in heterogeneous urban areas. In addition, as a simple smoothing filter, the rectangle-shaped window is often shown to be easy to incur false edge pixels near true edges. Therefore, its performance is limited. To overcome this restriction, we propose a new edge detector for PolSAR images, which utilizes the spherically invariant random vector product model to estimate the normalized covariance matrix for each pixel, and then replace the rectangle-shaped filter with a Gauss-shaped filter. The performance of our proposed methodology is presented and analyzed on two real PolSAR data sets, and the results show that the new edge detector attains better performance than the classic one, particularly for urban areas.\n",
      "Anonymous Identity-Based Broadcast Encryption  (AIBBE) allows a sender to broadcast a ciphertext to multi-receivers, and keeps receivers' anonymity. The existing AIBBE schemes fail to achieve efficient decryption or strong security, like the constant decryption complexity, the security under the adaptive attack, or the security in the standard model. Hence, we propose two new AIBBE schemes to overcome the drawbacks of previous schemes in the state-of-art. The biggest contribution in our work is the proposed AIBBE scheme with constant decryption complexity and the provable security under the adaptive attack in the standard model. This scheme should be the first one to obtain advantages in all above mentioned aspects, and has sufficient contribution in theory due to its strong security. We also propose another AIBBE scheme in the  Random Oracle  (RO) model, which is of sufficient interest in practice due to our experiment.\n",
      "Computing similarity, especially Jaccard Similarity, between two datasets is a fundamental building block in big data analytics, and extensive applications including genome matching, plagiarism detection, social networking, etc. The increasing user privacy concerns over the release of has sensitive data have made it desirable and necessary for two users to evaluate Jaccard Similarity over their datasets in a privacy-preserving manner. In this paper, we propose two efficient and secure protocols to compute the Jaccard Similarity of two users' private sets with the help of an unfully-trusted server. Specifically, in order to boost the efficiency, we leverage Minhashing algorithm on encrypted data, where the output of our protocols is guaranteed to be a close approximation of the exact value. In both protocols, only an approximate similarity result is leaked to the server and users. The first protocol is secure against a semi-honest server, while the second protocol, with a novel consistency-check mechanism, further achieves result verifiability against a malicious server who cheats in the executions. Experimental results show that our first protocol computes an approximate Jaccard Similarity of two billion-element sets within only 6 minutes (under 256-bit security in parallel mode). To the best of our knowledge, our consistency-check mechanism represents the very first work to realize an efficient verification particularly on approximate similarity computation.\n",
      "In this paper, a novel cascaded seven-level inverter topology with a single input source integrating switched-capacitor techniques is presented. Compared with the traditional cascade multilevel inverter, the proposed topology replaces all the separate dc sources with capacitors, leaving only one H-bridge cell with a real dc voltage source and only adds two charging switches. The capacitor charging circuit contains only power switches, so that the capacitor charging time is independent of the load. The capacitor voltage can be controlled at a desired level without complex voltage control algorithm and only use the most common carrier phase-shifted sinusoidal pulse width modulation strategy. The operation principle and the charging–discharging characteristic analysis are discussed in detail. A 1-kW experimental prototype is built and tested to verify the feasibility and effectiveness of the proposed topology.\n",
      "Verbs are important in semantic understanding of natural language. Traditional verb representations, such as FrameNet, PropBank, VerbNet, focus on verbs' roles. These roles are too coarse to represent verbs' semantics. In this paper, we introduce verb patterns to represent verbs' semantics, such that each pattern corresponds to a single semantic of the verb. First we analyze the principles for verb patterns: generality and specificity. Then we propose a nonparametric model based on description length. Experimental results prove the high effectiveness of verb patterns. We further apply verb patterns to context-aware conceptualization, to show that verb patterns are helpful in semantic-related tasks.\n",
      "Tone injection (TI) can effectively reduce the peak-to-average power ratio (PAPR) of orthogonal frequency division multiplexing (OFDM) signals without incurring data rate loss and extra side information. However, the optimal TI scheme requires an exhaustive search over all combinations of the possible perturbations of the expanded constellation over all perturbed subcarriers, which is not suitable for practical applications. To reduce the complexity while still achieving good PAPR performance, a low-complexity TI scheme based on distortion signals is proposed in this paper. Motivated by the goal of mitigating the distortions when OFDM signals pass high power amplifiers, we intuitively get the perturbation information for OFDM signals directly from the distortion signals. By exploiting the distribution of the perturbation vectors depending on the distortion signals, we design a search range limited extended constellation. Then, based on the statistical information of perturbed subcarriers, the number of the subcarriers to be perturbed is restricted. Moreover, to further reduce the complexity, we choose a proper subcarrier perturbation sequence from candidate subcarrier sets according to the mutual information between the peak sample and the distortion signals. With the selected subcarrier perturbation sequence, the original problem is decomposed into a sequential search problem, which provides a dramatic complexity reduction. Simulation results demonstrate that the proposed scheme can achieve significant complexity savings while maintaining a good PAPR performance.\n",
      "In this paper, our goal is to speed up a standard sliding window detector while maintaining detection accuracies. We do this by decomposing its weight template into multiple component detectors with no redundancy. Each component detector captures partial discriminative appearance, and they can be used in a cascade detection pipeline to aggressively reduce the size of search space. We formulate the decomposition problem as an orthogonal procrustes problem. We further approximate the component detector in the first cascade layer as separable 2D filters (i.e. a product between two vector filters). The running time of such component detector is thus reduced from quadratic to linear with the dimension lengths of the detector template. We conduct extensive experiments using our approach on two well-known object detection datasets: INRIA pedestrian detection dataset and PASCAL VOC 2007 detection dataset. We used HOG features and CNN features in our experiments. Our approach is almost \"free lunch\": without manipulating the feature representations, it makes the detection process several times faster.\n",
      "In this paper, we propose a novel Deep Localized Makeup Transfer Network to automatically recommend the most suitable makeup for a female and synthesis the makeup on her face. Given a before-makeup face, her most suitable makeup is determined automatically. Then, both the before-makeup and the reference faces are fed into the proposed Deep Transfer Network to generate the after-makeup face. Our end-to-end makeup transfer network have several nice properties including: (1) with complete functions: including foundation, lip gloss, and eye shadow transfer; (2) cosmetic specific: different cosmetics are transferred in different manners; (3) localized: different cosmetics are applied on different facial regions; (4) producing naturally looking results without obvious artifacts; (5) controllable makeup lightness: various results from light makeup to heavy makeup can be generated. Qualitative and quantitative experiments show that our network performs much better than the methods of [Guo and Sim, 2009] and two variants of NerualStyle [Gatys et al., 2015a].\n",
      "D2D offloading reduces the load of cellular network by asking mobile nodes to download content directly from storage of neighboring helpers via short range links. In this paper, we introduce a novel storage assignment scheme that can enhance storage utilization for D2D networks that have different types of storage nodes. Unlike traditional D2D systems that only use storage as static content cache, our scheme uses on-demand relaying to enhance storage utilization. Our on-demand relaying scheme replicates rare content when it is requested. Therefore, the proposed scheme can greatly increase the amount of content supported by the offloading system. We develop a convex optimization based algorithm to find the optimal storage assignment tradeoff between static caching and on-demand relaying. Numerical results and real-world trace-driven simulations show that our algorithm can achieve 30% reduction in offloading failure rate compared to static schemes.\n",
      "In this paper, we develop and analyze a new multiscale discontinuous Galerkin (DG) method for one-dimensional stationary Schrodinger equations with open boundary conditions which have highly oscillating solutions. Our method uses a smaller finite element space than the WKB local DG method proposed in Wang and Shu (J Comput Phys 218:295---323, 2006) while achieving the same order of accuracy with no resonance errors. We prove that the DG approximation converges optimally with respect to the mesh size $$h$$h in $$L^2$$L2 norm without the typical constraint that $$h$$h has to be smaller than the wave length. Numerical experiments were carried out to verify the second order optimal convergence rate of the method and to demonstrate its ability to capture oscillating solutions on coarse meshes in the applications to Schrodinger equations.\n",
      "Device-to-device (D2D) communication plays a crucial role in improving the performance of cellular systems, and it is expected to be an innovative technology for next-generation wireless systems. Although significant progress has been made toward cellular and D2D coexistence, the issue of access control for D2D communications in the cellular network has received limited attention. In this paper, we address this issue by employing the network calculus (NC) theory for the first time. We propose a multipriority model, which assigns the strictly highest priority to cellular users and multiple levels of priority to D2D users to characterize the communication requests' access. The proposed model facilitates interference avoidance between cellular and D2D communications and, thus, enhances the quality of service (QoS) of the cellular system. We also apply the NC theory to analyze the worst-case performance of service rate, delay, and backlog for processing communication requests of cellular and D2D users. Both theoretical and experimental results demonstrate that the proposed model is effective and applicable to characterize the access control for D2D communications underlaying cellular networks.\n",
      "In discriminative tracking algorithms, the accuracy of classifier which relies heavily on the selection of training samples can directly influence the performance of visual tracking. Motivated by above, a tracking algorithm is presented based on regularized approximate residual weighted subsampling in the paper. Through the subsampling procedure, the corrupted samples which exert adverse impacts on the estimated classifier are ensured to be selected infrequently, thus making the classifier trained with the selected sample subset more robust to the noise caused by object appearance variations. Furthermore, an effective model updating strategy is adopted to enhance the flexibility of the tracker to the changes. Compared with some state-of-the-art trackers, our tracking algorithm performs better on a typical benchmark.\n",
      "Range-based positioning by estimating the time of arrival (ToA) of the line-of-sight (LoS) path using orthogonal frequency-division multiplexing (OFDM) signals has gained remarkable attention. Multipath propagation significantly influences the ToA estimate, resulting in position errors. To mitigate these errors, the multipath channel needs to be accurately estimated, usually with a large number of known pilot symbols. A tradeoff between the data rate and the multipath channel estimation performance must be made for joint communication and positioning applications. This paper proposes a semiblind range tracking algorithm, which allows the reduction of the required number of pilot symbols significantly. For moving receivers, the algorithm needs, only in the beginning, an OFDM symbol with known pilot symbols, which increases the data rate dramatically. Instead of tracking the complex amplitude of individual subcarriers, the proposed algorithm directly tracks the multipath channel parameters, including the complex amplitude, delay, and angle of arrival (AoA) of individual multipath components. The estimated delay of the first arrived path, i.e., its ToA, is used to calculate the range between the transmitter and receiver antennas. Evaluations of the algorithm are provided by simulations in an artificial scenario and using channel sounder measurement data. The results show that the proposed tracking algorithm can effectively track the range between transmitter and receiver and detect the data.\n",
      "A generalized Chinese remainder theorem (CRT) for multiple integers from residue sets has been studied recently, where the correspondence between the remainders and the integers in each residue set modulo several moduli is not known. A robust CRT has also been proposed lately to robustly reconstruct a single integer from its erroneous remainders. In this paper, we consider the reconstruction problem of two integers from their residue sets, where the remainders not only are out of order but also may have errors. We prove that two integers can be robustly reconstructed if their remainder errors are less than    $M/8$   , where    $M$    is the greatest common divisor of all the moduli. We also propose an efficient reconstruction algorithm. Finally, we present some simulations to verify the efficiency of the proposed algorithm. This paper is motivated from and has applications in the determination of multiple frequencies from multiple undersampled waveforms.\n",
      "Extracting opinion words and opinion targets from online reviews is an important task for fine-grained opinion mining. Usually, traditional extraction methods under the pipeline-based framework have higher precision but lower recall, while methods in the propagation-based framework possess greater recall but poorer precision. To achieve better performance both in precision and recall, this paper proposes a unified framework for fine-grained opinion mining, combining propagation with refinement in a dynamic and iterative process. In the propagation process, syntactic patterns are chosen as opinion relations to extract new opinion words and targets. Besides, syntactic patterns are further generalized to make them more flexible and scalable. In the refinement process, a three-layer opinion relations graph (ORG) model is constructed based on three types of candidates: opinion word candidates, opinion target candidates and syntactic pattern candidates. A sorting algorithm based on ORG model is proposed to rank all the candidates in their own type, and low-rank candidates are removed from candidate datasets. Repeat propagation and refinement until the syntactic pattern candidate set reaches stable. Experimental results on both English and Chinese online reviews demonstrate the effectiveness of proposed framework and its methods, comparing with the-state-of-the-art methods.\n",
      "Resource competition and conflicts in datacenter networks (DCNs) are frequent and intense. They become inevitable when mixing elephant and mice flows on shared transmission paths, resulting in arbitration between throughput and latency and performance degradation. We propose a novel flow scheduling scheme, Freeway, that leverages on path diversity in the DCN topology to guarantee, simultaneously, mice flow completion within deadline and high network utilization. Freeway adaptively partitions the available paths into low latency and high throughput paths and provides different transmission services for each category. A M/G/1-based model is developed to theoretically obtain the highest value of average delay over the path that will guarantee for 99% of mice flows their completion time before the deadline. Based on this bound, Freeway proposes a dynamic path partitioning algorithm to adjust dynamically with varying traffic load the number of low latency and high throughput paths. While mice flows are transmitted over low latency paths using a simple equal cost multiple path (ECMP) scheduling, Freeway load balances elephant flows on different high-throughput paths. We evaluate Freeway in a series of simulation on a large scale topology and use real traces. Our evaluation results show that Freeway significantly reduces the mice flows completion time within deadlines, while achieving remarkable throughput compared with current schemes. It is remarkable that Freeway does not need any change of DCN switch fabrics or scheduling algorithms and can be deployed easily on any generic datacenter network with switches implementing VLANs and trunking.\n",
      "Linz-Donawitz converter Gas (LDG), regarded as an essential secondary energy resource, plays a significant role for the entire production process of steel industry. In a LDG system, the gas holders are crucial equipment for temporary energy storage and buffers connecting with the gas generation units and the gas users. The accurate long-term prediction for the holders levels of such a system would be very necessary for energy scheduling and its optimal decision making. Given the practical characteristics of the LDG system in a steel plant, a granular-computing (GrC)-based hybrid collaborative fuzzy clustering (HCFC) algorithm is proposed in this study for the long-term prediction of the multiple holders levels. The hybrid structure considers the features regarding to a gas holder, of which the horizontal part elaborates the mutual influences among different time spaces of a holder level, while the vertical one describes them among the influence factors (denoting the gas generation units or the users). Then, the modeling algorithm is also explicitly derived in this study. To verify the performance of the proposed approach, two groups of simulation are carried out by employing the real-world industrial data coming from this plant, in which the single-output method and the iterative computing-based one are comparatively analyzed. The results indicate that the proposed approach provides a remarkable accuracy for such an industrial application.\n",
      "Security of data is an issue that is of significant interest. In this paper, we propose a new compressive sensing-based data encryption system that can represent the original signal with far fewer samples than the conventional Nyquist sampling-based system. Compressive sensing could also be treated as an encryption algorithm with good secrecy. As an application example, we apply it to sense-through-wall ultra-wideband UWB noise radar that requires enormous storage space and high security. Interestingly, a random Gaussian matrix is sufficient to capture the information of UWB noise radar signal; no knowledge of UWB signal is required in advance. Simulation results indicate only one-third of the original samples are needed to perfectly recover UWB noise radar signal, and compressive sensing provides good secrecy as an encryption algorithm. It is impossible to retrieve the original message without the entire sensing matrix. Copyright © 2013 John Wiley & Sons, Ltd.\n",
      "In this paper, we study the problem of joint power control and beamforming design for simultaneous wireless information and power transfer (SWIPT) in an amplify-and-forward (AF) based two-way relaying (TWR) network. The considered system model consists of two source nodes and a relay node. Two single-antenna source nodes receive information and energy simultaneously via power splitting (PS) from the signals sent by a multi-antenna relay node. Our objective is to maximize the weighted sum energy at the two source nodes subject to quality of service (QoS) constraints and the transmit power constraints. However, the joint optimization of the relay beamforming matrix, the source transmit power and PS ratio is intractable. To find a closed-form solution of the formulated problem, we decouple the primal problem into two subproblems. In the first problem, we intend to optimize the beamforming vectors for given transmit powers and PS ratio. In the second subproblem, we optimize the remaining parameters with obtained beamformers. It is worth noting that although the corresponding subproblem are nonconvex, the optimal solution of each subproblem can be found by using certain techniques. The iterative optimization algorithm finally converges. Simulation results verify the effectiveness of the proposed joint design.\n",
      "Given a set of objects and a query q, a point p is q’s Reverse k Nearest Neighbour (RkNN) if q is one of p’s k-closest objects. RkNN queries have received significant research attention in the past few years. However, we realize that the state-of-the-art algorithm, SLICE, accesses many objects that do not contribute to its RkNN results when running the filtering phase, which deteriorates the query performance. In this paper, we propose a novel RkNN algorithm with pre-computation by partitioning the data space into disjoint rectangular regions and constructing the guardian set for each region R. We guarantee that, for each q that lies in R, its RkNN results are only affected by the objects in R’s guardian set. The advantage of this approach is that the results of a query \\(q\\in R\\) can be computed by using SLICE on only the objects in its guardian set instead of using the whole dataset. Besides, we raise two new useful variants of RkNN and propose algorithms. Our comprehensive experimental study on synthetic and real the proposed approaches are the most efficient algorithms for RkNN and its variants.\n",
      "Antenna switch enables multiple antennas to share a common RF chain, thus an additional spatial dimension, i.e., antenna index, can be utilized in the design of single RF chain MIMO and information can be conveyed via both signal space and spatial dimension. In this paper, we propose a unified adaptive transmission scheme - adaptive spatial modulation that allocates information into signal space and spatial dimension in order to maximize the overall channel capacity for single RF chain MIMO. The proposed adaptive spatial modulation is realized by using Huffman coding, i.e., designing variable length codes to activate the transmit antenna with different probabilities. The optimal antenna activation probability is derived through optimizing channel capacity. To make the optimization tractable, closed form upper bound and lower bound are derived as the effective approximations for channel capacity. Numerical results show that the proposed adaptive spatial modulation offers considerable performance improvement over both spatial modulation and transmit antenna selection.\n",
      "Motion trajectory tracing in indoor environment has become increasingly important, has the potential to support a broad array of applications including elder care, business analysis, pedestrian navigation. Traditional approaches involve wearable sensors, specialized hardware installations. This paper presents SmartMTra, an infrastructure-free, inertial sensor based motion trajectory tracing system in indoor environment through the use of smartphone. SmartMTra exploits various scenarios in daily activity. It figures out common feature states both in pedestrian's motion mode, smartphone carrying pattern, which can be uniquely identified by adapting domain-divided statistics model techniques. Pedestrian Dead Reckoning (PDR) is applied to trace the pedestrian's real-time motion trajectory with the detected step counts, step-length, heading information. Our experimental evaluation in a campus building demonstrates that errors during 150m travelled distance were 5.2m horizontally, 0.2m during up-down stairs with robustness to scenarios.\n",
      "Many predictive resource scaling approaches have been proposed to overcome the limitations of the conventional reactive approaches most often used in clouds today. In general, due to the complexity of clouds, these reactive approaches were often forced to make significant limiting assumptions in either the operating conditions/requirements or expected workload patterns. As such, it is extremely difficult for cloud users to know which – if any – existing workload predictor will work best for their particular cloud activity, especially when considering highly-variable workload patterns, non-trivial billing models, variety of resources to add/subtract, etc. To solve this problem, we conduct comprehensive evaluations for a variety of workload predictors under real-world cloud configurations. The workload predictors cover four classes of 21 predictors: naive, regression, temporal, and non-temporal methods. We simulate a cloud application under four realistic workload patterns, two different cloud billing models, and three different styles of predictive scaling. Our evaluation confirms that no workload predictor is universally best for all workload patterns, and shows that Predictive Scaling-out + Predictive Scaling-in has the best cost efficiency and the lowest job deadline miss rate in cloud resource management, on average providing 30% better cost efficiency and 80% less job deadline miss rate compared to other styles of predictive scaling.\n",
      "Recent years have seen the proliferation in versatile mobile devices and application services that demand different data rates and latencies. Fixed channelization configuration in today’s wireless devices fail to be efficient in the presence of such dynamic demands. In this regard, fine-grained spectrum management designs have been advocated by the research community to embrace the heterogeneity in devices and services. However, manufacturers hesitate to make hardware investments without comprehensive understanding of these designs. To break this stalemate, software-defined wireless networking (SDWN) has been pushed to market as a cost-effective paradigm. Motivated by recent innovations in SDWN, this article systematically investigates the spectrum management architecture design that reaps the benefits of SDWN while maintaining the features of fine-grained channelization. We shed light on design principles and key challenges in realizing the SDWN-enabled spectrum management architecture. With these principles and challenges in mind, we develop a general architecture with a new baseband virtualization design. We build a prototype that seamlessly integrates with the IEEE 802.11 protocol stack and commodity RF front-end. We demonstrate that the proposed architecture improves spectrum efficiency by emulating the upper layer behaviors using the traces captured in a campus WLAN.\n",
      "It remains unclear whether brain networks are altered during conversion blindness in electroencephalogram (EEG) representation, which is of significance both on improving clinical management of conversion blindness, and providing objective evidence for judicial disputes. Functional brain network was constructed on coherence extracted from scalp EEGs, and conventional network metrics were analyzed in various frequency bands. Performance indices using relevant global features were evaluated with nonlinear and linear classifiers. To complement the incompetence of global feature in differentiating conversion blindness from control, local characteristics were selected by algorithm of minimum redundancy maximum relevance. Global network features were found most pronounced in the alpha band. Further consideration of local characteristics fused with global feature for differentiating conversion blindness from control, performance elevation was achieved with total accuracy of 92.32%, sensibility of 91.29%, and specificity of 93.36%. Experimental results demonstrated the potential of the proposed approach to represent brain network of conversion blindness with symptom of complete bilateral visual loss, it also provided new perspective to understand conversion blindness.\n",
      "Information diffusion and disease spreading in communication-contact layered network are typically asymmetrically coupled with each other, in which disease spreading can be significantly affected by the way an individual being aware of disease responds to the disease. Many recent studies have demonstrated that human behavioral adoption is a complex and non-Markovian process, where the probability of behavior adoption is dependent on the cumulative times of information received and the social reinforcement effect of the cumulative information. In this paper, the impacts of such a non-Markovian vaccination adoption behavior on the epidemic dynamics and the control effects are explored. It is found that this complex adoption behavior in the communication layer can significantly enhance the epidemic threshold and reduce the final infection rate. By defining the social cost as the total cost of vaccination and treatment, it can be seen that there exists an optimal social reinforcement effect and optimal information transmission rate allowing the minimal social cost. Moreover, a mean-field theory is developed to verify the correctness of simulation results.\n",
      "For the next generation of spaceborne synthetic aperture radar remote sensing satellites, high resolution and wide coverage are important goals. Digital beam-forming (DBF) with multichannels in elevation is a great and promising candidate to cover wide swaths. In this letter, we focus on onboard digital processing for DBF in elevation. It is generally believed that the onboard processing for DBF is challenging due to the limitations in flight-hardware availability, the heavy computational load, and the high resource occupation. In order to reduce the computational load of DBF, one novel processing scheme is proposed. This proposed scheme performs a modified time-variant weighting on a real intermediate frequency signal of each subchannel, and the weighted signals of all channels are summed to two real data streams. To obtain a correct DBF output, a modified quadrature demodulation process is presented. Then, the scheme is extended to apply FIR filters for overcoming pulse extension loss. Furthermore, improved time-variant weighting coefficients are derived to compensate the phase errors brought by the FIR filtering process. Compared with the present processing flow, the proposed scheme could significantly reduce the computational load and resource occupation.\n",
      "This paper presents an autopilot design for a robotic unmanned surface vehicle in the presence of unknown yaw dynamics and measurement noises. A robust adaptive steering law is developed with the aid of a predictor, neural networks, and a modified dynamic surface control technique. Specifically, a predictor together with a low-frequency learning-based neural updating law is developed to identify the unknown yaw dynamics, as well as to reconstruct the states corrupted by measurement noises. Besides, to avoid the noise amplification effect of first-order filter, a linear tracking differentiator is incorporated into the dynamic surface control design approach to produce a noise-tolerant estimate of virtual control derivative. The stability of the closed-loop autopilot system is established by Lyapunov analysis. A salient feature of the developed controller is that it can achieve the desired performance in the presence of model uncertainty and measurement noises, simultaneously. Both simulation and experiment results are given to validate the efficacy of the proposed method.\n",
      "In this letter, a generalized multi-relay selection scheme is proposed to improve the security in a cooperative relay network. Assuming that only the statistical channel state information of the eavesdropper is available at the transmitter, a semiclosed-form expression of the secrecy outage probability (SOP) is derived. Based on this, both the power allocation factor and the number of relay nodes are jointly optimized to minimize the SOP. Simulation results show that the proposed multi-relay selection scheme outperforms the conventional single-relay selection scheme without increasing the overhead.\n",
      "In this paper, we address adaptive predictor feedback design for a simplified drilling system in the presence of disturbance and time-delay. The main objective is to stabilize the bottomhole pressure at a critical depth at a desired set-point directly. The stabilization of the dynamic system and the asymptotic tracking are demonstrated by the proposed adaptive control, where the adaptation employs Lyapunov update law design with normalization. The proposed method is evaluated using a high fidelity drilling simulator and cases from a North Sea drilling operation are simulated. The results show that the proposed predictor controller is effective to stabilize the bottom hole pressure within the desired margins and compensate the effects of the delay and disturbance.\n",
      "Researchers and engineers have paid more attention to cloud-based application systems, whose features are virtualization and services provisioning. Fortunately the technology can be just right to healthcare. Meanwhile, security has also become more challenging. Although many cloud-based RFID authentication protocols have been proposed, some of them only improve the function and performance without considering security and privacy, and most of them are heavyweight. It is not appropriate in the field of healthcare, because improving the trustworthiness of anonymous virtual computing services should be the primary consideration. So we propose a lightweight privacy protection authentication scheme which can be applied in the cloud environment, in the scheme, service providers could be anonymous or unknown to the application consumer. Assuming many hospitals build a cloud platform together, the information of patient and his physician will be stored anonymously in the cloud. Patient can go to a doctor in any one hospital with a unique RFID tag. Reader may be fixed or mobile; Readers read tags and upload collected data to the cloud for further processing in real time. Compared with some traditional schemes, our scheme is lightweight, cost-efficient, elastic scalability, real-time, and easy to against synchronization attack.\n",
      "For quadrotor aircraft, atmospheric turbulence, wind shear, and model uncertainty are important factors which affect the quality of flight, or even lead to a failure of flight. In this paper, we consider the control problem of quadrotor aircraft with linear or nonlinear disturbances including model uncertainty. By employing backstepping technique, adaptive backstepping controllers are proposed which ensure the stability and reliability of the closed-loop system. Simulation results illustrate the effectiveness of proposed scheme.\n",
      "In this paper, a distributed adaptive control scheme is proposed for nth order multi-agent systems with pure integrator type of subsystem dynamics. It is assumed that the information transmission condition among different subsystems is represented by a fixed, balanced and weakly connected directed graph. The full knowledge of desired trajectory is allowed totally unknown by part of the subsystems, except that its first nth derivatives are bounded. It is shown that the globally uniform boundedness of all closed-loop signals and asymptotically consensus tracking for all the subsystem outputs can be guaranteed.\n",
      "As one of the promising candidates for the next generation network, Named Data Networking (NDN) has more advantages than the TCP/IP network in areas such as mobility, content distribution and security. Although NDN is designed to defense the majority Distributed Denial of Service (DDoS) attack in the current Internet, it anticipates some new varietal DDoS attacks. A representative DDoS form is called Interest Flooding Attacks (IFA), which can be launched easily by overflowing the PIT and can do immeasurable damage to the NDN. The existing IFA detection and countermeasure methods are mainly based on the PIT abnormal state statistics. However, these methods may cause misjudgment and damage the legitimate users, especially in the case of low-rate DDoS attacks or network congestion. In this paper, we propose an IFA detection scheme based on cumulative entropy by monitoring the content request abnormal distribution and then provide the malicious prefix identification method by relative entropy theory. An Interest traceback countermeasure is also used to restrain the attacker after detection. Therefore, the proposed scheme can reduce the IFA misjudgment and protect the legitimate user, and at the same time, can avoid overreaction to normal traffic fluctuation. Simulation results reveal that our methods can effectively mitigate the IFA in NDN.\n",
      "In this paper, the position control problem of a gear transmission servo system with dead-zone nonlinearity is investigated. All the parameters involved in both system model and dead-zone nonlinearity model are allowed totally unknown. An adaptive back stepping control scheme is presented. The effects of dead-zone nonlinearities are described by mismatched and matched disturbances, which are compensated by introducing additional estimates of their bounds in control laws and robust terms in parameter update laws. It is shown that all the closed-loop signals can be ensured bounded and the output regulation error will converge to a compact set. Simulation results are provided to show the effectiveness of the proposed adaptive control scheme.\n",
      "The watershed is an efficient algorithm for the segmentation of images. However, over-segmentation, which contains so many tiny regions that regions of interest cannot be identified easily, decreases the effectiveness. In this paper, pre-processing of images and the modification of watershed algorithm are both studied to restrain the over-segmentation. In the process of pre-processing, a kind of multi-scaled transform, contrast a trous wavelet based contourlet transform, is proposed and constructed to get sparse representation. In the aspect of modifying watershed, the “texture gradient” is defined, and the texture gradient is combined with marker-based watershed algorithm to reduce the number of segmented regions. The proposed method is tested by 36 prostate MR images and compared with several image segmentation algorithms; the experiment and comparison results show that the proposed method consistently restrains the number of segmented regions. The segmentation results correctly correspond to the main tissues in the images, and each tissue is integrally segmented, respectively with the elimination of small regions. The segmentation accuracy rate is 87.29%, which is higher than other methods under comparison.\n",
      "Radar emitter identification has been recognized as an indispensable task for electronic intelligence system. With the increasingly accumulated radar emitter intelligence and information, one key issue is to rebuild the radar emitter classifier efficiently with the newly-arrived information. Although existing incremental learning algorithms are superior in saving significant computational cost by incremental learning on continuously increasing training samples, they are not adaptable enough yet when emitter types, features and samples are increasing dramatically. For instance, the intra-pulse characters of emitter signals could be further extracted and thus expand the feature dimension. The same goes for the radar emitter type dimension when samples from new radar emitter types are gathered. In addition, existing incremental classifiers are still problematic in terms of computational cost, sensitivity to data input order, and difficulty in multiemitter type identification. To address the above problems, we bring forward a three-way incremental learning algorithm (TILA) for radar emitter identification which is adaptable for the increase in emitter features, types and samples.\n",
      "In this paper, we consider designing adaptive finite-time controllers for a class of SISO strict feedback nonlinear plants with parametric uncertainties based on given specifications. In addition to system stability and for the system output and virtual control errors to converge to zero, the specifications also include requirement on transient response in terms of convergence time and convergence rate. If the bound of the compact set in which unknown parameter lies is incorporated, the designed finite-time controller can ensure convergence within two stages. In the first stage the squared norm of the system states including the virtual control errors converges to a given invariant set faster than a specified linear rate. In the second stage the states converge to the origin with a convergence rate faster than a given exponential speed. If the bound of system initial states is also incorporated into the controller, then the states converge to the origin within the given time at a rate faster than the pre-specified exponential rate.\n",
      "As Internet-of-Things (IoT) and its applications are increasingly popular, where diverse multi-scale sensors and devices are seamlessly blended for ubiquitous communication infrastructure, broadcast operation still plays an essential role in scalable information dissemination to enhance information accessibility and availability. A unit-disk signal propagation model has been implicitly assumed and extensively applied to prior broadcast protocols, but we need to relax this assumption in reality. In this paper, we propose a transitional region aware broadcast protocol, called TCast, in variable wireless link qualities due to the signal propagation effects and non-uniform radiation pattern from the omni-directional antenna. The TCast is a stateless protocol and consists of two major operations, forwarder search and probabilistic rebroadcast. A sender neither maintains any neighbor information nor searches for a set of forwarders, but broadcasts a set of Beacon packets followed by a single Data packet. The sender repeatedly conducts the broadcast operations depending on the number of rebroadcasted packets overheard. Each receiver independently makes its own rebroadcast decision based on the number of received Beacon packets. A network-level random backoff mechanism is also proposed to avoid any packet contentions and collisions. The transitional region and its corresponding probability of packet reception are further investigated through a simple mathematical analysis. Extensive simulation experiments are also conducted using the OMNeT++, and simulation results indicate that the TCast shows competitive and scalable performance and is deployable in time-varying packet reception rates at receivers.\n",
      "Efficiently answering XML keyword queries has attracted much research effort in the last decade. The key factors resulting in the inefficiency of existing methods are the  common-ancestor-repetition  (CAR) and  visiting-useless-nodes  (VUN) problems. To address the CAR problem, we propose a  generic    top-down  processing strategy to answer a given keyword query w.r.t. LCA/SLCA/ELCA semantics. By “ top-down ”, we mean that we visit all  common ancestor  (CA) nodes in a depth-first, left-to-right order; by “ generic ”, we mean that our method is independent of the query semantics. To address the VUN problem, we propose to use child nodes, rather than descendant nodes to test the satisfiability of a node   $v$       w.r.t. the given semantics. We propose two algorithms that are based on either traditional inverted lists or our newly proposed LLists to improve the overall performance. We further propose several algorithms that are based on hash search to simplify the operation of finding CA nodes from all involved LLists. The experimental results verify the benefits of our methods according to various evaluation metrics.\n",
      "Intelligent transportation systems (ITS) has been one of the most active research fields in recent years. This paper identifies most productive authors, institutions, and countries/regions in  IEEE Transactions on Intelligent Transportation Systems  from 2000 to 2015. The results of bibliographic analysis show that the USA is the most influential country in that it not only has the most papers but also has six out of the ten most-cited papers. Meanwhile, researchers from China and Europe have published nearly half of the papers in this field. In addition, we generate three networks (including coauthorship network, keyword co-occurrence network, and author co-keyword network) to analyze collaboration patterns among authors in the field of ITS. The active keywords are investigated, and the top three are   vehicles  ,   road vehicles  , and   road traffic  . Finally, visual pictures are presented to show topological interactions of authors' collaboration.\n",
      "As a promising tool for dissecting the genetic basis of common diseases, expression quantitative trait loci (eQTL) study has attracted increasing research interest. Traditional eQTL methods focus on testing the associations between individual single-nucleotide polymorphisms (SNPs) and gene expression traits. A major drawback of this approach is that it cannot model the joint effect of a set of SNPs on a set of genes, which may correspond to biological pathways.\n",
      "We extract co-occurrence term relations using IdeaGraph.We extract semantic term relations using topic model.We fuse multiple types of relations to form a coupled term graph.We extract topics from the graph using a graph analytical approach. Topic detection as a tool to detect topics from online media attracts much attention. Generally, a topic is characterized by a set of informative keywords/terms. Traditional approaches are usually based on various topic models, such as Latent Dirichlet Allocation (LDA). They cluster terms into a topic by mining semantic relations between terms. However, co-occurrence relations across the document are commonly neglected, which leads to the detection of incomplete information. Furthermore, the inability to discover latent co-occurrence relations via the context or other bridge terms prevents the important but rare topics from being detected.To tackle this issue, we propose a hybrid relations analysis approach to integrate semantic relations and co-occurrence relations for topic detection. Specifically, the approach fuses multiple relations into a term graph and detects topics from the graph using a graph analytical method. It can not only detect topics more effectively by combing mutually complementary relations, but also mine important rare topics by leveraging latent co-occurrence relations. Extensive experiments demonstrate the advantage of our approach over several benchmarks.\n",
      "This study proposes a quality-driven approach that jointly selects the frequency reuse mode of device-to-device (D2D) links at lower layers, the video coding mode of frames, and multiple paths at higher layers for multimedia transmission in a cellular network with multi-antenna relays placed at the intersection of adjacent cells. Based on power control of the shared relay, the user-centered mode selection problem is formulated by maximising the quality of service (QoS) with total energy consumption constrained. Underlay mode and overlay mode are both considered. For each frame, one coding mode (I, P, or B) and multiple paths (cellular links and/or D2D links with/without relays) are selected to optimise the received video quality expectation. Different retransmission polices are considered to give higher protection levels to paths with higher transmission rates. The authors’ evaluation shows that the proposed unified power allocation and mode control scheme could enhance the perceived QoS as changing the energy constraint and the distances of different paths. The underlay mode is optimal in most cases. It provides evidences that it is not always the best to code all frames as I-frames, to initialise the relay transmission power as the maximum, or to select one single path for video transmission.\n",
      "This paper proposes a new model for speaker verification by employing kurtosis statistical method based on sparse coding of human auditory system. Since only a small number of neurons in primary auditory cortex are activated in encoding acoustic stimuli and sparse independent events are used to represent the characteristics of the neurons. Each individual dictionary is learned from individual speaker samples where dictionary atoms correspond to the cortex neurons. The neuron responses possess statistical properties of acoustic signals in auditory cortex so that the activation distribution of individual speaker’s neurons is approximated as the characteristics of the speaker. Kurtosis is an efficient approach to measure the sparsity of the neuron from its activation distribution, and the vector composed of the kurtosis of every neuron is obtained as the model to characterize the speaker’s voice. The experimental results demonstrate that the kurtosis model outperforms the baseline systems and an effective identity validation function is achieved desirably.\n",
      "Online News has become one of the most popular channels for consuming and understanding the real-world events. However, it is increasingly difficult for users to hold the full picture of massive events in a comprehensive perspective. In addition, how to motivate human to perceive the important rare events, which may trigger the subsequent emergency events, remains to be a challenge. To address these issues, we present EventPanorama, a framework to detect and visualize events. Firstly, we introduce a hybrid event detection method which combines topic modeling and Chance Discovery, and detects events more effectively by coupling multiple term-relations. Secondly, we propose a heterogeneous event-graph layout algorithm which takes the significance of events into consideration by leveraging latent co-occurrence relations to represent important rare events and thus enhance human cognition. An experiment demonstrates the superiority of EventPanorama by comparing with several benchmarks.\n",
      "Passive localization is fundamental for many applications such as activity monitoring and real-time tracking. Existing received signal strength (RSS)-based passive localization approaches have been proposed in the literature, which depend on dense deployment of wireless communication nodes to achieve high accuracy. Thus, they are not cost-effective and scalable. This paper proposes the RSS distribution-based localization (RDL) technique, which can achieve high localization accuracy without dense deployment. In essence, RDL leverages the RSS and the diffraction theory to enable RSS-based passive localization in sensor networks. Specifically, we analyze the fine-grained RSS distribution properties at a variety of node distances and reveal that the structure of the triangle is efficient for low-cost passive localization. We further construct a unit localization model aiming at high accuracy localization. Experimental results show that RDL can improve the localization accuracy by up to 50%, compared to existing approaches when the error tolerance is less than 1.5 m. In addition, we apply RDL to facilitate the application of moving trajectory identification. Our moving trajectory identification includes two phases: an offline phase where the possible locations can be estimated by RDL and an online phase where we precisely identify the moving trajectory. We conducted extensive experiments to show its effectiveness for this application—the estimated trajectory is close to the ground truth.\n",
      "Brain state decoding based on whole-head MEG has been extensively studied over the past decade. Recent MEG applications pose an emerging need of decoding brain states based on MEG signals originating from prespecified cortical regions. Toward this goal, we propose a novel region-of-interest-constrained discriminant analysis algorithm (RDA) in this paper. RDA integrates linear classification and beamspace transformation into a unified framework by formulating a constrained optimization problem. Our experimental results based on human subjects demonstrate that RDA can efficiently extract the discriminant pattern from prespecified cortical regions to accurately distinguish different brain states.\n",
      "We consider robust one-way trading with limited information on price fluctuations. Our analysis finds the best guarantee of difference from the optimal offline performance. We provide closed-form solution, and reveal for the first time all possible worst-case scenarios. Numerical experiments show that our policy is more tolerant of information inaccuracy than Bayesian policies, and can earn higher average revenue than other robust policies while keeping a lower standard deviation.\n",
      "In this paper, we focus on mining surprising periodic patterns in a sequence of events. In many applications, e.g., computational biology, an infrequent pattern is still considered very significant if its actual occurrence frequency exceeds the prior expectation by a large margin. The traditional metric, such as  support , is not necessarily the ideal model to measure this kind of  surprising patterns  because it treats all patterns equally in the sense that every occurrence carries the same weight towards the assessment of the significance of a pattern regardless of the probability of occurrence. A more suitable measurement,  information , is introduced to naturally value the degree of surprise of each occurrence of a pattern as a continuous and monotonically decreasing function of its probability of occurrence. This would allow patterns with vastly different occurrence probabilities to be handled seamlessly. As the accumulated degree of surprise of all repetitions of a pattern, the concept of  information gain  is proposed to measure the overall degree of surprise of the pattern within a data sequence. The  bounded information gain  property is identified to tackle the predicament caused by the violation of the  downward closure  property by the information gain measure and in turn provides an efficient solution to this problem. Empirical tests demonstrate the efficiency and the usefulness of the proposed model.\n",
      "The idea of virtual backbone has emerged to improve the efficiency of flooding based routing algorithms for wireless networks. The effectiveness of virtual backbone can be improved as its size decreases. The minimum connected dominating set (CDS) problem was used to compute minimum size virtual backbone. However, as this formulation requires the virtual backbone nodes to connect all other nodes, even the size of minimum virtual backbone can be large. This observation leads to consider the minimum partial CDS problem, whose goal is to compute a CDS serving only more than a certain portion of the nodes in a given network. So far, the performance ratio of the best approximation algorithm for the problem is $$O(\\ln \\varDelta ),$$O(lnΔ), where $$\\varDelta$$Δ is the maximum degree of the input general graph. In this paper, we first assume the input graph is a growth-bounded graph and introduce the first constant factor approximation for the problem. Later, we show that our algorithm is an approximation for the problem in unit disk graph with a much smaller performance ratio, which is of practical interest since unit disk graph is popular to abstract homogeneous wireless networks. Finally, we conduct simulations to evaluate the average performance of our algorithm.\n",
      "ABSTRACTThis paper investigates the distributed linear quadratic regulation (LQR) controller design method for discrete-time homogeneous scalar systems. Based on the optimal centralised control theory, the existence condition for distributed optimal controller is firstly proposed. It shows that the globally optimal distributed controller is dependent on the structure of the penalty matrix. Such results can be used in consensus problems and used to find under which communication topology (may not be an all-to-all form) the optimal distributed controller exists. When the proposed condition cannot hold, a suboptimal design method with the aid of the decomposition of discrete algebraic Riccati equations and robustness of local controllers is proposed. The computation complexity and communication load for each subsystem are only dependent on the number of its neighbours.\n",
      "Precipitation stations are important components of a hydrological monitoring network. Given their critical role in rainfall forecasting and flood warnings, along with limited observation resources, determining the optimal locations to deploy precipitation stations presents an important problem. In this paper, we use a maximal covering location problem to identify the best precipitation station sites. Considering the terrain conditions and the characteristics of a rainfall network, the original maximal covering location model is modified with the introduction of a set of additional constraints. The minimum density requirement is used to determine a precipitation station’s coverage range, and three weighting schemes are used to evaluate each demand object’s covering priority. As a typical mountainous watershed with high annual precipitation, the Jinsha River Basin is selected as the study area to test the applicability of the proposed method. Results show that the proposed method is effective for precipitation station configuration optimization, and the model solution achieves higher coverage than the real-world deployment. Compared with the commercial solver CPLEX, a genetic algorithm-based heuristic can significantly reduce the computation time when the problem size is large. Several deployment strategies are also discussed for establishing the optimal configuration of precipitation stations.\n",
      "In the t -Latency-Bounded Target Set Selection ( t -LBTSS) problem, we are given a simple graph G = ( V , E ) , a certain latency bound t and a threshold function ? ( v ) = ? ? d ( v ) ? for every vertex v of G , where 0 < ? < 1 is a rational number and d ( v ) is the degree of v in V , the goal is to find a target set S with smallest cardinality such that all vertices in V are activated by S by a so called \"diffusion process\" within t rounds as follows: Initially, all vertices in the target set become activate. Then at each step i of the process, each vertex get activated if the number of active vertices in its neighbor after i - 1 exceeds its threshold.For general graphs, the t -LBTSS problem is not only NP-hard, it is also hard to be approximated by Chen's inapproachability results (Chen, 2009). In this paper, we are interested in finding an optimal target set for some special family of graphs. A simple, tight but nontrivial inequality was presented which gives the lower bound of the total sum of degrees in a feasible target set to t -LBTSS problem, in terms of the number of edges in the graph. Necessary and sufficient conditions for equality to hold have been established, based on which we are able to construct families of infinite number of graphs for which the optimal solution to t -LBTSS problem become obvious. In particular, we gave an exact formula for the optimal solution of a kind of toroidal mesh graphs, while it seems difficult to tell what the optimal solutions are for these graphs without using the equality given in the paper.\n",
      "In this paper, we consider a class of globally Lipschitz nonlinear continuous networked control systems (NCS) incorporating large time-varying transmission delays and transmission protocols of communication networks with periodic sampling. To stabilize the NCS, we propose a new predictive control design scheme with plant outputs as the only available data. With uniformly globally exponentially stable (UGES) protocols, input-to-state stability of the entire NCS is ensured by small gain theory. In particular, the predictive controller can compensate transmission delays with any finite upper bound under the constraint that the sampling periods of the plant output and the observer output are small enough as well as the constraint that the predictor is accurate enough. The scheme is applied to a benchmark example to illustrate the effectiveness of our proposed method.\n",
      "Steady-State Visual Evoked Potentials (SSVEPs) are widely used in spatial selective attention. In this process the two kinds of visual simulators, Light Emitting Diode (LED) and Liquid Crystal Display (LCD), are commonly used to evoke SSVEP. In this paper, the differences of SSVEP caused by these two stimulators in the study of spatial selective attention were investigated. Results indicated that LED could stimulate strong SSVEP component on occipital lobe, and the frequency of evoked SSVEP had high precision and wide range as compared to LCD. Moreover a significant difference between noticed and unnoticed frequencies in spectrum was observed whereas in LCD mode this difference was limited and selectable frequencies were also limited. Our experimental finding suggested that average classification accuracies among all the test subjects in our experiments were 0.938 and 0.853 in LED and LCD mode, respectively. These results indicate that LED simulator is appropriate for evoking the SSVEP for the study of spatial selective attention.\n",
      "Multiview action recognition has received increasing attention over the past decade. Various approaches have been proposed to extract view-invariant features; among them, self-similarity matrices (SSMs) have shown outstanding performance. However, SSMs become sensitive when there's a very large view change. To make SSMs more robust to viewpoint changes, the authors propose a collaborative sparse coding framework. They integrate the classifier training process and sparse coding process into a unified collaborative filtering framework; this lets more discriminative sparse video representations and classifiers be learned by optimizing the dictionary and classifier jointly. Experimental results demonstrate the effectiveness of the framework.\n",
      "Video captioning which automatically translates video clips into natural language sentences is a very important task in computer vision. By virtue of recent deep learning technologies, e.g., convolutional neural networks (CNNs) and recurrent neural networks (RNNs), video captioning has made great progress. However, learning an effective mapping from visual sequence space to language space is still a challenging problem. In this paper, we propose a Multimodal Memory Model (M3) to describe videos, which builds a visual and textual shared memory to model the long-term visual-textual dependency and further guide global visual attention on described targets. Specifically, the proposed M3 attaches an external memory to store and retrieve both visual and textual contents by interacting with video and sentence with multiple read and write operations. First, text representation in the Long Short-Term Memory (LSTM) based text decoder is written into the memory, and the memory contents will be read out to guide an attention to select related visual targets. Then, the selected visual information is written into the memory, which will be further read out to the text decoder. To evaluate the proposed model, we perform experiments on two publicly benchmark datasets: MSVD and MSR-VTT. The experimental results demonstrate that our method outperforms the state-of-theart methods in terms of BLEU and METEOR.\n",
      "Multi-modal retrieval is emerging as a new search paradigm that enables seamless information retrieval from various types of media. For example, users can simply snap a movie poster to search for relevant reviews and trailers. The mainstream solution to the problem is to learn a set of mapping functions that project data from different modalities into a common metric space in which conventional indexing schemes for high-dimensional space can be applied. Since the effectiveness of the mapping functions plays an essential role in improving search quality, in this paper, we exploit deep learning techniques to learn effective mapping functions. In particular, we first propose a general learning objective that effectively captures both intramodal and intermodal semantic relationships of data from heterogeneous sources. Given the general objective, we propose two learning algorithms to realize it: (1) an unsupervised approach that uses stacked auto-encoders and requires minimum prior knowledge on the training data and (2) a supervised approach using deep convolutional neural network and neural language model. Our training algorithms are memory efficient with respect to the data volume. Given a large training dataset, we split it into mini-batches and adjust the mapping functions continuously for each batch. Experimental results on three real datasets demonstrate that our proposed methods achieve significant improvement in search accuracy over the state-of-the-art solutions.\n",
      "Two-dimensional contingency tables or co-occurrence matrices arise frequently in various important applications such as text analysis and web-log mining. As a fundamental research topic, co-clustering aims to generate a meaningful partition of the contingency table to reveal hidden relationships between rows and columns. Traditional co-clustering algorithms usually produce a predefined number of flat partition of both rows and columns, which do not reveal relationship among clusters. To address this limitation, hierarchical co-clustering algorithms have attracted a lot of research interests recently. Although successful in various applications, the existing hierarchical co-clustering algorithms are usually based on certain heuristics and do not have solid theoretical background. In this paper, we present a new co-clustering algorithm, HICC, with solid theoretical background. It simultaneously constructs a hierarchical structure of both row and column clusters, which retains sufficient mutual information between rows and columns of the contingency table. An efficient and effective greedy algorithm is developed, which grows a co-cluster hierarchy by successively performing row-wise or column-wise splits that lead to the maximal mutual information gain. Extensive experiments on both synthetic and real datasets demonstrate that our algorithm can reveal essential relationships of row (and column) clusters and has better clustering precision than existing algorithms. Moreover, the experiments on real dataset show that HICC can effectively reveal hidden relationships between rows and columns in the contingency table.\n",
      "Precise range history model and accurate focusing algorithms are challenges for high-resolution spaceborne Synthetic Aperture Radar (SAR) due to the curved orbits of satellites and spatial variations of signal models. The existing range models include HRM, AHRM and DRM. This paper focuses on the range history model for spaceborne SAR. In the paper, an accurate range model based on fourth-order Doppler parameters is proposed to accurately formulate the range history between SAR and targets. Compared with other models, such as DRM4 which is also based on fourth-order Doppler parameters, the proposed range model has much improved accuracy. All the simulation results validate the high precision of the proposed range model.\n",
      "In this paper, we joint autoencoder with active learning for hyperspectral imagery classification. Specifically, we learn the classifier via autoencoder, where the most informative samples are acitvely selected through the interaction between the autoencoder and active learning. Experimental results, conducted using both the Kennedy Space Center and the Indian Pines hyperspectral images, show that driven by active learning, the performance of autoencoder can be greatly improved.\n",
      "Anomaly detection has been known to be a challenging, ill-posed problem due to the uncertainty of anomaly and the interference of noise. In this paper, we propose a novel low rank anomaly detection algorithm in hyperspectral images (HSI), where three components are involved. First, due to the highly mixed nature of pixels in HSI, instead of using the raw pixel directly for anomaly detection, the proposed algorithm applies spectral unmixing algorithms to obtain the abundance vectors and uses these vectors for anomaly detection. Second, for better classification, a dictionary is built based on the mean-shift clustering of the abundance vectors to better represent the highly-correlated background and the sparse anomaly. Finally, a low-rank matrix decomposition is proposed to encourage the sparse coefficients of the dictionary to be low-rank, and the residual matrix to be sparse. Anomalies can then be extracted by summing up the columns of the residual matrix. The proposed algorithm is evaluated on both synthetic and real datasets. Experimental results show that the proposed approach constantly achieves high detection rate while maintaining low false alarm rate regardless of the type of images tested.\n",
      "This paper proposes a deep convolutional neural networks (CNNs) based method to automatically detect suburban buildings from high resolution Google Earth imagery. Traditional methods based on low-level hand-engineered features or mid-level bag of features have great limitations in complex environment, especially in suburban areas. Inspired by the astounding achievement of CNNs in object recognition and detection, we develop a novel method to detect buildings in cluttered images which consists of three main steps. Firstly, a multi-scale saliency computation is employed to extract built-up areas and a sliding windows approach is applied to generate candidate regions. Then, a CNN is applied to classify the regions. Finally, an improved non maximum suppression is used to remove false buildings. We test our method on a collection of very challenging Google Earth images and achieve 89% precision, which shows robustness and efficiency of our method.\n",
      "Analysis of fuel consumption on orbit satellite is presented. The fuel consumption model is nonlinear based on the orbit thruster and attitude control thrusters' misalignment, and is assumed to have unknown dynamics. The issue regarding as the fuel consumption of orbit optimization can be converted to multi-variable optimization with constraint. A modification of the Analytic Hierarchy Process (AHP) optimization algorithm is introduced, by which the fuel consumption estimation models and judgment matrix are established. Simulation results are presented and the optimization design criterion for AHP is also put forward.\n",
      "Reliable prediction intervals (PIs) construction for industrial time series is substantially significant for decision-making in production practice. Given the industrial data feature of high level noises and incomplete input, a high order dynamic Bayesian network (DBN)-based PIs construction method for industrial time series is proposed in this study. For avoiding to designate the amount and type of the basis functions in advance, a linear combination of kernel functions is designed to describe the relationships between the nodes in the network, and a learning method based on the scoring criterion--the sparse Bayesian score, is then reported to acquire suitable model parameters such as the weights and the variances. To verify the performance of the proposed method, two types of time series which are the classical Mackey-Glass data mixed by additive noises and a real-world industrial data are employed. The results indicate the effectiveness of our proposed method for the PIs construction of the industrial data with incomplete input.\n",
      "In the Minimum Weight Partial Connected Set Cover problem, we are given a finite ground set $$U$$U, an integer $$q\\le |U|$$q≤|U|, a collection $$\\mathcal {E}$$E of subsets of $$U$$U, and a connected graph $$G_{\\mathcal {E}}$$GE on vertex set $$\\mathcal {E}$$E, the goal is to find a minimum weight subcollection of $$\\mathcal {E}$$E which covers at least $$q$$q elements of $$U$$U and induces a connected subgraph in $$G_{\\mathcal {E}}$$GE. In this paper, we derive a \"partial cover property\" for the greedy solution of the Minimum Weight Set Cover problem, based on which we present (a) for the weighted version under the assumption that any pair of sets in $$\\mathcal {E}$$E with nonempty intersection are adjacent in $$G_{\\mathcal {E}}$$GE (the Minimum Weight Partial Connected Vertex Cover problem falls into this range), an approximation algorithm with performance ratio $$\\rho (1+H(\\gamma ))+o(1)$$?(1+H(?))+o(1), and (b) for the cardinality version under the assumption that any pair of sets in $$\\mathcal {E}$$E with nonempty intersection are at most $$d$$d-hops away from each other (the Minimum Partial Connected $$k$$k-Hop Dominating Set problem falls into this range), an approximation algorithm with performance ratio $$2(1+dH(\\gamma ))+o(1)$$2(1+dH(?))+o(1), where $$\\gamma =\\max \\{|X|:X\\in \\mathcal {E}\\}$$?=max{|X|:X?E}, $$H(\\cdot )$$H(·) is the Harmonic number, and $$\\rho $$? is the performance ratio for the Minimum Quota Node-Weighted Steiner Tree problem.\n",
      "Contiguous channel bonding for using more than one channels together in TV white space has been considered in the IEEE 802.22-specified wireless regional area network system. To support more flexible channel usage, this paper investigates non-contiguous channel bonding as a potential extended feature of IEEE 802.22 standard. The transceiver structure with Non-Contiguous OFDM (NC-OFDM) transmission to support non-contiguous channel bonding is presented. The simulation results show that non-contiguous channel bonding achieves better symbol error rate performance with slightly higher peak to average power ratio comparing with contiguous channel bonding. Finally, the power spectrum density of NC-OFDM signals with non-contiguous channel bonding is analyzed and the number of additional guard subcarriers for mitigating out-of-band emissions are determined to comply with the required transmission spectrum mask.\n",
      "Classical dictionary learning algorithms that rely on a single source of information have been successfully used for the discriminative tasks. However, exploiting multiple sources has demonstrated its effectiveness in solving challenging real-world situations. We propose a new framework for feature fusion to achieve better classification performance as compared to the case where individual sources are utilized. In the context of multimodal data analysis, the modality configuration induces a strong group/coupling structure. The proposed method models the coupling between different modalities in space of sparse codes while at the same time within each modality a discriminative dictionary is learned in an all-vs-all scheme whose class-specific sub-parts are non-correlated. The proposed dictionary learning scheme is referred to as the multimodal weighted dictionary learning (MWDL). We demonstrate that MWDL outperforms state-of-the-art dictionary learning approaches in various experiments.\n",
      "Drift is the most difficult issue in object visual tracking based on framework of “tracking-by-detection”. Due to the self-taught learning, the mis-aligned samples are potentially to be incorporated in learning and degrade the discrimination of the tracker. This paper proposes a new tracking approach that resolves this problem by three multi-level collaborative components: a high-level global appearance tracker provides a basic prediction, upon which the structure preserved low-level local patches matching helps to guarantee precise tracking with minimized drift. Those local patches are deliberately deployed on the foreground object via foreground/background segmentation, which is realized by a simple and efficient classifier trained by super-pixel segments. Experimental results show that the three closely collaborated components enable our tracker runs in real time and performs favourably against state-of-the-art approaches on challenging benchmark sequences.\n",
      "Early detection of myocardial ischemia via electrocardiographic methods is important and challenging. In the study, based on the standard 12-lead electrocardiography (ECG), a new method called cardiodynamicsgram (CDG) is proposed for early detection of myocardial ischemia. Using a recently proposed deterministic learning algorithm, the cardiodynamics information is extracted from the ST-T segments of standard 12-lead ECG. The CDG is generated by plotting the three-dimensional cardiodynamics information. By analyzing CDG morphology, it is found that significant correlations exist between CDG and ischemia. By evaluating ischemia patients and healthy controls from the Physikalisch-Technische Bundesanstalt (PTB) database and the General Hospital of Guangzhou Military Command, the CDG method achieves a mean sensitivity of 90.3% and a mean specificity of 87.8%, which are higher than those of both the standard 12-lead ECG and the exercise ECG. As it is noninvasive, convenient, and inexpensive, it is hopeful that CDG may become a cost-effective screening method for early detection of ischemic heart diseases.\n",
      "We investigate the scenario where a controller communicates with a plant at discrete time instants generated by an event-triggering mechanism. In particular, the latter collects sampled data from the plant and the controller at each sampling instant, and then decides whether the control input needs to be updated, leading to periodic event-triggered control (PETC). In this paper, we propose a systematic design procedure for PETC that stabilize general nonlinear systems. The design is based on the existence of a continuous-time state-feedback controller, which stabilizes the system in the absence of communication constraints. We then take into account the sampling and we design an event-triggering condition, which is only updated at some of the sampling instants, to preserve stability. An explicit bound on the maximum sampling period with which the triggering rule is evaluated is provided. We show that there exists a trade-off between the latter and a parameter used to define the triggering condition. The results are applied to a van de Pol oscillator as an illustration.\n",
      "Traditionally, magnetic loop detectors are often used to count vehicles passing over them in intelligent transportation system. Real-time image sequences are captured by video surveillance system. Virtual loop, which emulates the functionality of inductive loop detectors, is placed on images. It is more convenient, but it occurs in false detection and discrimination when vehicles are lane departure due to overtaking or crossing. This paper presents an effective approach for vehicle counting based on double virtual lines (DVL). Double virtual lines are assigned on images, which are across bidirectional multi-lane. The region between DVL is the detection zone, rather than virtual loop zone in each lane, so as to reduce the proportion of false detection and misjudgment from lane departure for vehicles. Then, in the detection zone, the dual-template convolution is designed to detect and locate moving vehicles to eliminate the mapping of one to many, many to one. The effective rules are given in terms of the constraint of the horizontal and vertical distances to improve the accuracy of vehicle counting. Experimental comparisons with the other method demonstrate the performance of the proposed method.\n",
      "Abstract Data-dependent acquisition (DDA) is the most common method used to control the acquisition process of shotgun proteomics experiments. While novel DDA approaches have been proposed, their evaluation is made difficult by the need of programmatic control of a mass spectrometer. An alternative is in silico analysis, for which suitable software has been unavailable. To meet this need, we have developed MSAcquisitionSimulator-a collection of C ++ programs for simulating ground truth LC-MS data and the subsequent application of custom DDA algorithms. It provides an opportunity for researchers to test, refine and evaluate novel DDA algorithms prior to implementation on a mass spectrometer. The software is freely available from its Github repository http://www.github.com/DennisGoldfarb/MSAcquisitionSimulator/ which contains further documentation and usage instructions. weiwang@cs.ucla.edu or ben_major@med.unc.eduSupplementary information: Supplementary data are available at Bioinformatics online.\n",
      "Categories play a fundamental role in human cognition. Defining features (short for DFs) are the key elements to define a category, which enables machines to categorize objects. Categories enriched with their DFs significantly improve the machine's ability of categorization and benefit many applications built upon categorization. However, defining features can rarely be found for categories in current knowledge bases. Traditional efforts such as manual construction by domain experts are not practical to find defining features for millions of categories. In this paper, we make the first attempt to automatically find defining features for millions of categories in the real world. We formalize the defining feature learning problem and propose a bootstrapping solution to learn defining features from the features of entities belonging to a category. Experimental results show the effectiveness and efficiency of our method. Finally, we find defining features for overall 60,247 categories with acceptable accuracy.\n",
      "This paper proposes a modular caterpillar climbing robot using spines as the attaching tools. To improve the reliability of the spines' engagement and disengagement, this paper discusses the reasonable trajectory of the spine and designs a driving mechanism of the spine based on the compliant mechanism theory. Then some compliant modules are designed and realized to build the caterpillar climbing robot. A climbing gait is designed to avoid collisions between the spines and the wall, and allows the robot to climb on a stucco-like wall with a 72 degrees incline. The real tests reveal that the deformation of the compliant toes reduces the sliding forces between the spines and the wall, and improve the climbing action obviously.\n",
      "The next generation of spaceborne synthetic aperture radar (SAR) remote sensing systems will emphasize on high-resolution and wide-coverage imaging. For these design goals, digital beamforming (DBF) in elevation is a promising candidate. DBF-SAR can provide global monitoring capacity for the continuous observation of a highly dynamic and rapidly changing world with high spatial resolution and short repeat intervals. A spaceborne experiment regarding a real complex scene and real spaceborne wave propagation channel effects remains a necessary step to complete the experimental verification of this advanced technique. Fortunately, the spaceborne–stationary bistatic configuration offers a potential chance to validate the advanced technique. The aforementioned experiment can be considered as a test bed for the development and implementation of DBF radar techniques applicable to Earth observation science and planetary measurements. The DBF experiment based on spaceborne–stationary bistatic configuration with TerraSAR-X as an illuminator has been successfully conducted in June 2013 by the Department of Space Microwave Remote Sensing System, Institute of Electronics, Chinese Academy of Sciences.\n",
      "Content-Centric Networking (CCN) proposals rethink the communication model around named data. In-network caching is a fundamental feature to distinguish the CCN from the current host-centric IP network. Caching scheme by hash-routing is a prominent solution as it enables a joint consideration of content placement and request-to-cache routing through a hash function, and therefore, makes the cached contents visible in the domain and meanwhile, eliminates the content redundancy. In this paper, we present here a collaborative caching scheme in a CCN AS domain, which can fully exploit in-network caching by network clustering and hash-routing. Specifically, a heuristic algorithm is proposed to dynamically adjust caching capacity of each cluster based on its egress traffic while controlling the total path stretch incurred. The proposed scheme can improve the caching utility and reduce caching redundancy. Extensive experiments have been performed to evaluate the proposed scheme. Simulation results show that the proposed is effective and outperforms the existing caching mechanisms in CCN.\n",
      "In data stream clustering studies, majority of methods are traditional hard clustering, the literatures of fuzzy clustering in clustering are few. In this paper, the fuzzy clustering algorithm is used to research data stream clustering, and the clustering results can truly reflect the actual relationship between objects and classes. It overcomes the either-or shortcoming of hard clustering. This paper presents a new method to detect concept drift. The membership degree of fuzzy clustering is used to calculate the information entropy of data, and according to the entropy to detect concept drift. The experimental results show that the detection of concept drift based on the entropy theory is effective and sensitive.\n",
      "Transparent computing is an implementation of ubiquitous computing that is aimed at providing active services for users. In transparent computing, the execution (computation) of computer instructions and data is temporally and spatially separated from their storage. Cloud computing solves the issue of data cloudlization, while transparent computing solves the one of software cloudlization. This paper define the concept of  Cloudware , and discusses how to deploy Cloudware in cloud environment efficiently. Based on a loosely coupled von Neumann computing model (also called Cygnus Model), we proposes a new platform to construct the PaaS platform which can directly deploy software on the cloud without any modification, while achieving a new model by the browser services, and we call it  transparent computing 2.0.  By using micro-service architecture, we can achieve such characteristics as good performance, scalable deployment, faults tolerance and flexible configuration. Finally, we presents a Cloudware PaaS platform prototype, called  CloudwareHub , and demonstrates the promising of the transparent computing 2.0 with the support of Cloudware technology.\n",
      "Chunking refers to a phenomenon whereby individuals group items together when performing a memory task to improve the performance of sequential memory. In this work, we build a bio-plausible hierarchical chunking of sequential memory (HCSM) model to explain why such improvement happens. We address this issue by linking hierarchical chunking with synaptic plasticity and neuromorphic engineering. We uncover that a chunking mechanism reduces the requirements of synaptic plasticity since it allows applying synapses with narrow dynamic range and low precision to perform a memory task. We validate a hardware version of the model through simulation, based on measured memristor behavior with narrow dynamic range in neuromorphic circuits, which reveals how chunking works and what role it plays in encoding sequential memory. Our work deepens the understanding of sequential memory and enables incorporating it for the investigation of the brain-inspired computing on neuromorphic architecture.\n",
      "With a significant advance in ciphertext searchability, public-key encryption with keyword search (PEKS) guarantees both security and convenience for outsourced keyword search over ciphertexts. In this paper, we establish static index (SI) and dynamic index (DI) for PEKS to make search efficient and secure in the state of the art. Suppose there are u senders to generate n searchable ciphertexts for w keywords. The search complexity of PEKS always is O ( n ) for each query, even if the keyword has been searched for multiple times. It is obviously inefficient for massive searchable ciphertexts. Fortunately, SI and DI help PEKS lowering the burden respectively in two phases: if the queried keyword is the first time to be searched, apply SI to reduce the complexity from O ( n ) to O ( u ? w ) ; otherwise, apply DI to reduce the complexity from O ( n ) to O ( w ) . Because DI is invalid for the first time search on any keyword, SI and DI are simultaneously applied with PEKS to complete our work as the secure hybrid indexed search (SHIS) scheme. Since u ? w ? n in practice, our SHIS scheme is significantly more efficient than PEKS as demonstrated by our analysis. In the end, we show the extension of SHIS to multi-receiver applications, which is absent for pure PEKS. We propose our high efficient Secure Hybrid Indexed Search (SHIS) scheme.We model semantic secure SHIS with universal transformations from PEKS and DE.We show the complexity of SHIS is much lower than PEKS and convergent.We universally extend SHIS by PKE schemes towards multiple-receiver applications.\n",
      "Compared to desktop computers, smartphones frequently connect to the Internet via various applications through private protocols, resulting in severe difficulties in searching publicly shared information and in measuring network-layer performance. Most related work has focused on device-based software-assisted measurement, which suffers scalability as well as usability issues. In this article, the authors propose TAM, a transparent agent architecture to monitor applications in terms of both network performance and content. TAM sets up distributed virtual agents for target mobile applications and can enable parallel large-scale measurements through high-density computing resources, such as the cloud. TAM can also identify and measure the infrastructures that provide services to mobile applications. It does this without any built-in assistance from mobile devices, making TAM act as a transparent service in a scalable and usable manner. Experimental results of the prototype implementation verify the architecture.\n",
      "With the popularity of multimedia sensing at cloud edges and the reducing cost of the Internet of Things (IoT) fog devices and systems, new challenges have been posed to efficiently deal with the big data multimedia traffic generated from IoT sensing units. Specifically, in this paper we introduce the concept of multimedia sensing as a service (MSaaS), and propose a generalized premium prioritization-based quality of experience paradigm for wireless big-volumes-of-data (BVDs) multimedia communications, with significant energy saving potentials for future multimedia IoT devices and systems. The key contribution of this new framework is its data diversity flexibility at the application layer, which could be flexibly adopted by future multimedia communication systems. Data dependencies in spatial, frequency and temporal domains are analyzed, and interaction with uplink resource allocation optimization are investigated with regards to wireless communication energy cost estimation. Extensive simulation results demonstrate that the proposed prioritization-based communication paradigm has significant energy saving potentials for BVD MSaaS wireless multimedia communications at cloud edges and fogs.\n",
      "This paper studies simultaneous wireless information and power transfer (SWIPT) systems in two-way relaying (TWR) channels. Here, two source nodes receive information and energy simultaneously via power splitting (PS) from the signals sent by a multi-antenna relay node. Our objective is to maximize the weighted sum of the harvested energy at two source nodes subject to quality of service (QoS) constraints and the relay power constraints. Three well-known and practical two-way relay strategies are considered, i.e., amplify-and-forward (AF), bit level XOR based decode-and-forward (DF-XOR) and symbol level superposition coding based DF (DF-SUP). For each relaying strategy, we formulate the joint energy transmit beamforming and PS ratios optimization as a nonconvex quadratically constrained problem. To find a closed-form solution of the formulated problem, we decouple the primal problem into two subproblems. In the first problem, we intend to optimize beamforming vector for a given PS ratio. In the second subproblem, we optimize the PS ratio with a given beamforming vector. It is worth noting that although the corresponding subproblem are nonconvex, the optimal solution of each subproblem can still be found by using certain techniques. We provide numerical results that demonstrate the advantage of adapting the different relaying strategies and weighted factors to harvest energy in two-way relaying channel.\n",
      "The isogeometric method is used to study the free vibration of thick plates based on Mindlin theory. The Non-uniform Rational B-Spline (NURBS) basis functions are employed to build the thick plate’s geometry models and serve as the shape functions for solution field approximation in finite element analysis. The Reissner–Mindlin plates built with multiple NURBS patches are investigated, in which several patches of the model have multi-interface and different patches may share a common point. In order to solve the non-conforming interface problems, Nitsche method is employed to glue different NURBS patches and only refers to the coupling conditions in this work. Various plate shapes, different boundary conditions and several kinds of thickness-span ratios are considered to verify the validity of the presented method. The dimensionless frequencies for different cases are obtained by solving the eigenvalue equation problems and compared with the existing reference solutions or the results calculated by ABAQUS software. Several numerical examples exhibit the effectiveness of the isogeometric approach. It shows that the natural frequencies of the Reissner–Mindlin plate can be successfully predicted by the combination of isogeometric analysis and Nitsche method.\n",
      "With the rapid growth of digital publishing, harvesting, managing, and analyzing scholarly information have become increasingly challenging. The term Big Scholarly Data is coined for the rapidly growing scholarly data, which contains information including millions of authors, papers, citations, figures, tables, as well as scholarly networks and digital libraries. Nowadays, various scholarly data can be easily accessed and powerful data analysis technologies are being developed, which enable us to look into science itself with a new perspective. In this paper, we examine the background and state of the art of big scholarly data. We first introduce the background of scholarly data management and relevant technologies. Second, we review data analysis methods, such as statistical analysis, social network analysis, and content analysis for dealing with big scholarly data. Finally, we look into representative research issues in this area, including scientific impact evaluation, academic recommendation, and expert finding. For each issue, the background, main challenges, and latest research are covered. These discussions aim to provide a comprehensive review of this emerging area. This survey paper concludes with a discussion of open issues and promising future directions.\n",
      "Abstract Different from traditional centralized control, one major challenge of distributed consensus tracking control lies in the constraint that the desired reference trajectory is only accessible by part of the subsystems. Currently, most existing schemes require the availability of partial knowledge of the reference trajectories to all of the subsystems or information exchange of local control inputs. In this paper, we investigate distributed adaptive consensus tracking control without such requirements for nonlinear high-order multi-agent systems subjected to mismatched unknown parameters and uncertain external disturbances. By introducing compensating terms in a smooth function form of consensus errors and certain positive integrable functions in each step of virtual control design, a new backstepping based distributed adaptive control protocol is proposed. An extra estimator is designed in each subsystem to handle the parametric uncertainties involved in its neighbors’ dynamics, which avoids information exchange of local neighborhood consensus errors among connected subsystems. It is shown that global uniform boundedness of all the closed-loop signals and asymptotically output consensus tracking can be achieved. Simulation results are provided to verify the effectiveness of our scheme.\n",
      "In this technical note, the problem of adaptive tracking control is investigated for a class of stochastic uncertain nonlinear systems in the presence of input saturation. To analyze the effect of input saturation, an auxiliary system is employed. With the help of backstepping technique, an adaptive stochastic tracking control approach is developed. Under the proposed adaptive tracking controller, the boundedness of all the signals in the closed-loop system is achieved almost surely. Moreover, distinct from most of the existing results, the ultimate tracking error can be bounded by an explicit function of design parameters and input saturation error (the error between the control input and saturated input) in the mean quartic sense. Finally, an example is given to show the effectiveness of the proposed scheme.\n",
      "The d-improper chromatic number \\(\\chi ^d(G)\\) of a graph G is the minimum number of colors to color G such that each color class induces a subgraph of maximum degree at most d. The d-improper choice number is the list-coloring version of this concept. A graph is called d-improperly chromatic-choosable if its d-improper choice number equals its d-improper chromatic number. As a generalization of a recently confirmed conjecture of Ohba that every graph G with \\(|V(G)| \\le 2\\chi (G)+1\\) is chromatic-choosable, Yan et al. proposed an improper coloring-based version of Ohba’s conjecture: every graph G with \\(|V(G)|\\le (d+2)\\chi ^d(G)+(d+1)\\) is d-improperly chromatic-choosable. In this paper, using graph theoretic and probabilistic methods we prove that the conjecture is true for \\(|V(G)| \\le (d+\\frac{3}{2})\\chi ^d(G)+\\frac{d}{2}\\). We also construct a family of graphs G with \\(|V(G)|=(d+3)\\chi ^d(G)+(d+3)\\) which are not d-improperly chromatic-choosable.\n",
      "Data mining has been an area of increasing interest. The association rule discovery problem in particular has been widely studied. However, there are still some unresolved problems. For example, research on mining patterns in the evolution of numerical attributes is still lacking. This is both a challenging problem and one with significant practical applications in business, science, and medicine. In this paper we present a temporal association rule model for evolving numerical attributes. Metrics for qualifying a temporal association rule include the familiar measures of support and strength used in traditional association rule mining and a new metric called density. The density metric not only gives us a way to extract the rules that best represent the data, but also provides an effective mechanism to prune the search space. An efficient algorithm is devised for mining temporal association rules, which utilizes all three thresholds (especially the strength) to prune the search space drastically. Moreover, the resulting rules are represented in a concise manner via rule sets to reduce the output size. Experimental results on real and synthetic data sets demonstrate the efficiency of our algorithm.\n",
      "Data mining has been an area of increasing interest. The association rule discovery problem in particular has been widely studied. However, there are still some unresolved problems. For example, research on mining patterns in the evolution of numerical attributes is still lacking. This is both a challenging problem and one with significant practical applications in business, science, and medicine. In this paper we present a temporal association rule model for evolving numerical attributes. Metrics for qualifying a temporal association rule include the familiar measures of support and strength used in traditional association rule mining and a new metric called density. The density metric not only gives us a way to extract the rules that best represent the data, but also provides an effective mechanism to prune the search space. An efficient algorithm is devised for mining temporal association rules, which utilizes all three thresholds (especially the strength) to prune the search space drastically. Moreover, the resulting rules are represented in a concise manner via rule sets to reduce the output size. Experimental results on real and synthetic data sets demonstrate the efficiency of our algorithm.\n",
      "When competing in eBay bidding, online games, or e-exams in embedded computing environments, people naturally face asynchronous starts from different computing devices, which is treated as a security risk of online contests. The security risks of online contests also include eavesdropping during data transmission without intended rights, and false starts by malicious competitors, which also means asynchrony in contests. Accordingly, online contests need security guarantees, especially on synchronization. In this article, for synchronic and secure starts in a contest, we update security requirements of confidentiality, anonymity, and synchrony, comparing the current work to our previous work. Based on the updated requirements, we propose a general framework for the Advanced Secure Synchronized Reading (ASSR) system, which can hold multiple contests simultaneously in the cloud. It is important to note that the system can ignore the impacts of heterogeneity among competitors. Considering the heterogeneity both on transmission and computing, we construct a novel Randomness-reused Identity Based Key Encapsulation Mechanism (RIBKEM) to support separable decapsulation, which can shorten both decryption delay and transmission delay with the best efforts. Finally, ASSR enhances synchronization achievement for contest starts with heterogeneous delays of competitors while satisfying other security requirements. As a complement, the analysis on the provable security of ASSR is given, as well as a further analysis on the achievement of synchronization.\n",
      "In recent years, cognition map techniques for human insights have already played a significant part in complex or ill-structured problem solving. There are increasing interests on computational methods rather than hand-drawing methods to build an cognition graph for insights generation. In this paper, a systematic approach called Temporal-IdeaGraph is proposed to build a directed cognition graph based on event sequences. Firstly, an algorithm of frequent sequence mining is employed to capture sequential patterns and a method is then designed to remove duplicate patterns. Secondly, relevant patterns are merged and visualized into a directed cognition graph. An algorithm is further proposed to identify bridge events and bridge patterns which would trigger human’s deeper insights for better decision making. Finally, two real case studies validate the effectiveness of proposed approach.\n",
      "AbstractThe electronic word-of-mouth (e-WOM) is one of the most important among all the factors affecting consumers’ behaviours. Opinions towards a product through online reviews will influence purchase decisions of other online consumers by changing their perceptions on the product quality. Furthermore, each product aspect may impact consumers’ intentions differently. Thus, sentiment analysis and econometric models are incorporated to examine the relationship between purchase intentions and aspect-opinion pairs, which enable the weight estimation for each product aspect. We first identify product aspects and reduce dimensions to extract aspect-opinion pairs. Next the information gain is calculated for each aspect through entropy theory. Based on sentiment polarity and sentiment strength, we formulate an econometric model by integrating the information gain to measure the aspect’s weight. In the experiment, we track 386 digital cameras on Amazon for 39 months, and results show that the aspect weight for d...\n",
      "This letter concerns low complexity receiver design for Wiener phase-noise channels, where the variance of the additive white Gaussian noise (AWGN) is unknown. By representing the system with a factor graph, both the variational message passing and the sum-product message passing are used to achieve efficient joint decoding, phase noise estimation, and AWGN precision (the reciprocal of AWGN variance) estimation. Compared with the state-of-the-art receiver, the proposed receiver is able to achieve the same performance with much lower complexity.\n",
      "Dissipativity of stochastic nonlinear systems with state-dependent switching is investigated in this brief. Switching instants are introduced reasonably, based on the evolution of the state trajectory, which are proved to be stopping times. They are the key to apply Ito’s formula and Dynkin’s formula in stochastic systems. Based on these stopping times, some sufficient conditions on dissipativity of switched stochastic systems are provided. Furthermore, the criteria on global asymptotic stability and input-to-state stability in probability are presented by using a common Lyapunov function technique and multiple Lyapunov functions techniques, respectively. Finally, a numerical example is given to illustrate the validity of our results.\n",
      "This paper proposes a novel occlusion detection method for urban true orthophoto generation. In this new method, occlusion detection is performed using a ghost image; this method is therefore considerably different from the traditional Z-buffer method, in which occlusion detection is performed during the generation of a true orthophoto (to avoid ghost image occurrence). In the proposed method, a model is first established that describes the relationship between each ghost image and the boundary of the corresponding building occlusion, and then an algorithm is applied to identify the occluded areas in the ghost images using the building displacements. This theory has not previously been applied in true orthophoto generation. The experimental results demonstrate that the method proposed in this paper is capable of effectively avoiding pseudo-occlusion detection, with a success rate of 99.2%, and offers improved occlusion detection accuracy compared with the traditional Z-buffer detection method. The advantage of this method is that it avoids the shortcoming of performing occlusion detection and true orthophoto generation simultaneously, which results in false visibility and false occlusions; instead, the proposed method detects occlusions from ghost images and therefore provides simple and effective true orthophoto generation.\n",
      "Image forgery is becoming a growing threat to information credibility. Among all kinds of image forgeries, photographic composites of human faces have very serious impacts. To combat this kind of forgery, some forensic methods propose to estimate the 3D lighting environments from different faces and investigate the consistency between them. Although they are very effective, existing 3D lighting-based forensic methods are limited by many simplifying assumptions about the surface reflection model, among which convexity and constant reflectance are two critical ones. In this paper, we propose an optimized 3D lighting estimation method by incorporating a more general surface reflection model. In this model, we relax the convexity and constant reflectance assumptions by taking the occlusion geometry and surface texture information into consideration. The proposed reflection model is more general and accurate; hence, it can achieve better lighting estimation accuracy and more reliable discrimination performance. Comprehensive experiments on both synthetic and real data sets validate the correctness and efficacy of the proposed method. Comparisons with two existing 3D lighting-based forensic methods also demonstrate the superiority of the proposed method for detecting face splicing.\n",
      "In this paper, the secure transmission strategy for a multi-input-single-output multi-eavesdropper system with coexistence of a secure user (Bob) and a normal user (NU) is investigated. The NU and Bob require normal and secure data transmissions, respectively, and thus, the stream for the NU can be exploited to confuse the eavesdroppers. To guarantee the security of Bob, artificial noise (AN) is also deliberately injected into the null space of Bob and NU. The power allocation among Bob, NU, and AN, as well as the wiretap code rates, is jointly optimized to maximize the effective secrecy throughput (EST), under the average throughput constraint of the NU and statistical channel state information (CSI) of eavesdroppers. Both non-adaptive and adaptive transmission schemes are proposed, based on the statistical and instantaneous CSI of the legitimate channels, respectively. An alternative optimization algorithm is proposed to obtain the optimal parameters. It is proved that the EST is a quasi-concave function of the secrecy rate and the power allocated to Bob, and for fixed wiretap code rates, the optimal power allocation is derived in a closed-form expression. Numerical results show that the EST increases with the increase in transmitting power and the number of transmit antennas, and decreases with the increasing throughput constraint of the NU. Improved EST can be achieved through injecting AN and concurrent transmission of Bob and NU.\n",
      "Massive multiple input multiple output (MIMO) is one of the promising technologies to address the growing capacity requirement in fifth-generation cellular systems. However, the massive MIMO deployed at base stations (BSs) or relay stations (RSs) brings new challenges on channel state information (CSI) feedback because of its large-scale complex channel matrix. In this paper, we propose a hybrid limited feedback with selective eigenvalue information (HLFSEI), which adopts the individual quantized feedback and the codebook-based feedback jointly. In HLFSEI, we feed back only selective eigenvalue elements for power allocation by individual quantized feedback and the precoding matrix by codebook-based feedback. Furthermore, we study the optimal feedback bit allocation of the aforementioned two feedback methods to minimize the throughput loss with the feedback-link capacity constraint. Specifically, the feedback bits are allocated according to the properties of the throughput loss in high- and low-signal-to-noise regimes. Both the BS–RS communications and the BS/RS–user-equipment communications in cellular systems are taken into consideration. Finally, we evaluate the performance of the proposed HLFSEI strategy by simulation and show its performance gain compared with conventional feedback strategies.\n",
      "Miniaturized and rapid blood coagulation assay technologies are critical in many clinical settings. In this paper, we present a ZnO film bulk acoustic resonator for the kinetic analysis of human blood coagulation. The resonator operated in thickness shear resonance mode at 1.4 GHz. When the resonator contacted the liquid environment, the viscous loading effect was considered as the additional resistance and inductance in the equivalent circuits, resulting in a linear relationship with a slope of approximately −217 kHz/cP between the liquid viscosity and the frequency of the resonator. The downshift of the resonant frequency and the viscosity change during the blood coagulation were correlated to monitor the coagulation process. The sigmoidal trend was observed in the frequency response for the blood samples activated by thromboplastin and calcium ions. The coagulation kinetics involving sequential phases of steady reaction, growth and saturation were revealed through the time-dependent frequency profiles. The enzymatic cascade time, the coagulation rate, the coagulation time and the clot degree were provided by fitting the time-frequency curves. The prothrombin times were compared with the results measured by a standard coagulometer and show a good correlation. Thanks to the excellent potential of integration, miniaturization and the availability of direct digital signals, the film bulk acoustic resonator has promising application for both clinical and personal use coagulation testing technologies.\n",
      "Popular distributed graph processing frameworks, such as Pregel and GraphLab, are based on the vertex-centric computation model, where users write their customized Compute function for each vertex to process the data iteratively. Vertices are evenly partitioned among the compute nodes, and the granularity of parallelism of the graph algorithm is normally tuned by adjusting the number of compute nodes. Vertex-centric model splits the computation into phases. Inside one specific phase, the computation proceeds as an embarrassingly parallel process, because no communication between compute nodes incurs. By default, current graph engine only handles one iteration of the algorithm in a phase. However, in this paper, we find that we can also tune the granularity of parallelism, by aggregating the computation of multiple iterations into one phase, which has a significant impact on the performance of the graph algorithm. In the ideal case, if all computations are handled in one phase, the whole algorithm turns into an embarrassingly parallel algorithm and the benefit of parallelism is maximized. Based on this observation, we propose two approaches, a function-based approach and a parameter-based approach, to automatically transform a Pregel algorithm into a new one with tunable granularity of parallelism. We study the cost of such transformation and the trade-off between the granularity of parallelism and the performance. We provide a new direction to tune the performance of parallel algorithms. Finally, the approaches are implemented in our graph processing system, N2, and we illustrate their performance using popular graph algorithms.\n",
      "Quantum communication has attracted much attention in recent years. Deterministic joint remote state preparation (DJRSP) is an important branch of quantum secure communication which could securely transmit a quantum state with 100% success probability. In this paper, we study DJRSP of an arbitrary two-qubit state in noisy environment. Taking a GHZ based DJRSP scheme of a two-qubit state as an example, we study how the scheme is influenced by all types of noise usually encountered in real-world implementations of quantum communication protocols, i.e., the bit-flip, phase-flip (phase-damping), depolarizing, and amplitude-damping noise. We demonstrate that there are four different output states in the amplitude-damping noise, while there is the same output state in each of the other three types of noise. The state-independent average fidelity is presented to measure the effect of noise, and it is shown that the depolarizing noise has the worst effect on the DJRSP scheme, while the amplitude-damping noise or the phase-flip has the slightest effect depending on the noise rate. Our results are also suitable for JRSP and RSP.\n",
      "Twitter has become one of largest social networks for users to broadcast burst topics. There have been many studies on how to detect burst topics. However, mining burst patterns in burst topics has not been solved by the existing works. In this paper, we investigate the problem of mining burst patterns of burst topic in Twitter. A burst topic user graph model is proposed, which can represent the topology structure of burst topic propagation across a large number of Twitter users. Based on the model, hierarchical clustering is applied to cluster burst topics and reveal burst patterns from the macro perspective. Frequent sub-graph mining is used to discover the information flow patterns of burst topic from the micro perspective. Experimental results show that several interesting burst patterns are discovered, which can reveal different burst topic clusters and frequent information flows of burst topic.\n",
      "This paper is concerned with the problem of average consensus control for multi-agent systems with linear and Lipschitz nonlinear dynamics under a switching topology. First, a proportional and derivative-like consensus algorithm for linear cases with a time delay is designed to address such a problem. By a system transformation, such a problem is converted to the stability problem of a switched delay system. The stability analysis is performed based on a proposed Lyapunov–Krasoversusii functional including a triple-integral term and sufficient conditions are obtained to guarantee the average consensus for multi-agent systems under arbitrary switching. Second, extensions to the Lipschitz nonlinear cases are further presented. Finally, numerical examples are given to illustrate the effectiveness of the results.\n",
      "Cooperative multicast has been demonstrated to achieve significant performance gain over the classic source-destination transmission paradigm by exploiting spatial diversity through the participation of multiple relay nodes. As a major technical challenge, the selection of relays for a multicast session has significant impact on the multicast performance. The challenge is even more pronounced when the number of channels are limited as the relay selection is in this context coupled with channel allocation. We establish an analytical framework for joint relay selection and channel allocation problem and develop a lexicographic max-min multicast relay selection scheme. Our design consists of two technical steps. 1) We consider the maximization of the minimal data rate. By decoupling relay selection and channel allocation, the problem is transformed to a max-min-max problem, which is difficult to solve. To make this problem tractable, we reformulate it as a convex optimization problem via relaxation and smoothing, and prove the asymptotic equivalence from a geometrical perspective. 2) We propose an adjustment algorithm based on the initial max-min solution, and prove that the proposed scheme achieves lexicographic optimality.\n",
      "It is laborious for researchers to find proper collaborators who can provide researching guidance besides collaborating. Beneficial Collaborators (BCs), researchers who have a high academic level and relevant topics, can genuinely help researchers to enrich their research. Though many efforts have made to develop collaborator recommendation, most of existing works have mainly focused on recommending most possible collaborators with no intention to recommend specifically the BCs. In this paper, we propose the Beneficial Collaborator Recommendation (BCR) model that considers the dynamic research interest of researcher's and academic level of collaborators to recommend the BCs. First, we run the LDA model on the abstract of researchers' publications in each year for topic clustering. Second, we fix generated topic distribution matrix by a time function to fit interest dynamic transformation. Third, we compute the similarity between the collaboration candidate's feature matrix and the target researcher. Finally, we combine the similarity and influence in collaborators network to fix rank score and mine the candidates with high academic level and academic social impact. BCR generates the topN BCs recommendation. Extensive experiments on a dataset with citation network demonstrate that BCR performs better in terms of precision, recall, F1 score and the recommendation quality compared to baseline methods.\n",
      "Super-resolution (SR) image reconstruction is a technique used to recover a high-resolution image using the cumulative information provided by several low-resolution images. With the help of SR techniques, satellite remotely sensed images can be combined to achieve a higher-resolution image, which is especially useful for a two- or three-line camera satellite, e.g., the ZY-3 high-resolution Three Line Camera (TLC) satellite. In this paper, we introduce the application of the SR reconstruction method, including motion estimation and the robust super-resolution technique, to ZY-3 TLC images. The results show that SR reconstruction can significantly improve both the resolution and image quality of ZY-3 TLC images.\n",
      "We design state observers for nonlinear networked control systems (NCS) implemented over FlexRay. FlexRay is a communication protocol used in the automotive industry, which has the feature to switch between two scheduling rules during its communication cycles. These switches induce technical difficulties when modeling, designing and analyzing observers for such systems compared to standard NCS. We present a solution based on the emulation approach. Given an observer in the absence of communication constraints, we implement it over the network and we provide sufficient conditions on the latter, to preserve the stability property of the observer. In particular, we provide explicit bounds on the maximal allowable transmission intervals, which adapt to the lengths of the segment associated to each scheduling rule. We assume that the plant dynamics and measurements are affected by noise and we guarantee an input-to-state stability property for the corresponding estimation error system. The overall system is modeled as a hybrid system and the analysis relies on the use of a novel hybrid Lyapunov function.\n",
      "In this paper, physical layer security in multi-antenna small-cell networks is investigated, where the multi-antenna base stations (BSs), cellular users, and eavesdroppers are all randomly distributed according to three independent Poisson point processes. To improve the secrecy performance, artificial noise (AN) aided transmission is adopted at each BS. Based on the stochastic geometry, we first derive the closed-form expressions of the connection and secrecy outage probabilities, and then comprehensively analyze the impact of different parameters through asymptotic analysis. It shows that in a low cell-load case, deploying more BSs will improve the connection and secrecy outage performance, and deploying more transmit antennas at each BS will only improve the connection outage performance. For a fixed-rate transmission, the condition under which AN becomes unnecessary is derived. We also derive a semi closed-form expression of the lower bound of the achievable average secrecy rate, which is numerically efficient to evaluate. Finally, we extend the study to a high cell-load case and adopt the zero-forcing beamforming scheme to support multi-user transmission. The connection and secrecy outage probabilities are also analyzed. Moreover, the optimal number of users maximizing the secrecy area spectral efficiency is discussed, and it is shown to be a fixed portion of the number of transmit antennas. Simulation results are presented to validate the theoretical analysis.\n",
      "This paper concerns the time-dependent shortest path problem, which is difficult to come up with global optimal solution by means of classical shortest path approaches such as Dijkstra, and pulse-coupled neural network (PCNN). In this study, we propose a time-delay neural network (TDNN) framework that comes with the globally optimal solution when solving the time-dependent shortest path problem. The underlying idea of TDNN comes from the following mechanism: the shortest path depends on the earliest auto-wave (from start node) that arrives at the destination node. In the design of TDNN, each node on a network is considered as a neuron, which comes in the form of two units: time-window unit and auto-wave unit. Time-window unit is used to generate auto-wave in each time window, while auto-wave unit is exploited here to update the state of auto-wave. Whether or not an auto-wave leaves a node (neuron) depends on the state of auto-wave. The evaluation of the performance of the proposed approach was carried out based on online public Cordeau instances and New York Road instances. The proposed TDNN was also compared with the quality of classical approaches such as Dijkstra and PCNN.\n",
      "Clustering is the process of grouping a set of objects into classes of  similar  objects. Although definitions of similarity vary from one clustering model to another, in most of these models the concept of similarity is based on distances, e.g., Euclidean distance or cosine distance. In other words, similar objects are required to have close values on at least a set of dimensions. In this paper, we explore a more general type of similarity. Under the pCluster model we proposed, two objects are similar if they exhibit a coherent pattern on a subset of dimensions. For instance, in DNA microarray analysis, the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very much alike. Discovery of such clusters of genes is essential in revealing significant connections in gene regulatory networks. E-commerce applications, such as collaborative filtering, can also benefit from the new model, which captures not only the closeness of values of certain leading indicators but also the closeness of (purchasing, browsing, etc.) patterns exhibited by the customers. Our paper introduces an effective algorithm to detect such clusters, and we perform tests on several real and synthetic data sets to show its effectiveness.\n",
      "Memory caches are being aggressively used in today's data-parallel systems such as Spark, Tez, and Piccolo. However, prevalent systems employ rather simple cache management policies--notably the Least Recently Used (LRU) policy--that are oblivious to the application semantics of data dependency, expressed as a directed acyclic graph (DAG). Without this knowledge, memory caching can at best be performed by \"guessing\" the future data access patterns based on historical information (e.g., the access recency and/or frequency), which frequently results in inefficient, erroneous caching with low hit ratio and a long response time. In this paper, we propose a novel cache replacement policy, Least Reference Count (LRC), which exploits the application-specific DAG information to optimize the cache management. LRC evicts the cached data blocks whose reference count is the smallest. The reference count is defined, for each data block, as the number of dependent child blocks that have not been computed yet. We demonstrate the efficacy of LRC through both empirical analysis and cluster deployments against popular benchmarking workloads. Our Spark implementation shows that, compared with LRU, LRC speeds up typical applications by 60%.\n",
      "Microarray image technology is a powerful tool for monitoring the expression of thousands of genes simultaneously. Each microarray experiment produces immense amounts of image data, and efficient storage and transmission requires compression that utilizes the microarray image's structure and unique analysis goals. We have developed a progressive compression scheme for microarray images which can be either lossy or lossless. Our scheme has a coded data structure that allows fast decoding and reprocessing of image subsets, and includes summary statistics and image segmentation information. Since visual fidelity is not the end goal for microarray images, we introduce a new measure of distortion for lossy compression: the sensitivity of microarray information extraction to compression loss. We find that a lossy compression ratio of 8:1 for cDNA microarrays minimally affects downstream processing. The average lossless compression ratio is 1.83:1 for cDNA images and 2.43:1 for inkjet images, comparable to state-of-the-art lossless schemas, yet with added flexibility and information.\n",
      "In this paper, we present several improvements on the conventional Active Shape Models (ASM) for face alignment. Despite the accuracy and robustness of the ASMs in the image alignment, its performance depends heavily on the initial parameters of the shape model, aswell as the local texture model for each landmark and the corresponding local matching strategy. In this work, to improve the ASMs for face alignment, several measures are taken. First, salient facial features, such as the eyes and the mouth, are localized based on a face detector. These salient features are then utilized to initialize the shape model and provide region constraints on the subsequent iterative shape searching. Secondly, we exploit the edge information to construct better local texture models for the landmarks on the face contour. The edge intensity at the contour landmark is used as a self-adaptive weight when calculating the Mahalanobis distance between the candidate profile and the reference one. Thirdly, to avoid their unreasonable shift from the pre-Iocalized salient features, landmarks around the salient features are adjusted before applying the global subs pace constraints. Experiments on a database containing 300 labeled face images show that the proposed method performs significantly better than traditional ASMs.\n",
      "Signals that represent information may be classified into two forms: numeric and symbolic. Symbolic signals are discrete-time sequences that at, any particular index, have a value that is a member of a finite set of symbols. Set membership defines the only mathematical structure that symbolic sequences satisfy. Consequently, symbolic signals cannot be directly processed with existing signal processing algorithms designed for signals having values that are elements of a field (numeric signals) or a group. Generalizing an approach due to Stoffer (see Biometrika, vol.85, p.201-213, 1998), we extend time-frequency and time-scale analysis techniques to symbolic signals and describe a general linear approach to developing processing algorithms for symbolic signals. We illustrate our techniques by considering spectral and wavelet analyses of DNA sequences.\n",
      "Constraints are essential for many sequential pattern mining applications. However, there is no systematic study on  constraint-based sequential pattern mining . In this paper, we investigate this issue and point out that the framework developed for constrained frequent-pattern mining does not fit our missions well. An extended framework is developed based on a  sequential pattern growth  methodology. Our study shows that constraints can be effectively and efficiently pushed deep into sequential pattern mining under this new framework. Moreover, this framework can be extended to constraint-based structured pattern mining as well.\n",
      "In this paper we focus on mining periodic patterns allowing some degree of imperfection in the form of random replacement from a perfect periodic pattern. Information gain was proposed to identify patterns with events of vastly different occurrence frequencies and adjust for deviation from a pattern. However, it does not involve a penalty if there exists some gap between pattern occurrences. In many applications, e.g., bioinformatics, it is important to identify subsequences that a pattern repeats perfectly (or near perfectly). As a solution, we extend the information gain measure to include a penalty for gaps between pattern occurrences. We call this measure generalized information gain. Furthermore, we need to find a subsequence S' such that for a pattern P, the generalized information gain of P in S' is high. This is particularly useful in locating repeats in DNA sequences. In this paper, we developed an effective mining algorithm, InfoMiner+, to simultaneously mine significant patterns and associated subsequences.\n",
      "Analyzing protein sequence data becomes increasingly important recently. Most previous work on this area has mainly focused on building classification models. In this paper we investigate in the problem of automatic clustering of unlabeled protein sequences. As a widely recognized technique in statistics and computer science, clustering has been proven very useful in detecting unknown object categories and revealing hidden correlations among objects. One difficulty, that prevents clustering from being performed directly on protein sequence is the lack of an effective similarity measure that can be computed efficiently. Therefore, we propose a novel model for protein sequence cluster by exploring significant statistical properties possessed by the sequences. The concept of imprecise probabilities are introduced to the original probabilistic suffix tree to monitor the convergence of the empirical measurement and to guide the clustering process. It is demonstrated that the proposed method can successfully discover meaningful families without the necessity of learning models of different families from pre-labeled \"training data\".\n",
      "Presents, XParent, an XML document management system built on top of RDBMS. It is based on an efficient, model-mapping-based approach that uses a fixed database schema to store any XML documents without assistance of DTD. The visual query interface of XParent provides both expressive power for professionals and user friendliness for naive users. The proposed multi-level query translation scheme makes it possible to develop a generic XML application that supports multiple XML query languages and mapping schemas.\n",
      "As the complexity of Grid applications increase, it becomes more important and urgent to provide grid workflow to construct and manage the applications. The grid workflow based on dynamic scheduling and performance evaluation is presented in this paper according to the standards of GCC and WFMC, which consists of user portal, resource management component, grid services management, performance management, grid workflow engine featured by dynamic scheduling. Then the prototype implemented on the Globus is introduced. Finally the experiment of genome sequencing based on shotgun algorithm is analyzed and the result is promising.\n",
      "Analyzing sequence data has become increasingly important recently in the area of biological sequences, text documents, Web access logs, etc. We investigate the problem of clustering sequences based on their sequential features. As a widely recognized technique, clustering has proven to be very useful in detecting unknown object categories and revealing hidden correlations among objects. One difficulty that prevents clustering from being performed extensively on sequence data (in categorical domain) is the lack of an effective yet efficient similarity measure. Therefore, we propose a novel model (CLUSEQ) for sequence cluster by exploring significant statistical properties possessed by the sequences. The conditional probability distribution (CPD) of the next symbol given a preceding segment is derived and used to characterize sequence behavior and to support the similarity measure. A variation of the suffix tree, namely probabilistic suffix tree, is employed to organize (the significant portion of) the CPD in a concise way. A novel algorithm is devised to efficiently discover clusters with high quality and is able to automatically adjust the number of clusters to its optimal range via a unique combination of successive new cluster generation and cluster consolidation. The performance of CLUSEQ has been demonstrated via extensive experiments on several real and synthetic sequence databases.\n",
      "The study on database technologies, or more generally, the technologies of data and information management, is an important and active research field. Recently, many exciting results have been reported. In this fast growing field, Chinese researchers play more and more active roles. Research papers from Chinese scholars, both in China and abroad, appear in prestigious academic forums.In this paper, we, nine young Chinese researchers working in the United States, present concise surveys and report our recent progress on the selected fields that we are working on. Although the paper covers only a small number of topics and the selection of the topics is far from balanced, we hope that such an effort would attract more and more researchers, especially those in China, to enter the frontiers of database research and promote collaborations. For the obvious reason, the authors are listed alphabetically, while the sections are arranged in the order of the author list.\n",
      "Microarray image technology is a powerful tool for monitoring the expression of thousands of genes simultaneously. Each microarray experiment produces immense amounts of image data, and efficient storage and transmission require compression that takes advantage of microarray image structure. In this paper we develop a compression scheme for microarray images which can be either lossless or lossy with successive refinements. Existing measures of distortion such as mean squared pixel-wise error and visual fidelity are not appropriate for microarray images. We introduce a new measure of distortion for lossy compression: the sensitivity of microarray information extraction to compression loss. Furthermore, our scheme has a coded data structure that allows fast decoding and reprocessing of image sub-blocks, and includes summary statistics and image segmentation information. The average lossless compression ratio is 1.83:1 for our cDNA test images and 2.43:1 for our inkjet test images, comparable or better than state-of-the-art lossless schemas, yet with additional structure and information. At an average lossy compression ratio of 8:1 for cDNA microarrays, we find that our scheme minimizes the effects of compression loss compared to other algorithms. We show that the variability in differential gene expression levels extracted from lossily vs. losslessly compressed microarray images is less than both the variability between different arrays and the variability between different extraction algorithms. In fact, lossy compression can improve the estimation of gene expression levels for cDNA images.\n",
      "Role-based access control has been a focal area for many researchers over the last decade. There have been a large number of models, and many rich specification languages. However there has been little attention paid to the way in which access control policy is stored persistently. This paper investigates policy storage form the perspective of access control to the policy itself, and of its distributed administration.\n",
      "The Web resource is a rich collection of the dynamic information, which is useful in various disciplines. There has also been much research work related to improving the quality of information searching in the Web. However, most of the work is still inadequate to satisfy a diversified demand from users. In this paper, we exploit the hyperlinks in the Web and propose a new approach called SFP in order to improve the quality of research results obtain from search engines. The SFP algorithm evolves from the frequent pattern mining technique, which is a common data mining technique for conventional databases. The essential idea of our approach is to mine the frequent structures of links from a given Web topology. By using the SFP algorithm, we extract the authoritative pages and communities from the complex Web topology. We demonstrate our approach by running several experiments and show that the performance and functionalities of using the SFP in managing search results are better than other known methods such as HITS.\n",
      "Active Shape Model (ASM) has been widely recognized as one of the best methods for image understanding. In this paper, we propose to enhance ASMs by introducing global texture constraints expressed by its reconstruction residual in the texture subspace. In the proposed method, each landmark is firstly matched by its local profile in its current neighborhood, and the overall configure of all the landmarks is re-shaped by the statistical shape constraint as in the ASMs. Then, the global texture is warped out from the original image according to the current shape model, and its reconstruction residual from the pre-trained texture subspace is further exploited to evaluate the fitting degree of the current shape model to the novel image. Also, the texture is exploited to predict and update the shape model parameters before we turn to the next iterative local matching for each landmark. Our experiments on the facial feature analysis have shown the effectiveness of the proposed method.\n",
      "Mining frequent patterns from datasets is one of the key success stories of data mining research. Currently, most of the works focus on independent data, such as the items in the marketing basket. However, the objects in the real world often have close relationship with each other. How to extract frequent patterns from these relations is the objective in this paper. We use graphs to model the relations, and select a simple type for analysis. Combining the graph theory and algorithms to generate frequent patterns, a new algorithm Topology, which can mine these graphs efficiently, has been proposed. We evaluate the performance of the algorithm by doing experiments with synthetic datasets and real data. The experimental results show that Topology can do the job well. At the end of this paper, the potential improvement is mentioned.\n",
      "In this paper, a new two-phase scheme for video similarity detection is proposed. For each video sequence, we extract two kinds of signatures with different granularities: coarse and fine. Coarse signature is based on the Pyramid Density Histogram (PDH) technique and fine signature is based on the Nearest Feature Trajectory (NFT) technique. In the first phase, most of unrelated video data are filtered out with respect to the similarity measure of the coarse signature. In the second phase, the query video example is compared with the results of the first phase according to the similarity measure of the fine signature. Different from the conventional nearest neighbor comparison, our NFT based similarity measurement method well incorporates the temporal order of video sequences. Experimental results show that our scheme achieves better quality results than the conventional approach.\n",
      "Over-segmentation could be relieved by adopting a divisive image segmentation model. This also requires the binary classification of whether a segmented region corresponds to a single semantic object. In this paper, we propose a model to address this classification problem, by detecting if a region contains both \"background\" and \"foreground\" regions. When \"background\" and \"foreground\" both present, the region is considered to have multiple objects, otherwise it corresponds to a single object. We implement the model based on certain image features of the region that effectively tell the difference between \"background\" and \"foreground\". Experiments show that our model can effectively perform the classification tasks.\n",
      "We develop a new approach for Web information discovery and filtering. Our system, called WID, allows the user to specify long-term information needs by means of various topic profile specifications. An entire example page or an index page can be accepted as input for the discovery. It makes use of a simulated annealing algorithm to automatically explore new Web pages. Simulated annealing algorithms possess some favorable properties to fulfill the discovery objectives. Information retrieval techniques are adopted to evaluate the content-based relevance of each page being explored. The hyperlink information, in addition to the textual context, is considered in the relevance score evaluation of a Web page. WID allows users to provide three forms of the relevance feedback model, namely, the positive page feedback, the negative page feedback, and the positive keyword feedback. The system is domain independent and does not rely on any prior knowledge or information about the Web content. Extensive experiments have been conducted to demonstrate the effectiveness of the discovery performance achieved by WID.\n",
      "Finding all the occurrences of a twig pattern specified by a selection predicate on multiple elements in an XML document is a core operation for efficient evaluation of XML queries. Holistic twig join algorithms were proposed recently as an optimal solution when the twig pattern only involves ancestor-descendant relationships. In this paper, we address the problem of efficient processing of holistic twig joins on all/partly indexed XML documents. In particular, we propose an algorithm that utilizes available indices on element sets. While it can be shown analytically that the proposed algorithm is as efficient as the existing state-of-the-art algorithms in terms of worst case I/O and CPU cost, experimental results on various datasets indicate that the proposed index-based algorithm performs significantly better than the existing ones, especially when binary structural joins in the twig pattern have varying join selectivities.\n",
      "Periodicy detection in time series data is a challenging problem of great importance in many applications. Most previous work focused on mining synchronous periodic patterns and did not recognize the misaligned presence of a pattern due to the intervention of random noise. In this paper, we propose a more flexible model of asynchronous periodic pattern that may be present only within a subsequence and whose occurrences may be shifted due to disturbance. Two parameters min/spl I.bar/rep and max/spl I.bar/dis are employed to specify the minimum number of repetitions that is required within each segment of nondisrupted pattern occurrences and the maximum allowed disturbance between any two successive valid segments. Upon satisfying these two requirements, the longest valid subsequence of a pattern is returned. A two-phase algorithm is devised to first generate potential periods by distance-based pruning followed by an iterative procedure to derive and validate candidate patterns and locate the longest valid subsequence. We also show that this algorithm cannot only provide linear time complexity with respect to the length of the sequence but also achieve space efficiency.\n",
      "The classic nonlinear filter performs well in impulse noise suppression and edge preserving. However, the classic nonlinear filtering is not good at reducing the mixture of Gaussian noise and impulse noise. In this paper, we investigate the nonlinear filtering techniques to eliminate the mixture of impulse noise and Gaussian noise. Based on fuzzy theory, we present a weighted average filter by making use of the fuzzy membership functions to optimize the weights of the filter. Computational results, which have been obtained from experiments for noise attenuation, indicate that the new algorithm is promising.\n",
      "In this paper, we focus on mining surprising periodic patterns in a sequence of events. In many applications, e.g., computational biology, an infrequent pattern is still considered very significant if its actual occurrence frequency exceeds the prior expectation by a large margin. The traditional metric, such as support, is not necessarily the ideal model to measure this kind of surprising patterns because it treats all patterns equally in the sense that every occurrence carries the same weight towards the assessment of the significance of a pattern regardless of the probability of occurrence. A more suitable measurement, information, is introduced to naturally value the degree of surprise of each occurrence of a pattern as a continuous and monotonically decreasing function of its probability of occurrence. This would allow patterns with vastly different occurrence probabilities to be handled seamlessly. As the accumulated degree of surprise of all repetitions of a pattern, the concept of information gain is proposed to measure the overall degree of surprise of the pattern within a data sequence. The bounded information gain property is identified to tackle the predicament caused by the violation of the downward closure property by the information gain measure and in turn provides an efficient solution to this problem. Furthermore, the user has a choice between specifying a minimum information gain threshold and choosing the number of surprising patterns wanted. Empirical tests demonstrate the efficiency and the usefulness of the proposed model.\n",
      "Intrusion detection is an important technique in the defense-in-depth network security framework and a hot topic in computer security in recent years. In this paper, a new intrusion detection method based on Principle Com- ponent Analysis (PCA) with low overhead and high efficiency is presented. System call data and command sequences data are used as information sources to validate the proposed method. The frequencies of individual system calls in a trace and individual commands in a data block are computed and then data col- umn vectors which represent the traces and blocks of the data are formed as data input. PCA is applied to reduce the high dimensional data vectors and distance between a vector and its projection onto the subspace reduced is used for anomaly detection. Experimental results show that the proposed method is promising in terms of detection accuracy, computational expense and imple- mentation for real-time intrusion detection.\n",
      "We present a new adaptive broadcast dissemination model to support flexible responses to client requests. Several features distinguish our model. First, client queries do not target individual documents, but specify the required information by attributes. Second, clients are satisfied by responses that are sufficiently close to the desired information. Finally, the server in our model solicits randomized feedback from clients to adapt its broadcast program to client needs. Our simulation results show that our model captures the interest patterns of clients more efficiently and more accurately and scales very well with the number of clients, while reducing overall client average waiting times.\n",
      "The rapid growth of wireless LAN (WLAN) deployments will bring about many novel mobile applications. Among them will be real-time multimedia streaming applications running on UDP, which may interfere with current data applications running on TCP. This paper is a first attempt to investigate how to ensure the performance of these two groups of applications when they co-exist over a WLAN. Toward this end, we have designed and implemented a UDP rate adaptation scheme called adaptive-buffer rate control (ABRC) for multimedia streaming over WLAN. ABRC has two distinguishing features compared with other schemes: 1) it can achieve arbitrary bandwidth allocations between UDP and TCP in the WLAN, as opposed Io previously proposed \"TCP friendly\" schemes, which can only achieve uniform bandwidth allocations; and 2) the majority of previously proposed flexible bandwidth-allocation schemes achieve arbitrary bandwidth allocations by prioritizing and scheduling packet transmissions within network equipment (i.e., within routers, base stations, etc.). In contrast, ABRC is an end-to-end application-layer solution that does not require changes to current WLAN products, making it more readily deployable over existing networks.\n",
      "The soundness of clustering in the analysis of gene expression profiles and gene function prediction is based on the hypothesis that genes with similar expression profiles may imply strong correlations with their functions in the biological activities. Gene ontology (GO) has become a well accepted standard in organizing gene function categories. Different gene function categories in GO can have very sophisticated relationships, such as 'part of' and 'overlapping'. Until now, no clustering algorithm can generate gene clusters within which the relationships can naturally reflect those of gene function categories in the GO hierarchy. The failure in resembling the relationships may reduce the confidence of clustering in gene function prediction. In this paper, we present a new clustering technique, smart hierarchical tendency preserving clustering (SMTP-clustering), based on a bicluster model, tendency preserving cluster (TP-Cluster). By directly incorporating gene ontology information into the clustering process, the SMTP-clustering algorithm yields a TP-cluster tree within which any subtree can be well mapped to a part of the GO hierarchy. Our experiments on yeast cell cycle data demonstrate that this method is efficient and effective in generating the biological relevant TP-clusters.\n",
      "Approximate range aggregate queries are one of the most frequent and useful kinds of queries for Decision Support Systems (DSS). Traditionally, sampling-based techniques have been proposed to tackle this problem. How- ever, its effectiveness will degrade when the underlying data distribution is skewed. Another approach based on the outlier management can limit the effect of data skew but fails to address other requirements of approximate range ag- gregate queries, such as error guarantees and query processing efficiency. In this paper, we present a technique that provide approximate answers to range aggregate queries on OLAP data cubes efficiently with theoretical error guaran- tees. Our basic idea is to build different data structures for outliers and the rest of the data. Experimental results verified the effectiveness of our proposed methods.\n",
      "Subspace clustering is one of the best approaches for discovering meaningful clusters in high dimensional space. One cluster in high dimensional space may be transcribed into multiple distinct maximal clusters by projecting onto different subspaces. A direct consequence of clustering independently in each subspace is an overwhelmingly large set of overlapping clusters which may be significantly similar. To reveal the true underlying clusters, we propose a similarity measurement of the overlapping clusters. We adopt the model of Gaussian tailed hyper-rectangles to capture the distribution of any subspace cluster. A set of experiments on a synthetic dataset demonstrates the effectiveness of our approach. Application to real gene expression data also reveals impressive meta-clusters expected by biologists.\n",
      "PGMS (P2P-based Grid Monitoring System) is a Grid monitoring system, which is based on peer-to-peer technology and GMA specification. Network performance measurement is a key component of monitoring system. Always, by all means, many monitoring systems avoid intrusiveness introduced by measurement and interferences between multiple sensors. In PGMS, the basic architecture and function cell is the peer, and some peers constitute a peer group, which is the management cell of PGMS and interconnected by peer-to-peer means. The architecture character of PGMS imposes special influences on network performance measurement. Some network measurement methodologies and rough estimate algorithm of bandwidth and delay are presented, which are based PGMS framework. Adaptive control of sensors and on-demand measurement methods are adopted to reduce intrusiveness and interferences, and increase scalability of PGMS.\n",
      "The advent of DNA microarray technologies has revolutionized the experimental study of gene expression. Clustering is the most popular approach of analyzing gene expression data and has indeed proven to be successful in many applications. Our work focuses on discovering a subset of genes which exhibit similar expression patterns along a subset of conditions in the gene expression matrix. Specifically, we are looking for the order preserving clusters (OP-cluster), in each of which a subset of genes induce a similar linear ordering along a subset of conditions. The pioneering work of the OPSM model, which enforces the strict order shared by the genes in a cluster, is included in our model as a special case. Our model is more robust than OPSM because similarly expressed conditions are allowed to form order equivalent groups and no restriction is placed on the order within a group. Guided by our model, we design and implement a deterministic algorithm, namely OPC-tree, to discover OP-clusters. Experimental study on two real datasets demonstrates the effectiveness of the algorithm in the application of tissue classification and cell cycle identification. In addition, a large percentage of OP-clusters exhibit significant enrichment of one or more function categories, which implies that OP-clusters indeed carry significant biological relevance.\n",
      "Mining frequent patterns from datasets is one of the key success of data mining research. Currently, most of the studies focus on the data sets in which the elements are independent, such as the items in the marketing basket. However, the objects in the real world often have close relationship with each other. How to extract frequent patterns from these relations is the objective of this paper. The authors use graphs to model the relations, and select a simple type for analysis. Combining the graph theory and algorithms to generate frequent patterns, a new algorithm called Topology, which can mine these graphs efficiently, has been proposed. The performance of the algorithm is evaluated by doing experiments with synthetic datasets and real data. The experimental results show that Topology can do the job well. At the end of this paper, the potential improvement is mentioned.\n",
      "An input-output decoupling and linearization problem for a class of nonlinear time-delay systems is considered. The method is based on differential geometry theory to transform the nonlinear state equation into a normal form that has the property of being input-output linear. Several sufficient conditions are discussed, under which there exist a state feedback controller and the nonlinear coordinate transformation that can realize the output variables being independent of time delays and decoupling from the inputs. The Isidori-Brunovsky canonical form of the original system is given as well, which help to simplify the theory analysis and bring convenience to the practical ends.\n",
      "Currently component-based software engineering is increasingly being adopted for software development. This approach relies on using reusable components as the building blocks for constructing software systems. As the growth in the popularity of Internet, component providers can publish the components easily on the Internet. Therefore, the major problem facing the component reusers is to find suitable components on the Internet. The solution to this problem lies in applying search engine technology. However, general-purpose search engines are inappropriate to search for software components. Some people developed the specific search engines for software components. But those engines have some limitations. This paper proposes a specific search engine for software components, which provides convenient support for component reusers to search for software components on the Internet.\n",
      "A new method for eyes localization is presented. By incorporating some eye geometrical constraints, our method includes such procedures as eyes image segmentation, eye candidates selection and eyeball detection. In order to locate the eyes more precisely, the valley map transformed from the grayscale face image using an arithmetical morphology method is successively binarized with a series of threshold values determined adaptively, and many possible eyeball candidates are extracted from them. The final position of the two eyes is obtained using a statistical method. Experimental results on AR and Yale face databases show that a locating rate of over 94% and an average locating disparity of below 2 pixels can be achieved\n",
      "Voice-over-IP (VoIP) is.an important application on the Internet. With the emergence of WLAN technology and its various advantages compared with the traditional wired LAN, it is fast becoming the \"last-mile\" of choice for the overall Internet infrastructure. This work considers the support of VoIP over 802.11b WLAN. We show that although the raw WLAN capacity can potentially support more than 500 VoIP sessions, various overheads bring this down to only 12 VoIP sessions when using GSM 6.10 codec. We propose a novel multiplexing scheme for VoIP which exploits multicasting over WLAN for the downlink VoIP traffic. This scheme can achieve nearly 100% improvement in system capacity. In addition, we present results showing that the delay and delay jitter introduced by the proposed scheme are small. We believe that the scheme can reduce the blocking probability of VoIP sessions in an enterprise WLAN significantly.\n",
      "In this paper, resource allocation for heterogeneous services is studied in multiuser orthogonal frequency division multiplexing (OFDM) systems. We propose a resource allocation algorithm, which is designed to improve the system throughput while satisfying the quality of service (QoS) requirements of both the real-time and nonreal-time services. In the proposed algorithm, the resources, composed of subcarriers and transmit power, are adaptively allocated to the users based on their service types and channel states over two sequential steps: (1) resource allocation for the real-time users that minimizes the resource usage required to satisfy the data rate requirement, and (2) resource allocation for the nonreal-time users that maximizes the system throughput using the remaining resource. The performance of the proposed resource allocation algorithm is evaluated in a frequency-selective fading channel, and compared with that of a simple resource allocation algorithm. Numerical results show that the proposed algorithm provides a significant throughput gain over the conventional algorithm.\n",
      "In this paper, we extend the traditional association rule problem by allowing a weight to be associated with each item in a transaction to reflect the interest/intensity of each item within the transaction. In turn, this provides us with an opportunity to associate a weight parameter with each item in a resulting association rule; we call them weighted association rules (WAR). One example of such a rule might be 80% of people buying more than three bottles of soda will also be likely to buy more than four packages of snack food, while a conventional association rule might just be 60% of people buying soda will be also be likely to buy snack food. Thus WARs cannot only improve the confidence of the rules, but also provide a mechanism to do more effective target marketing by identifying or segmenting customers based on their potential degree of loyalty or volume of purchases. Our approach mines WARs by first ignoring the weight and finding the frequent itemsets (via a traditional frequent itemset discovery algorithm), followed by introducing the weight during the rule generation. Specifically, the rule generation is achieved by partitioning the weight domain space of each frequent itemset into fine grids, and then identifying the popular regions within the domain space to derive WARs. This approach does not only support the batch mode mining, i.e., finding WARs for the dataset, but also supports the interactive mode, i.e., finding and refining WARs for a given (set) of frequent itemset(s).\n",
      "We present new algorithms for performing fast computation of several common database operations on commodity graphics processors. Specifically, we consider operations such as conjunctive selections, aggregations, and semi-linear queries, which are essential computational components of typical database, data warehousing, and data mining applications. While graphics processing units (GPUs) have been designed for fast display of geometric primitives, we utilize the inherent pipelining and parallelism, single instruction and multiple data (SIMD) capabilities, and vector processing functionality of GPUs, for evaluating boolean predicate combinations and semi-linear queries on attributes and executing database operations efficiently. Our algorithms take into account some of the limitations of the programming model of current GPUs and perform no data rearrangements. Our algorithms have been implemented on a programmable GPU (e.g. NVIDIA's GeForce FX 5900) and applied to databases consisting of up to a million records. We have compared their performance with an optimized implementation of CPU-based algorithms. Our experiments indicate that the graphics processor available on commodity computer systems is an effective co-processor for performing database operations.\n",
      "We consider the in-network computation of approximate \"big picture\" summaries in bandwidth-constrained sensor networks. First we review early work on computing the Haar wavelet decomposition as a User-Defined Aggregate in a sensor query engine. We argue that this technique can be significantly improved by choosing a function-specific network topology. We generalize this discussion to a loose definition of a 2-level optimization problem that maps from a function to what we call a  support graph  for the function, and from there to an aggregation tree that is chosen from possible subgraphs of the physical network connectivity. This work is frankly quite preliminary: we raise a number of questions but provide relatively few answers. The intent of the paper is to lay groundwork for discussion and further research.\n",
      "Workflow is referred to automatic business process, and current involving methods mainly depend on traditional centralized C/S infrastructure and email systems. As far as we know, the existing methods have following weaknesses in nature: 1) The abilities of workflow processing is difficult to scale up; 2) Workflow execution is not flexible; 3) Workflow transfer process is not reliable; 4) Handheld devices in mobile environments are seldom considered. However, peer-to-peer technology is characterized by decentralized control, large scale, and extreme dynamism, while agent technology can construct overlay network more easily and process workflow automatically. In this paper, we present a novel framework based on office-secretary model, which utilizes concept of peer-to-peer, agent and transaction mechanism. In particular, we believe that iOmS framework could easily support scalable, dynamic, reliable, and ubiquitous workflow applications.\n",
      "With the development of Internet, frequent pattern mining has been extended to more complex patterns like tree mining and graph mining. Such applications arise in complex domains like bioinformatics, web mining, etc. In this paper, we present a novel algorithm, named Chopper, to discover frequent subtrees from ordered labeled trees. An extensive performance study shows that the newly developed algorithm outperforms TreeMiner V, one of the fastest methods proposed previously, in mining large databases. At the end of this paper, the potential improvement of Chopper is mentioned.\n",
      "One fundamental challenge for mining recurring subgraphs from semi-structured data sets is the overwhelming abundance of such patterns. In large graph databases, the total number of frequent subgraphs can become too large to allow a full enumeration using reasonable computational resources. In this paper, we propose a new algorithm that mines only  maximal  frequent subgraphs, i.e. subgraphs that are not a part of any other frequent subgraphs. This may exponentially decrease the size of the output set in the best case; in our experiments on practical data sets, mining maximal frequent subgraphs reduces the total number of mined patterns by two to three orders of magnitude.Our method first mines all frequent trees from a general graph database and then reconstructs all maximal subgraphs from the mined trees. Using two chemical structure benchmarks and a set of synthetic graph data sets, we demonstrate that, in addition to decreasing the output size, our algorithm can achieve a five-fold speed up over the current state-of-the-art subgraph mining algorithms.\n",
      "In this paper, we propose a framework, called XAR-Miner, for mining ARs from XML documents efficiently. In XAR-Miner, raw data in the XML document are first preprocessed to transform to either an Indexed Content Tree (IX-tree) or Multi-relational databases (Multi-DB), depending on the size of XML document and memory constraint of the system, for efficient data selection and AR mining. Task-relevant concepts are generalized to produce generalized meta-patterns, based on which the large ARs that meet the support and confidence levels are generated.\n",
      "Privacy-preserving classification mining is one of the fast-growing sub-areas of data mining. How to perturb original data and then build a decision tree based on perturbed data is the key research challenge. By applying transition probability matrix this paper proposes a novel privacy-preserving classification mining algorithm which suits all data types, arbitrary probability distribution of original data, and perturbing all attributes (including label attribute). Experimental results demonstrate that decision tree built using this algorithm on perturbed data has comparable classifying accuracy to decision tree built using un-privacy-preserving algorithm on original data.\n",
      "Microarrays are one of the latest breakthroughs in experimental molecular biology, which provide a powerful tool by which the expression patterns of thousands of genes can be monitored simultaneously and are already producing huge amount of valuable data. The concept of bicluster was introduced by Cheng and Church1 to capture the coherence of a subset of genes and a subset of conditions. A set of heuristic algorithms were also designed to either find one bicluster or a set of biclusters, which consist of iterations of masking null values and discovered biclusters, coarse and fine node deletion, node addition, and the inclusion of inverted data. These heuristics inevitably suffer from some serious drawback. The masking of null values and discovered biclusters with random numbers may result in the phenomenon of random interference which in turn impacts the discovery of high quality biclusters. To address this issue and to further accelerate the biclustering process, we generalize the model of bicluster to incorporate null values and propose a probabilistic algorithm (FLOC) that can discover a set of k possibly overlapping biclusters simultaneously. Furthermore, this algorithm can easily be extended to support additional features that suit different requirements at virtually little cost. Experimental study on the yeast gene expression data2 shows that the FLOC algorithm can offer substantial improvements over the previously proposed algorithm.\n",
      "On the basis of sliding mode control, a new cascade sliding-mode controller (CSMC) for a class of large-scale underactuated systems is proposed. The large-scale underactuated systems include several subsystems. Firstly, two states are chosen to construct the first-layer sliding surface. Secondly, the first-layer sliding surface and one of the left states are used to construct the second-layer sliding surface. This process continues till the last-layer sliding surface is obtained. By theoretical analysis, the cascade sliding-mode controller is proved to be globally stable in the sense that all signals involved are bounded. The simulation results show the validity of this method.\n",
      "In dynamic peer networks, how to promote the performance of group key management without sacrificing the desired security is a critical and difficult problem. In this paper, a secure, efficient and distributed group key management scheme is presented and its security is proved. The scheme is based on hierarchical key tree and multi-party key agreement, and has the desired properties, suck as key independence and statelessness. The related analysis shows that the ternary key tree is most applicable to group key management, and the corresponding key management scheme is efficient in the computation cost, storage cost and feasibility.\n",
      "In order to realize automatic service composition in Manufacturing Grid (MG), a framework based on manufacturing domain-specific ontology (MGOnto) is proposed. MGOnto integrates three existing manufacturing ontologies, and represented in First Order Logic and Rules respectively. The framework consists of five core and three supporting services. It can reuse previously processed workflows in local repository and compose new workflows from services MG wide, by automatic service chaining which uses a MGOnto based backward recursive algorithm. A test bed is implemented and experiments on the example of an airfoil rib verify the feasibility of the framework.\n",
      "The growth of bioinformatics has resulted in datasets with new characteristics. The DNA sequences typically contain a large number of items. From them biologists assemble a whole genome of species based on frequent concatenate sequences, which ordinarily have hundreds of items. Such datasets pose a great challenge for existing frequent pattern discovery algorithms. Almost all of them are Apriori-like and so have an exponential dependence on the average sequence length. PrefixSpan is the most efficient algorithm, which presented the projection-based sequential pattern-growth approach. However it grows sequential patterns by exploring length-1 frequent patterns and so is not suitable for biological dataset with long frequent concatenate sequences. In this paper, we propose two novel algorithms, called MacosFSpan and MacosVSpan, to mine maximal frequent concatenate sequences. They are specially designed to handle datasets having long frequent concatenate sequences. Our performance study shows that MacosFSpan outperforms the traditional methods with length-1 sequences exploration and MacosVSpan is more efficient than Macos VSpan\n",
      "In this paper, we present techniques for a network forensics analysis mechanism that includes effective evidence presentation, manipulation and automated reasoning. We propose the evidence graph as a novel graph model to facilitate the presentation and manipulation of intrusion evidence. For automated evidence analysis, we develop a hierarchical reasoning framework that includes local reasoning and global reasoning. Local reasoning aims to infer the roles of suspicious hosts from local observations. Global reasoning aims to identify group of strongly correlated hosts in the attack and derive their relationships. By using the evidence graph model, we effectively integrate analyst feedback into the automated reasoning process. Experimental results demonstrate the potential and effectiveness of our proposed approaches\n",
      "The next generation resource monitoring system in distributed computing environments should not only accurately monitor the environment but also automatically reconfigure the environment. In this paper, we present INAPmon, a robust and scalable policy-based resource monitoring system in a multi- server network computer environment. It monitors the resource usage with sensors and aggregates the data in servers for reconfiguring the environment, which aims to guarantee load balance between servers and impartiality between users. This policy-based approach indeed provides a novel perspective for the design of future resource monitoring system.\n",
      "We demonstrate a data-driven approach for representing, compressing, and indexing human-motion databases. Our modeling approach is based on piecewise-linear components that are determined via a divisive clustering method. Selection of the appropriate linear model is determined automatically via a classifier using a subspace of the most significant, or principle features (markers). We show that, after offline training, our model can accurately estimate and classify human motions. We can also construct indexing structures for motion sequences according to their transition trajectories through these linear components. Our method not only provides indices for whole and/or partial motion sequences, but also serves as a compressed representation for the entire motion database. Our method also tends to be immune to temporal variations, and thus avoids the expense of time-warping.\n",
      "In many fields and applications, it is critical for users to make decisions through OLAP queries. How to promote accuracy and efficiency while answering multiple aggregate queries, e.g. COUNT, SUM, AVG, MAX, MIN and MEDIAN? It has been the urgent problem in the fields of OLAP and data summarization recently. There have been a few solutions such as MRA-Tree and GENHIST for it. However, they could only answer a certain aggregate query which was defined in a particular data cube with some limited applications. In this paper, we develop a novel framework ADenTS, i.e. Adaptive Density-based Tree Structure, to answer various types of aggregate queries within a single data cube. We represent the whole cube by building a coherent tree structure. Several techniques for approximation are also proposed. The experimental results show that our method outperforms others in effectiveness.\n",
      "This paper proposes a localized recursive estimation scheme for parameter estimation in wireless sensor networks. Given any parameter occurred at some location and time, a number of sensors recursively estimates the parameter by using their local measurements of the parameter that is attenuated with the distance between a sensor and the target location and corrupted by noises. Compared with centralized estimation schemes that transmit all measurements to a sink (or a fusion center), the recursive scheme needs only to transmit the final estimate to a sink. When a sink is faraway from the sensors and multihop communications have to be used, using localized recursive estimation can help to reduce energy consumption and reduce network traffic load. Furthermore, the most efficient sequence of sensors for estimation is defined and the necessary condition for such a sequence is determined. Some numerical examples are also provided. By using some typical industrial sensor parameter values, it is shown that recursive scheme consumes much less energy when the sink is three hops or more faraway from the local sensors.\n",
      "Currently, constraints are increasingly considered as a kind of means of user- or expert-control for filtering those unsatisfied and redundant patterns rapidly during the web mining process. Recent work has highlighted the importance of constraint-based mining paradigm in the context of frequent itemsets, sequences, and many other interesting patterns in large database. However, it is still not clear how to push various constraints systematically into graph mining process. In this paper, we categorize various graph-based constraints into several major classes and develop a framework CabGin (i.e.  C onstr a int- b ased  G raph M in ing) to push them into mining process by their categories. Non-monotonic aggregates like average also can be pushed into CabGin with minor revision. Experimental results show that CabGin can prunes a large search space effectively by pushing graph-based constraints into mining process.\n",
      "Web spider is a widely used approach to obtain information for search engines. As the size of the Web grows, it becomes a natural choice to parallelize the spider’s crawling process. This paper presents a parallel web spider model based on multi-agent system for cooperative information gathering. It uses the dynamic assignment mechanism to wipe off redundant web pages caused by parallelization. Experiments show that the parallel spider is effective to improve the information gathering performance within an acceptable interaction efficiency cost for controlling. This approach provides a novel perspective for the next generation advanced search engine.\n",
      "This paper describes a speaker detection system using cross-modal association methods. Four association approaches are designed using linear and nonlinear association models. Speaker detection experiments were conducted to compare the approaches\n",
      "In the framework of axiomatic fuzzy sets theory, we first study how to impersonally and automatically determine the membership functions for fuzzy sets according to original data and facts, and a new algorithmic framework of determining membership functions and their logic operations for fuzzy sets has been proposed. Then, we apply the proposed algorithmic framework to give a new clustering algorithm and show that the algorithm is feasible. A number of illustrative examples show that this approach offers a far more flexible and effective means for the intelligent systems in real-world applications. Compared with popular fuzzy clustering algorithms, such as c-means fuzzy algorithm and k-nearest-neighbor fuzzy algorithm, the new fuzzy clustering algorithm is more simple and understandable, the data types of the attributes can be various data types or subpreference relations, even descriptions of human intuition, and the distance function and the class number need not be given beforehand.\n",
      "Model-based clustering is one of the most important ways for time series data mining. However, the process of clustering may encounter several problems. In this paper, a novel clustering algorithm of time-series which incorporates recursive hidden Markov model(HMM) training is proposed. Our contributions are as follows: 1) We recursively train models and use these model information in the process agglomerative hierarchical clustering. 2) We built HMM of time-series clusters to describe clusters. To evaluate the effectiveness of the algorithm, several experiments are conducted on both synthetic data and real world data. The result shows that the proposed approach can achieve better performance in correctness rate than the traditional HMM-based clustering algorithm\n",
      "Fingerprint enhancement is a crucial step in automatic fingerprint recognition system, undiscriminating repeated filtering easily lead to false structure in low-quality regions of a fingerprint image. This paper presents a new adaptive enhancement algorithm that can automatically adjust the parameters of filters and the time of filtering according to the quality factors in different regions. In order to improve filtering efficiency, a template bank of 4-dimenstion scaleable array is also designed to quantize the filter and forms the basis of a new fused filter. Experimental results in eight low-quality images from FVC2004 data sets show that the proposed algorithm is higher 23.7% in Good Index (GI), and saves 54.06% time consumptions than traditional Gabor-based methods. Since the eight images are extremely bad, a little improvement is very meaningful.\n",
      "Frequent itemset mining is a popular and important first step in analyzing data sets across a broad range of applications. The traditional, \"exact\" approach for finding frequent itemsets requires that every item in the itemset occurs in each supporting transaction. However, real data is typically subject to noise, and in the presence of such noise, traditional itemset mining may fail to detect relevant itemsets, particularly those large itemsets that are more vulnerable to noise. In this paper we propose approximate frequent itemsets (AFI), as a noise-tolerant itemset model. In addition to the usual requirement for sufficiently many supporting transactions, the AFI model places constraints on the fraction of errors permitted in each item column and the fraction of errors permitted in a supporting transaction. Taken together, these constraints winnow out the approximate itemsets that exhibit systematic errors. In the context of a simple noise model, we demonstrate that AFI is better at recovering underlying data patterns, while identifying fewer spurious patterns than either the exact frequent itemset approach or the existing error tolerant itemset approach of Yang et al.\n",
      "In this paper, we will propose a novel outlier mining algorithm, called Grid-ODF, that takes into account both the local and global perspectives of outliers for effective detection. The notion ofOutlying Degree Factor(ODF), that reflects the factors of both the density and distance, is introduced to rank outliers. A grid structure partitioning the data space is employed to enable Grid-ODF to be implemented efficiently. Experimental results show that Grid-ODF outperforms existing outlier detection algorithms such as LOF and KNN-distance in terms of effectiveness and efficiency.\n",
      "This paper suggests a new diffusion method, which based on modified coherence diffusion for the enhancement of ocular fundus images (OFI) and parallel AOS scheme is applied to speed algorithm, which is faster than usual approach and shows good performance. A structure tensor integrating the second-order directional differential information is applied to analyze weak edges, narrow peak, and vessels structures of OFI in diffusion. The structure tensor and the classical one as complementary descriptor are used to build the diffusion tensor. The several experiment results are provided and suggest that it is a robust method to prepare image for intelligent diagnosis and instruction for treatment of ocular diseases. The modified diffusion for the enhancement of OFI can preserve important oriented patterns, including strong edges and weak structures.\n",
      "The information in many applications can be naturally represented as graph-structured XML document. Structural query on graph structured XML document matches the subgraph of graph structured XML document on some given schema. The query processing of graph-structured XML document brings new challenges.#R##N##R##N#In this paper, for the processing of subgraph query, we design a subgraph join algorithm based on reachability coding. Using efficient data structure, subgraph join algorithm can process subgraph query with various structures efficiently.\n",
      "Mining frequent structural patterns from graph databases is an important research problem with broad applications. Recently, we developed an effective index structure,  ADI,  and efficient algorithms for mining frequent patterns from large, disk-based graph databases [5], as well as constraint-based mining techniques. The techniques have been integrated into a research prototype system---  GraphMiner.  In this paper, we describe a demo of  GraphMiner  which showcases the technical details of the index structure and the mining algorithms including their efficient implementation, the mining performance and the comparison with some state-of-the-art methods, the constraint-based graph-pattern mining techniques and the procedure of constrained graph mining, as well as mining real data sets in novel applications.\n",
      "Most existing text classification methods (and text mining methods at large) are based on representing the documents using the traditional vector space model. We argue that important information, such as the relationship among words, is lost. We propose a term graph model to represent not only the content of a document but also the relationship among the keywords. We demonstrate that the new model enables us to define new similarity functions, such as considering rank correlation based on PageRank-style algorithms, for the classification purpose. Our preliminary results show promising results of our new model.\n",
      "There is some private and sensitive data in database, which need to be protected from attacking. In order to reinforce the security of data, an effective mechanism, cryptographic support has been widely used. However, we must make a tradeoff between the performance and the security because encryption and decryption greatly degrade the query performance. To solve such a problem, a novel approach is proposed in this paper that can quickly execute SQL query on the encrypted data. For character data, it not only encrypts them, but also turns the character data into characteristic values via a characteristic function and stores them as additional fields. For numerical data, it not only encrypts them, but also creates its B+ tree index before the encryption in order to keep the ordering of each record in the index. Furthermore, we give the algorithms of querying the encrypted data based on the storage models. Results of sets of experiments validate the functionality and usability of our approach\n",
      "We consider the problem of efficiently computing the skyline against the most recent N elements in a data stream seen so far. Specifically, we study the n-of-N skyline queries; that is, computing the skyline for the most recent n (/spl forall/n/spl les/N) elements. Firstly, we developed an effective pruning technique to minimize the number of elements to be kept. It can be shown that on average storing only O(log/sup d/ N) elements from the most recent N elements is sufficient to support the precise computation of all n-of-N skyline queries in a d-dimension space if the data distribution on each dimension is independent. Then, a novel encoding scheme is proposed, together with efficient update techniques, for the stored elements, so that computing an n-of-N skyline query in a d-dimension space takes O(log N+s) time that is reduced to O(d log log N+s) if the data distribution is independent, where s is the number of skyline points. Thirdly, a novel trigger based technique is provided to process continuous n-of-N skyline queries with O(/spl delta/) time to update the current result per new data element and O(log s) time to update the trigger list per result change, where /spl delta/ is the number of element changes from the current result to the new result. Finally, we extend our techniques to computing the skyline against an arbitrary window in the most recent N element. Besides theoretical performance guarantees, our extensive experiments demonstrated that the new techniques can support on-line skyline query computation over very rapid data streams.\n",
      "Frequent pattern mining is an important data-mining problem with broad applications. Although there are many in-depth studies on efficient frequent pattern mining algorithms and constraint pushing techniques, the effectiveness of frequent pattern mining remains a serious concern: It is non-trivial and often tricky to specify appropriate support thresholds and proper constraints. In this paper, we propose a novel theme of preference-based frequent pattern mining. A user simply can specify a preference instead of setting detailed parameters in constraints. We identify the problem of preference-based frequent pattern mining and formulate the preferences for mining. We develop an efficient framework to mine frequent patterns with preferences. Interestingly, many preferences can be pushed deep into the mining by properly employing the existing efficient frequent pattern mining techniques. We conduct an extensive performance study to examine our method. The results indicate that preference-based frequent pattern mining is effective and efficient. Furthermore, we extend our discussion from pattern-based frequent pattern mining to preference-based data mining in principle and draw a general framework.\n",
      "Abstract   A family of subsets of [ n ] is  positive linear combination free  if the characteristic vector of neither member is the positive linear combination of the characteristic vectors of some other ones. We construct a positive linear combination free family which contains   (  1  −  o  (  1  )  )   2  n    subsets of [ n ] and we give tight bounds on the   o  (  1  )   2  n    term. The problem was posed by Ahlswede and Khachatrian [R. Ahlswede and L. Khachatrian, Cone dependence – a basic combinatorial concept, Preprint 00-117, Diskrete Strukturen in der Mathematik SFB 343, Universitat Bielefeld, 2000] and the result has geometric consequences.\n",
      "Though research on the Semantic Web has progressed at a steady pace, its promise has yet to be realized. One major difficulty is that, by its very nature, the Semantic Web is a large, uncensored system to which anyone may contribute, especially when using a P2P-based multi-agent infrastructure for knowledge sharing. This raises the question of how much credence to give each information source. We cannot expect each user to know the trustworthiness of each source. We tackle this problem by employing a reputation mechanism, which offer a viable solution to encouraging trustworthy behavior in Semantic Web. In addition, we introduce a reputation multi-agent system, SemTrust, which enable Semantic Web to utilize reputation mechanism based on semantic similarity between agents. Our experiments show that the system with SemTrust outperforms the system without it and our approach is more robust with security under conditions where existing malicious agents. We hope that these methods will help move the Semantic Web closer to fulfilling its promise by using reputation-based multi-agent technology.\n",
      "A cell-based all-digital PWCL is presented in this paper. To improve design effort as well as facilitate system-level integration, the new design can be developed in hardware description language (HDL) and implemented with standard-cell libraries, therefore, easily portable between technologies. In addition, a high-resolution architecture is designed to enhance pulsewidth precision. For different requirements of applications, the characteristic of scalable modulating range allows hardware decision in early stage. The proposed methodology has been proven at UMC 0.18/spl mu/m CMOS technology. When operated at 350 MHz, the pulse width acquisition ranges from 10% to 85% with 0.9% steps.\n",
      "Predicting domains of proteins is an important and challenging problem in computational biology because of its significant role in understanding the complexity of proteomes. Although many template-based prediction servers have been developed, ab initio methods should be designed and further improved to be the complementarity of the template-based methods. In this paper, we present a novel domain prediction system KemaDom by ensembling three kernel machines with the local context information among neighboring amino acids. KemaDom, an alternative ab initio predictor, can achieve high performance in predicting the number of domains in proteins. It is freely accessible at http://www.iipl.fudan.edu.cn/lschen/kemadom.htm and http://www.iipl.fudan.edu.cn/~lschen/kemadom.htm.\n",
      "The knowledge discovered by data mining may contain sensitive information, which may cause potential threats towards privacy and security. In this paper, we address the problem of better preserving private knowledge by proposing an Item-based Pattern Sanitization to prevent the disclosure of private patterns. We also present two strategies to generate a safe and shareable pattern set for preserving private knowledge in frequent pattern mining.\n",
      "As the popularity of XML increases, the need for querying collections of XML data from various systems becomes imperative. Proposed by W3C, XQuery is becoming a standard for querying such systems. However, the complexity of XQuery prevents its usage by broad audience. This paper proposes a visual XQuery specification language called VXQ to address this issue. By intuitive abstractions of XML and XQuery, the proposed system can generate XQueries for users that have little knowledge about the language. We show that our visual language is more expressive than previous proposals. Finally, we extend our proposed visual XQuery to support query rewriting and optimization for multiple XQuery systems.\n",
      "A graph G is said to be determined by its spectrum (DS for short), if any graph having the same spectrum as G is necessarily isomorphic to G. One important topic in the theory of graph spectra is how to determine whether a graph is DS or not. The previous techniques used to prove a graph to be DS heavily rely on some special properties of the spectrum of the given graph. They cannot be applied to general graphs. In this paper, we propose a new method for determining whether a family of graphs (which have no special properties) are DS with respect to their generalized spectra. The method is obtained by employing some arithmetic properties of a certain matrix associated with a graph. Numerical examples are further given to illustrate the effectiveness of the proposed method.\n",
      "Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language fluency. Syntactic approaches seek to remedy these problems. In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words. Second, we propose probability estimates and a training procedure for weighting these rules. We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules.\n",
      "Ontologies represent data relationships as hierarchies of possibly overlapping classes. Ontologies are closely related to clustering hierarchies, and in this article we explore this relationship in depth. In particular, we examine the space of ontologies that can be generated by pairwise dissimilarity matrices. We demonstrate that classical clustering algorithms, which take dissimilarity matrices as inputs, do not incorporate all available information. In fact, only special types of dissimilarity matrices can be exactly preserved by previous clustering methods. We model ontologies as a partially ordered set (poset) over the subset relation. In this paper, we propose a new clustering algorithm, that generates a partially ordered set of clusters from a dissimilarity matrix.\n",
      "Chinese remainder theorem based RSA (CRT-RSA) digital signature has important applications in smart cards. Previous CRT-RSA algorithms such as CRT-2 and BOS are susceptible to practical hardware fault attacks. In this paper, a new CRT-RSA algorithm with countermeasures to hardware fault attacks is proposed. To our knowledge, the proposed algorithm is the first that can resist what we call the single-fault adversarial attacks. The proposed algorithm first computes the signature using CRT in a secret algebraic setting, then a set of fault-detection variables are computed to detect possible faults. Lastly, the signature is mapped from the secret algebraic setting to the intended setting. By using a random number and the fault detection variables, every step of the algorithm is protected from hardware faults. The output of the algorithm will be fully randomized in case of faults. The CRT's speed advantages are also maintained. The proposed algorithm is approximately two times as fast as the direct form RSA for the two-prime case, and about four times as fast for the three-prime case\n",
      "We consider the problem of continuously maintaining order sketches over data streams with a relative rank error guarantee ∊. Novel space-efficient and one-scan randomised techniques are developed. Our first randomised algorithm can guarantee such a relative error precision ∊ with confidence 1 - \\delta using O( 1\\_ \\in \\frac{1} {2}2 log 1d log ∊^2N) space, where N is the number of data elements seen so far in a data stream. Then, a new one-scan space compression technique is developed. Combined with the first randomised algorithm, the one-scan space compression technique yields another one-scan randomised algorithm that guarantees the space requirement is O( 1\\frac{1} { \\in } log(1\\frac{1}{ \\in } log 1\\begin{gathered} \\frac{1}{\\delta } \\hfill \\\\ \\hfill \\\\ \\end{gathered} )\\frac{{\\log ^{2 + \\alpha } \\in N}} {{1 - 1/2^\\alpha }} (for\\alpha \\gt 0) on average while the worst case space remains O( \\frac{1}{{ \\in ^2 }}\\log \\frac{1} {\\delta }\\log \\in ^2 N). These results are immediately applicable to approximately computing quantiles over data streams with a relative error guarantee \\in and significantly improve the previous best space bound O( \\frac{1} {{ \\in ^3 }}\\log \\frac{1}{\\delta }\\log N). Our extensive experiment results demonstrate that both techniques can support an on-line computation against high speed data streams.\n",
      "In this paper we propose the new paradigm of applying diffusion and graph spectral methods for network forensic analysis. Based on an evidence graph model built from collected evidence, graph spectral methods show potential in identifying key components and patterns of attack by extracting important graph structures. We also present the novel view that the propagation of suspicion in an attack scene could be modelled in analogy with heat diffusion in physics systems. In this paradigm, the evidence graph becomes the basis for a physical construct, which derives its properties such as conductivity and heat generation from evidence features. We argue that diffusion and graph spectral methods not only provide a mathematically well grounded approach to network forensic analysis, but also open up the opportunity for applying structured parameter refinement and high performance computation methods to forensic analysis field.\n",
      "In this paper, a stable robust adaptive control approach is presented for a class of unknown nonlinear systems in the strict-feedback form with disturbances. The key assumption is that neural network approximation errors and external disturbances satisfy certain bounding conditions. By combining neural network technique with backstepping method and introducing a special type of Lyapunov functions, the controller singularity problem is avoided perfectly. As the estimates of unknown neural network approximation error bound and external disturbances bound are adjusted adaptively, the robustness of the closed-loop system is improved and the application scope of nonlinear systems is extended. The overall neural network control systems can guarantee that all the signals of the closed-loop system are uniformly ultimately bounded and the tracking error converges to a small neighborhood of zero by suitably choosing the design parameters. The feasibility of the control approach is demonstrated through simulation results.\n",
      "We study synchronization conditions for distributed dynamic networks with different types of leaders. The role of a \"power\" leader specifying a desired global state trajectory through local interactions has long been recognized and modeled. This note introduces the complementary notion of a \"knowledge\" leader holding information on the target dynamics, which is propagated to the entire network through local adaptation mechanisms. Different types of leaders can coexist in the same network. For instance, in a network of locally connected oscillators, the power leader may set the global phase while the knowledge leader may set the global frequency and the global amplitude. Knowledge-based leader-followers networks have many analogs in biology, e.g., in evolutionary processes and disease propagation.\n",
      "Microarray technology is a powerful tool for geneticists to monitor interactions among tens of thousands of genes simultaneously. There has been extensive research on coherent subspace clustering of gene expressions measured under consistent experimental settings. However, these methods assume that all experiments are run using the same batch of microarray chips with similar characteristics of noise. Algorithms developed under this assumption may not be applicable for analyzing data collected from heterogeneous settings, where the set of genes being monitored may be different and expression levels may be not directly comparable even for the same gene. In this paper, we propose a model, F-cluster, for mining subspace coherent patterns from heterogeneous gene expression data. We compare our model with previously proposed models. We analyze the search space of the problem and give a naive solution for it.\n",
      "In this paper, various models of carbon nanotube (CNT) and NiSi nanowire are established and used to analyze their performance as future on-chip interconnections. These theoretical studies and simulation results lead to important new findings: NiSi nanowire has superior properties in terms of effective current density and propagation speed compared to Cu and CNT. These properties make NiSi nanowire the most promising candidate for future on-chip interconnections. Bundle CNT also shows promise for long interconnect applications.\n",
      "Privacy becomes a more and more serious concern in applications involving microdata. Recently, efficient anonymization has attracted much research work. Most of the previous methods use global recoding, which maps the domains of the quasi-identifier attributes to generalized or changed values. However, global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data. Moreover, anonymized data is often used for analysis. As well accepted in many analytical applications, different attributes in a data set may have different utility in the analysis. The utility of attributes has not been considered in the previous methods.   In this paper, we study the problem of  utility-based anonymization.  First, we propose a simple framework to specify utility of attributes. The framework covers both numeric and categorical data. Second, we develop two simple yet efficient heuristic local recoding methods for utility-based anonymization. Our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy. Furthermore, our utility-based method can boost the quality of analysis using the anonymized data.\n",
      "Mining outliers in database is to find exceptional objects that deviate from the rest of the data set. Besides classical outlier analysis algorithms, recent studies have focused on mining local outliers, i.e., the outliers that have density distribution significantly different from their neighborhood. The estimation of density distribution at the location of an object has so far been based on the density distribution of its k-nearest neighbors [2,11]. However, when outliers are in the location where the density distributions in the neighborhood are significantly different, for example, in the case of objects from a sparse cluster close to a denser cluster, this may result in wrong estimation. To avoid this problem, here we propose a simple but effective measure on local outliers based on a symmetric neighborhood relationship. The proposed measure considers both neighbors and reverse neighbors of an object when estimating its density distribution. As a result, outliers so discovered are more meaningful. To compute such local outliers efficiently, several mining algorithms are developed that detects top-n outliers based on our definition. A comprehensive performance evaluation and analysis shows that our methods are not only efficient in the computation but also more effective in ranking outliers.\n",
      "According to the characteristics of retinal fundus images, one novel method of registration based on phase-correlation was proposed in this paper. This method takes advantage of optic disc to match two images coarsely. Meanwhile edge detection was applied to extract the edge of abundant retinal veins and arteries in order to raise the precision of registration. This method also overcomes the gray-sensitive deficiency in phase-correlation method. And the experiment results show that the method not only enhances the precision of registration but also reduces time complexity\n",
      "We study stability of interacting nonlinear systems with time-delayed communications, using contraction theory and a simplified wave variable design inspired by robotic teleoperation. We show that contraction is preserved through specific time-delayed feedback communications, and that this property is independent of the values of the delays. The approach is then applied to group cooperation with linear protocols, where it is shown that synchronization can be made robust to arbitrary time delays.\n",
      "To efficiently find global patterns from a multi-database, information in each local database must first be mined and summarized at the local level. Then only the summarized information is forwarded to the global mining process. However, conventional sequential pattern mining methods based on support cannot summarize the local information and is ineffective for global pattern mining from multiple data sources. In this paper, we present an alternative local mining approach for finding sequential patterns in the local databases of a multi-database. We propose the theme of approximate sequential pattern mining roughly defined as identifying patterns approximately shared by many sequences. Approximate sequential patterns can effectively summerize and represent the local databases by identifying the underlying trends in the data. We present a novel algorithm, ApproxMAP, to mine approximate sequential patterns, called consensus patterns, from large sequence databases in two steps. First, sequences are clustered by similarity. Then, consensus patterns are mined directly from each cluster through multiple alignment. We conduct an extensive and systematic performance study over synthetic and real data. The results demonstrate that ApproxMAP is effective and scalable in mining large sequences databases with long patterns. Hence, ApproxMAP can efficiently summarize a local database and reduce the cost for global mining. Furthremore, we present an elegant and uniform model to identify both high vote sequential patterns and exceptional sequential patterns from the collection of these consensus patterns from each local databases.\n",
      "Supporting frequent updates is a key challenge in moving object indexing. Most of the existing work regards the update as an individual process for each object, and a large number of separate updates are issued respectively in update-intensive environments. In this paper, we propose the bulkloading updates for moving objects (BLU). Based on a common framework, we propose three bulkloading schemes of different spatial biases. By grouping the objects with near positions, BLU prefetches the nodes accessed on the shared update path and combines multiple disk accesses to the same node into one, which avoids I/O overhead for objects within the same group. We also propose a novel MBR-driven flushing algorithm, which utilizes the dynamic spatial correlation and improves the buffer hit ratio. The theoretical analysis and experimental evaluation demonstrate that BLU achieves the good update performance and does not affect the query performance.\n",
      "An avalanche of data available in the stream form is overstretching our data analyzing ability. In this paper, we propose a novel load shedding method that enables fast and accurate stream data classification. We transform input data so that its class information concentrates on a few features, and we introduce a progressive classifier that makes prediction with partial input. We take advantage of stream data's temporal locality . for example, readings from a temperature sensor usually do not change dramatically over a short period of time . for load shedding. We first show that temporal locality of the original data is preserved by our transform, then we utilize positive and negative knowledge about the data (which is of much smaller size than the data itself) for classification. We employ both analytical and empirical analysis to demonstrate the advantage of our approach.\n",
      "This paper studies new spike-based models for winner-take-all computation and coincidence detection. In both cases, fast convergence is achieved independent of initial conditions, and network complexity is linear in the number of inputs. Fully distributed versions can be modelled based on groups of interneurons connected through electrical synapses.\n",
      "Recent Advances in Distributed Source Coding (DSC) motivate the mission-driven wireless sensor network application that relates to compression of multiple correlated sensor outputs, such as real time target tracking and environment monitoring. The characteristic of multiple rates in DSC applications requires the associated sensor network to support Rate-Oriented Routing (ROR) in order to extend the network life time. In this paper, we propose a novel optimal Rate-Oriented Routing based on the Distributed Energy Usage Scheduling (DEUS) for mission-driven DSC applications. The ROR adopts a new method called rate assignment instead of the traditional rate adaptation method in wireless networks to extend the network life time. The maximum network lifetime is achieved by constructing multiple rate-oriented paths between the source and sink, which balance the loads and lead to energy-efficient transmissions to meet the DSC application needs. Our simulation has shown that the Rate-Oriented Routing can extend the network life time and save energy significantly under the DSC application requirements\n",
      "Traditional overlay network simulators provide accurate low-level models of the network hardware and protocols but are but none of them deal with the problem of trust in the large scale overlay networks. We tackle this problem by employing a trust overlay simulator, which offer a viable solution to simulate trustworthy behavior in overlay networks. With this simulator, we can exam varies kinds of the trust and reputation mechanisms in the overlay environment. We hope that this simulator will help move the overlay network closer to fulfilling its promise by developing and testing trust and reputation-based protocols on it.\n",
      "We propose a distributed multiresolution representation of sensor network data so that large-scale summaries are readily available by querying a small fraction of sensor nodes, anywhere in the network, and small-scale details are available by querying a larger number of sensors, locally in the region of interest. A global querier (such as a mobile collector or unmanned aerial vehicle) can obtain a lossy to lossless representation of the network data, according to the desired resolution. A local querier (such as a sensor node) can also obtain either large-scale trends or local details, by querying its immediate neighborhood. We want the encoding to be robust to arbitrary, even time-varying, wireless communication connectivity graphs. Thus we want to avoid cluster heads or deterministic hierarchies that are not robust to single points of failure. We propose a randomized encoding which enables both robustness, and distributed computation that does not require long distance coordination or awareness of network connectivity at individual sensors. Our distributed encoding algorithm operates on local neighborhoods of the communication graph.\n",
      "One major challenge in the content-based image retrieval (CBIR) and computer vision research is to bridge the so-called \"semantic gap\" between low-level visual features and high-level semantic concepts, that is, extracting semantic concepts from a large database of images effectively. In this paper, we tackle the problem by mining the decisive feature patterns (DFPs). Intuitively, a decisive feature pattern is a combination of low-level feature values that are unique and significant for describing a semantic concept. Interesting algorithms are developed to mine the decisive feature patterns and construct a rule base to automatically recognize semantic concepts in images. A systematic performance study on large image databases containing many semantic concepts shows that our method is more effective than some previously proposed methods. Importantly, our method can be generally applied to any domain of semantic concepts and low-level features.\n",
      "The robustness of obstacle avoidance algorithm is one of the important factors to successful applications of mobile robot systems. The sonar ring is used widely for autonomous mobile robot obstacle avoidance. This paper first analyzes the robustness of the existing obstacle avoidance algorithms based on sonar ring, indicates that the certainty grid method for obstacle representation is helpful to the robustness improvement of obstacle avoidance algorithms, but its effect is limited, it also has many disadvantages. By the simulation of two typical obstacle avoidance algorithms, the damage of interfered sonar data is revealed. Then the kinematics model of obstacle avoidance is built, Kalman filter which can restrain divergence is designed for interfered sonar data. Sonar data is used by obstacle avoidance algorithm after filtering. By the simulation contrast of the two obstacle avoidance algorithms, the effect of the Kalman filter for robustness improvement of obstacle avoidance algorithms is testified. Finally, the effect of the Kalman filter for eliminating noises in sonar data and for robustness improvement of obstacle avoidance algorithms is verified by experiments in two different situation.\n",
      "Non-industrial robot applications are getting more popular than ever. Moving capability is one of the most important features of non-industrial robots. In general, the movements of non-industrial robots are categorized as wheeled and legged mechanical platforms. The wheeled platform performs stable and fast movement characteristics; however, it can not move on humpy grounds or cross small doorsills. Contrarily, the legged platform performs better adaptations to different types of ground conditions; nevertheless, the walking velocity and stability and the larger energy consumptions restrict the practical applications. In this paper, we present a hybrid-structure robot with humanoid and vehicle types to perform home security tasks. To achieve home security issues, the smoke and temperature detection sensors are mounted on the robot. At the same time, the CCD camera is mounted on the head of a robot to capture the guarded videos and to assist remote manipulations. The security robot is controlled remotely in terms of wireless manner. The proposed hybrid-structure robot behaves vehicle type in most of operation time to perform stable and fast movements and to reduce energy consumptions. When the robot enters humpy grounds or crosses small doorsills, the robot structure is changed as humanoid type to pass the non-flat grounds. Therefore, the proposed hybrid-structure security robot provides flexible adaptations to different types of ground conditions in home. Due to size limitations, the security robot can be used in regular apartments. Finally, a 50 cm in height security robot prototype is implemented in laboratory. The robot had been successfully tested for legged walking, wheeled driving, changing structures, cross small doorsills, remote manipulations, and remote monitoring of security functions\n",
      "We present a power-efficient scheme to deliver time sensitive data packets in sensor networks. Data generated by sensors are frequently time sensitive in applications such as hazard monitoring systems, traffic controlling systems and battlefield commanding systems. Such data are associated with end-to-end deadlines, within which they must reach the base station. We make two contributions in this work. First, we propose a novel load-balanced routing scheme that distributes data packets evenly among the nodes relaying data towards the base station, avoiding bottlenecks and increasing the likelihood that packets will meet their deadlines. Second, we propose a method of grouping smaller packets into larger ones by delaying data transmissions at the relaying nodes whenever slack times are positive. Our packet grouping scheme significantly reduces packet transmissions, reduces congestion, and saves power in the sensor network. We verify the effectiveness of our approach through extensive simulations using the ns-2 simulation package.\n",
      "In this paper, we study the problem of efficiently computing k-medians over high-dimensional and high speed data streams. The focus of this paper is on the issue of minimizing CPU time to handle high speed data streams on top of the requirements of high accuracy and small memory. Our work is motivated by the following observation: the existing algorithms have similar approximation behaviors in practice, even though they make noticeably different worst case theoretical guarantees. The underlying reason is that in order to achieve high approximation level with the smallest possible memory, they need rather complex techniques to maintain a sketch, along time dimension, by using some existing off-line clustering algorithms. Those clustering algorithms cannot guarantee the optimal clustering result over data segments in a data stream but accumulate errors over segments, which makes most algorithms behave the same in terms of approximation level, in practice. We propose a new grid-based approach which divides the entire data set into cells (not along time dimension). We can achieve high approximation level based on a novel concept called (1−∊)-dominant. We further extend the method to the data stream context, by leveraging a density-based heuristic and frequent item mining techniques over data streams. We only need to apply an existing clustering once to computing k-medians, on demand, which reduces CPU time significantly. We conducted extensive experimental studies, and show that our approaches outperform other well-known approaches.\n",
      "We propose random distributed multiresolution representations of sensor network data, so that the most significant encoding coefficients are easily accessible by querying a few sensors, anywhere in the network. Less significant encoding coefficients are available by querying a larger number of sensors, local to the region of interest. Significance can be defined in a multiresolution way, without any prior knowledge of the source data, as global summaries versus local details. Alternatively, significance can be defined in a data-adaptive way, as large differences between neighboring data values. We propose a distributed encoding algorithm that is robust to arbitrary wireless communication connectivity graphs, where links can fail or change with time. This randomized algorithm allows distributed computation that does not require strict global coordination or awareness of network connectivity at individual sensors. Because computations involve sensors in local neighborhoods of the communication graph, they are communication-efficient. Our framework uses local interaction among sensors to enable flexible information retrieval at the global level.\n",
      "We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields. Experiments carried out on three language pairs and a variety of experiment conditions show that our model significantly outperforms a strong monolingual capitalization model baseline, especially when working with small datasets and/or European language pairs.\n",
      "Reconfigurable robots consist of many modules which are able to change the way they are connected. As a result, the robots have the capability of adopting different configurations to match various tasks and suit complex environments. This paper presents a novel field robot named JL-I which consists of three uniform modules. With the docking mechanisms, the modules can connect or disconnect flexibly and automatically. Furthermore the active joints formed by serial and parallel mechanisms endow the robot with the ability of changing shape in three dimensions. Consequently useful locomotion capabilities, such as crossing high vertical obstacles, getting self-recovery when the robot is upside-down are achieved. After describing the structural principle of the robot, the related kinematics analysis follows. In the end, the successful on-site tests confirm the principles described above and the robot's ability.\n",
      "The skyline operator is important for multicriteria decision-making applications. Although many recent studies developed efficient methods to compute skyline objects in a given space, none of them considers skylines in multiple subspaces simultaneously. More importantly, the fundamental problem on the  semantics  of skylines remains open: Why and in which subspaces is (or is not) an object in the skylineq Practically, users may also be interested in the skylines in any subspaces. Then, what is the relationship between the skylines in the subspaces and those in the super-spacesq How can we effectively analyze the subspace skylinesq Can we efficiently compute skylines in various subspaces and answer various analytical queriesqIn this article, we tackle the problem of multidimensional subspace skyline computation and analysis. We explore skylines in subspaces. First, we propose the concept of Skycube, which consists of skylines of all possible nonempty subspaces of a given full space. Once a Skycube is materialized, any subspace skyline queries can be answered online. However, Skycube cannot fully address the semantic concerns and may contain redundant information. To tackle the problem, we introduce a novel notion of  skyline group  which essentially is a group of objects that coincide in the skylines of some subspaces. We identify the  decisive subspaces  that qualify skyline groups in the subspace skylines. The new notions concisely capture the semantics and the structures of skylines in various subspaces. Multidimensional roll-up and drill-down analysis is introduced. We also develop efficient algorithms to compute Skycube, skyline groups and their decisive subspaces. A systematic performance study using both real data sets and synthetic data sets is reported to evaluate our approach.\n",
      "This paper describes an improved particle filters for mobile robot indoor localization based on a large-scale topological environmental model. The model is composed of multiple pivoted nodes and the connecting relations between them. We only choose the crossings as the nodes in the model so that the indoor environment can be presented by simplest structure. For localization, the traditional particle filters uses large number of samples when applied in large-scale environment, which greatly increase the computational cost and the memory space. To solve this problem, we improve the particle filters by selective distributing. We estimate the possible nodes which the robot is adjacent to in the large-scale topological environment model by laser scan-matching, then distribute the samples around the candidate nodes so that the number will decrease remarkably comparing with the traditional particle filters. Experiments result implemented in real mobile robot platform and further localization result analysis show the validity and practicability of this method.\n",
      "An incremental sliding mode controller for a class of second-order underactuated systems is proposed to achieve asymptotic stability and favorable performance. The second-order underactuated systems include two subsystems. Firstly, two state variables of a subsystem are chosen to construct the first-layer sliding surface. Secondly, the first-layer sliding surface and one of the left state variables are used to construct the second-layer sliding surface. The last sliding surface is constructed by the second-layer sliding surface and the left state variable. This proposed design method is applied to investigate the decoupled control of an inverted pendulum. The simulation results show the validity of incremental sliding mode control. At the same time, we analyze the way to construct the sliding surfaces and a rule of selecting parameters to construct the sliding surfaces.\n",
      "This paper introduces a method for building location information extraction using airborne C-band polarimetric SAR data. The method is based on the analysis of building feature. The method decomposes the scattering covariance matrix into three simple mechanism, i.e., odd-bounce; even-bounce; cross-bounce. Taking into account that the typical strong T-shaped echoes from quite large buildings are visible, a method is introduced to extract the location of the buildings.\n",
      "This paper presents a novel approach for visually tracking a colored target in a noisy and dynamic environment using weighted color histogram based particle filter algorithm. In order to make the tracking task robustly and effectively, color histogram based target model is integrated into particle filter algorithm, which considers the target's shape as a necessary factor in target model. Bhattacharyya distance is used to weight samples in the particle filter by comparing each sample's histogram with a specified target model and it makes the measurement matching and samples' weight updating more reasonable. The method is capable of successfully tracking moving targets in different indoor environment without initial positions information. A series of experiment results and experiment data analysis show this method's feasibility, and a surveillance system composed of two coordinating real mobile robots also given a practical application case of this method.\n",
      "Continuous range monitoring on moving objects has been increasingly important in mobile environments. With the computational power and memory capacity on the mobile side, the distributed processing could relieve the server from high workload and provide real-time results. The existing distributed approaches typically partition the space into subspaces and associate the monitoring regions with those subspaces. However, the spatial irrelevance of the subspaces and the monitoring regions incurs the redundant processing as well as the extra communication cost. In this paper, we propose continuous expansion (CEM), a novel approach for efficient processing of continuous range monitoring in mobile environments. Considering the concurrent execution of multiple continuous range queries, CEM abstracts the dynamic relations between the movement of objects and the change of query answers, and introduces the concept of query view. The query answers are affected if and only if there are objects changing their current query views, which lead to the minimum transmission cost on the moving object side. CEM eliminates the redundant processing by handling the updates only from the objects that potentially change the answers. The experimental results show that CEM achieves the good performance in terms of server load and communication cost.\n",
      "We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.\n",
      "There are almost more than 4,000 sorts of algae which could result in the red tide in the world, but only two or three, named the dominant species, place a premium on red tide at a time. This paper presents a method which uses the hyper-spectral images of different familiar dominant species to train the different networks respectively, then synthesizes the outputs of the networks with the same weight to recognize the red tide. It not only conquers the difficulties that are the selection of the training data and the networks training method, but also improves the generalization ability of the network system effectively. On the other hand, based on the neural network ensembles, the red tide recognition model could be extended easily and need not remodel the other networks. A mass of comparison experiments prove that the method recognizes the red tide and the dominant species effectively.\n",
      "There are large numbers of high-rise buildings with glass curtain walls that require constant cleaning and is carried out using permanent gondola systems. This is a laborious and dangerous work in midair. Due to a lack of uniform building structure, wall cleaning and maintenance of high-rise buildings is becoming one of the most appropriate fields for robotization. The development of walking and climbing offers a novel alternative solution to glass-wall cleaning. Application of a type of cleaning robotic system can free workers from this hazardous work and realize an automatic cleaning of high-rise buildings, thereby improving the technological level and productivity of the service industry.\n",
      "Real-time Digital Signal Processing (DSP) applications in wireless sensor network are often multi-rate in nature. These multi rate attributes provide additional significant opportunities in network design for achieving higher energy efficiency, besides traditional approaches of saving energy. In this paper, we propose a novel multi rate sensor network scheme to take advantage of power scaling for saving energy consumption. It can be easily synthesized into existing wireless sensor network MAC protocols, providing multi rate service for upper layer, and achieving communication energy efficiency. By monitoring a desirable BER threshold as the critical system parameter, the quality of received multi-rate data is assured. Based on this BER and the corresponding modulation scheme, the SNR at receiver end can be derived. The transmission power can be supplied optimally according to this SNR and the channel attenuation between sender and receiver. This dynamic power scaling scheme can lead to optimal energy efficiency since it demands that only a necessary amount of communication energy be consumed for multi-rate DSP applications. Our simulation shows that this multi data rate scheme with dynamic power scaling achieves energy saving significantly\n",
      "With the emerging of multi-frequency InSAR system, extraction of 3-D and variation information of earth surface using multi-frequency InSAR data has drawn more attention. Methods based on maximum-likelihood estimation (MLE), combining with some conventional phase unwrapping algorithm can be used to achieve this purpose. In this paper, MLE integrated with weighted multigrid phase-unwrapping method for high-sloped terrain profile reconstruction is presented and studied in detail. The performance of this method is also analyzed. Experiment result using simulated InSAR data shows that the method can be used to reconstruct high-sloped terrain with relatively high accuracy.\n",
      "SAR interferometry allows height reconstruction of the earth surface. A method based on the use of multi-frequency interferograms and Maximum Likelihood Estimation (MLE) has recently been proposed. However without a priori knowledge of the terrain, the result of the reconstruction is unsatisfied in practical cases. In this paper, we present a novel method to reconstruct highly sloped and discontinuous terrain height profiles using multi-frequency interferograms. It is based on MLE using multi-frequency interferograms joint statistic property, combining with some conventional signal frequency phase unwrapping algorithm. The method can not only improve efficiency of the MLE, but also ensure reliability of the estimation.\n",
      "An important application of wireless sensor networks is to perform the monitoring missions, for example, to monitor some targets of interests at all times. Sensors are often equipped with non-rechargeable batteries with limited energy and energy saving is a critical aspect for wireless sensor networks. If a target is monitored simultaneously by serval sensors, some of them can be switched off to save energy without causing mission failure and by which their operational times as well as the network lifetime can be prolonged. In this paper, we study the problem of scheduling sensor activity to cover a set of targets with known locations such that all targets can be monitored all the time and the network can operate as long as possible. A solution to this scheduling problem is to partition all sensors into sensor covers such that each cover can monitor all targets and the covers are activated successively. In this paper, we propose to use the notion of information coverage which is based on the estimation theory to exploit the collaborative nature of wireless sensor networks, instead of using the conventional definition of coverage. Due to the use of information coverage, a target that is not within the sensing disk of any single sensor can still be considered to be monitored (information covered) by the cooperation of more than one sensor.\n",
      "In this paper, we study an energy balanced routing protocol based on dynamic power scaling techniques for wireless sensor networks. In the proposed protocol, each node maintains a list of neighboring nodes and their corresponding statistical residual energy information. The energy balanced routing distributes the levels of residue energy evenly throughout the network, while the dynamic power scaling scheme achieves further energy savings by adjusting transmission power to meet signal strength requirement at the receiver. In order to demonstrate the increased energy efficiency of the proposed protocol, we have conducted an experimental study. The results of our experimental study clearly show that the proposed energy balanced routing scheme with dynamic power improves energy efficiency while achieving energy balance as well.\n",
      "In this paper, we propose a new model for coherent clustering of gene expression data called reg-cluster. The proposed model allows (1) the expression profiles of genes in a cluster to follow any shifting-and-scaling patterns in subspace, where the scaling can be either positive or negative, and (2) the expression value changes across any two conditions of the cluster to be significant. No previous work measures up to the task that we have set: the density-based subspace clustering algorithms require genes to have similar expression levels to each other in subspace; the pattern-based biclustering algorithms only allow pure shifting or pure scaling patterns; and the tendency-based biclustering algorithms have no coherence guarantees. We also develop a novel patternbased biclustering algorithm for identifying shifting-andscaling co-regulation patterns, satisfying both coherence constraint and regulation constraint. Our experimental results show that the reg-cluster algorithm is able to detect a significant amount of clusters missed by previous models, and these clusters are potentially of high biological significance.\n",
      "Protein-protein interactions, particularly weak and transient ones, are often mediated by peptide recognition domains, such as Src Homology 2 and 3 (SH2 and SH3) domains, which bind to specific sequence and structural motifs. It is important but challenging to determine the binding specificity of these domains accurately and to predict their physiological interacting partners. In this study, the interactions between 35 peptide ligands (15 binders and 20 non-binders) and the Abl SH3 domain were analyzed using molecular dynamics simulation and the Molecular Mechanics/Poisson-Boltzmann Solvent Area method. The calculated binding free energies correlated well with the rank order of the binding peptides and clearly distinguished binders from non-binders. Free energy component analysis revealed that the van der Waals interactions dictate the binding strength of peptides, whereas the binding specificity is determined by the electrostatic interaction and the polar contribution of desolvation. The binding motif of the Abl SH3 domain was then determined by a virtual mutagenesis method, which mutates the residue at each position of the template peptide relative to all other 19 amino acids and calculates the binding free energy difference between the template and the mutated peptides using the Molecular Mechanics/Poisson-Boltzmann Solvent Area method. A single position mutation free energy profile was thus established and used as a scoring matrix to search peptides recognized by the Abl SH3 domain in the human genome. Our approach successfully picked ten out of 13 experimentally determined binding partners of the Abl SH3 domain among the top 600 candidates from the 218,540 decapeptides with the PXXP motif in the SWISS-PROT database. We expect that this physical-principle based method can be applied to other protein domains as well.\n",
      "Recent efforts have been made to address the problem of privacy preservation in data publishing. However, they mainly focus on preserving data privacy. In this paper, we address another aspect of privacy preservation in data publishing, where some of the knowledge implied by a dataset are regarded as private or sensitive information. In particular, we consider that the data are stored in a transaction database, and the knowledge is represented in the form of patterns. We present a data sanitization algorithm, called SanDB, for effectively protecting a set of sensitive patterns, meanwhile attempting to minimize the impact of data sanitization on the non-sensitive patterns. The experimental results show that SanDB can achieve significant improvement over the best approach presented in the literature.\n",
      "Projective point elliptic curve cryptography (ECC) involves many look-up table-based field multiplications. In this paper, a novel lookup table sharing scheme is proposed to significantly reduce the number of the field multiplications and number of lookup tables. By requiring much less lookup table generation time, the proposed system offers high-speed performance for hardware realizations. Its application in a reconfigurable PC-FPGA platform is investigated, which can efficiently handle requests for different ECC key length and curves and is useful for many computer network applications.\n",
      "The existing network security assessment models have the problems of inadequate capacity of quantitative analysis and lacking for vulnerabilities correlation. To address these problems, a hierarchical network security evaluation model is proposed. The network is divided into vulnerability level, service level, equipment level and network level. The model uses attack graph to correlate the network vulnerabilities, and then calculates the probabilities of successfully exploiting the vulnerabilities. On this basis, the quantitative risks of each level are calculated. Since this model much more accords with the features of network structure, it is an effectively guidance for the network administrators to develop and improve the network security policies.\n",
      "As the Internet increases explosively, the QoS support in e-business server is becoming more and more important. However, the implementation of a fine-grained QoS control model in the e-business servers is a challenging task. In this paper, we propose a WorkManager-based QoS scheduling framework (WMQ) for e-business servers. This framework hides the complexity of QoS scheme implementations and provides a flexible architecture supporting fine-grained QoS control. A prototype of our framework has been implemented within the open-source server Tomcat. The evaluation of this prototype shows that our approach allows Tomcat to effectively meet the QoS requirement of the applications by fine-grained QoS control. Our experience also shows that the WMQ framework can efficiently simplify and reduce the implementation effort involved in developing QoS-enabled e-business servers.\n",
      "In this paper, we introduce a novel reconfigurable architecture, named  3D nFPGA , which utilizes 3D integration techniques and new nanoscale materials synergistically. The proposed architecture is based on CMOS-nano hybrid techniques that incorporate nanomaterials such as carbon nanotube bundles and nanowire crossbars into CMOS fabrication process. Using unique features of FPGAs and a novel 3D stacking method enabled by the application of nanomaterials, 3D nFPGA obtains a 4.5X footprint reduction compared to traditional CMOS-based 2D FPGAs. With a customized design automation flow, we evaluate the performance and power of 3D nFPGA driven by the 20 largest MCNC benchmarks. Results demonstrate that 3D nFPGA is able to provide a performance gain of 2.6X with a small power overhead comparing to the CMOS 2D FPGA architecture.\n",
      "We introduce non-negative matrix factorization with orthogonality constraints (NMFOC) for detection of a target spectrum in a given set of Raman spectra data. An orthogonality measure is defined and two different orthogonality constraints are imposed on the standard NMF to incorporate prior information into the estimation and hence to facilitate the subsequent detection procedure. Both multiplicative and gradient type update rules have been developed. Experimental results are presented to compare NMFOC with the basic NMF in detection, and to demonstrate its effectiveness in the chemical agent detection problem.\n",
      "The problem of selecting a sample subset sufficient to preserve diversity arises in many applications. One example is in the design of recombinant inbred lines (RIL) for genetic association studies. In this context, genetic diversity is measured by how many alleles are retained in the resulting inbred strains. RIL panels that are derived from more than two parental strains, such as the collaborative cross (Churchill et al., 2004), present a particular challenge with regard to which of the many existing lab mouse strains should be included in the initial breeding funnel in order to maximize allele retention. A similar problem occurs in the study of customer reviews when selecting a subset of products with a maximal diversity in reviews. Diversity in this case implies the presence of a set of products having both positive and negative ranks for each customer. In this paper, we demonstrate that selecting an optimal diversity subset is an NP-complete problem via reduction to set cover. This reduction is sufficiently tight that greedy approximations to the set cover problem directly apply to maximizing diversity. We then suggest a slightly modified subset selection problem in which an initial greedy diversity solution is used to effectively prune an exhaustive search for all diversity subsets bounded from below by a specified coverage threshold. Extensive experiments on real datasets are performed to demonstrate the effectiveness and efficiency of our approach.\n",
      "DNA microarray techniques present a novel way for geneticists to monitor interactions among tens of thousands of genes simultaneously, and have become standard lab routines in gene discovery, disease diagnosis, and drug design. There has been extensive research on coherent subspace clustering of gene expressions measured under consistent experimental settings. This implies that all experiments are run using the same batch of microarray chips with similar characteristics of noise. Algorithms developed under this assumption may not be applicable for analyzing data collected from heterogeneous settings, where the set of genes being monitored may be different and expression levels may be not directly comparable even for the same gene. In this paper, we propose a model, F-cluster, for mining subspace coherent patterns from heterogeneous gene expression data, which is shown effective for revealing truthful patterns and reducing spurious ones. We also develop an efficient and scalable hybrid approach that combines gene-pair based and sample-pair based pruning to generate F-clusters from multiple gene expression matrices simultaneously. The experimental results demonstrate that our model can discover significant clusters that may not be identified by previous models.\n",
      "Current deployed emergency communications systems are only available to the rescuing workers. In this paper, a framework of wireless emergency communications is proposed for common communications in the disasters based on relaying and cognitive radio. In this framework, relaying provides small coverage expansion and high capacity for common communications. On the other hand, cognitive radio based frequency lowering provides large coverage expansion and low system capacity for special number communications. The coverage performance is evaluated by the calculation and simulation. To balance the tradeoff between coverage and capacity, both two-hop relaying and cognitive radio are adopted appropriately to satisfy the requirements of emergency communications according to their characteristics. The performance of the proposed framework is also investigated in this paper.\n",
      "The knowledge discovered by frequent pattern mining is represented in the form of a collection of frequent patterns with their supports. Sharing the frequent patterns without discrimination may bring threats against privacy and security, because some of frequent patterns themselves may be sensitive and should not be disclosed. Furthermore, due to the existence of inference channels, an attacker may also derive sensitive patterns from a set of non-sensitive patterns. Therefore, just eliminating sensitive patterns from the mining result is not enough to prevent their disclosure. We classify the potential inference channels into three categories, and present two algorithms for blocking these inference channels by pattern sanitization. The main advantage of our work is that it does not bring about any fake knowledge, and also does not distort the original knowledge.\n",
      "In 2007, Shacham and Waters [9] propose the first effi- cient ring signature scheme, without random oracles, based on standard assumptions. And the signature size is linear in the size of the ring. In this paper, we analyze the security of Shacham and Waters ring signature [9] when using the structure of ring signature proposed by [6]. We claim that, in some cases, Shacham and Waters ring signature can be applied in a more efficient way, i.e., the size of the ring sig- nature can be constant. Moreover, the signer can decide whether to provide linkability by herself.\n",
      "Approximate range aggregate queries are one of the most frequent and useful kinds of queries for Decision Support Systems (DSS), as they are widely used in many data analysis tasks. Traditionally, sampling-based techniques have been proposed to tackle this problem. However, their effectiveness degrade when the underlying data distribution is skewed. Another approach based on the outlier management can limit the effect of data skews but fails to address other requirements of approximate range aggregate queries, such as error guarantees and query processing efficiency. In this paper, we present a technique that provides approximate answers to range aggregate queries on OLAP data cubes efficiently, with theoretical guarantees on the errors. Our basic idea is to build different data structures to manage outliers and the rest of the data. Carefully chosen outliers are organized in a quad-tree based indexing data structure to provide efficient access for query processing. A query-workload adaptive, tree-like synopsis data structure, called T unable P artition-Tree (TP-Tree), is proposed to organize samples extracted from non-outlier data. Our experiments clearly demonstrate the merits of our technique, by comparing with previous well-known techniques.\n",
      "Data streams are often locally correlated, with a subset of streams exhibiting coherent patterns over a subset of time points. Subspace clustering can discover clusters of objects in different subspaces. However, traditional sub- space clustering algorithms for static data sets are not readily used for incremental clustering, and is very expensive for frequent re-clustering over dynamically changing stream data. In this paper, we present an efficient incremental sub- space clustering algorithm for multiple streams over sliding windows. Our algorithm detects all the delta-CC-Clusters, which capture the coherent changing patterns among a set of streams over a set of time points. delta-CC'-Cluster s are incrementally generated by traversing a directed acyclic graph pDAG. We propose efficient insertion and deletion operations to update the pDAG dynamically. In addition, effective pruning techniques are applied to reduce the search space. Experiments on real data sets demonstrate the performance of our algorithm.\n",
      "Active learning is a promising tool to improve the performance of content-based image retrieval (CBIR). As a commonly used active learning approach, angle-diversity provides the most informative images to user for feedback. However, it suffers from the problem that the query concept is diverse and the numbers of the positive and the negative images are imbalanced. As a consequence, the positive samples obtained by active learning are inadequate, which degrades the learning efficiency. To deal with this issue, we propose a novel method based on angle-diversity and hyperplane shifting to increase the number of positive images in the active learning results. The experiment is conducted on a test data set with 10,000 images. Compared with the traditional angle-diversity technique, our method can improve the retrieval performance significantly.\n",
      "Recent advances in distributed source coding (DSC) for mission-driven wireless sensor networks (WSN) are related to the coding for multiple correlated sensors in applications such as real time target tracking and environment monitoring. The characteristic of these DSC applications provides significant potential opportunities in the associated sensor network to utilize multirate transmissions for enhancing network performance. In this paper, we study a methodology for interplay optimization between routing and DSC in WSN, and propose a novel multirate based routing scheme for mission-driven DSC applications that considerably extends network lifetime. The proposed scheme adopts the rate assignment based on the residual energy, and employs a joint rate and energy scheduling mechanism to meet the end-to-end transmission rate constraint, information precision requirement, and the energy constraints in the network for DSC. Simulation results show that this multirate based routing scheme achieves significantly longer network lifetime compared to other existing research in WSN for DSC applications.\n",
      "In wireless sensor network, it is always important to obtain accurate relative positions of each sensor node, especially when there is no anchor node in the network to provide an absolute coordinate system. In this paper, this problem was first addressed by building a local coordinate system to express relative locations. Then the Cramer-Rao lower bound (CRLB) in an anchor-free network is analyzed, and used to find the optimal path for locating a remote node through vector addition. Some geometric problems in the path setup process are also addressed. Through simulation, the authors showed that the proposed method could give better accuracy than previous similar approach, and even in certain case could outperform some other approach which has higher computational cost.\n",
      "While active rules have been applied in many areas including active databases, XML documentation and Semantic Web, current methods remain largely uncertain of how to terminate active behaviors. Some existing methods have been provided in the form of a logical formula for a rule set, but they suffer two problems, (i) Only those variables, which are non-updatable or finitely updatable, can be contained by a formula. (ii) They cannot conclude termination if a rule set only contains some cycles that can be executed in a finite number of times. Many active rule systems, which only contain updatable variables, can still be terminated. This paper presents an algorithm to construct a formula, which can contain updatable variable. Also, a method is proposed to detect if a cycle can only be executed in a finite number of times. Theoretical analysis shows more termination cases, which is indetctive for existing methods, can be detected by our method.\n",
      "K-anonymity is a simple yet practical mechanismto protect privacy against attacks of re-identifying individuals by joining multiple public data sources. All existing methods achieving k-anonymity assume implicitly that the data objects to be anonymized are given once and fixed. However, in many applications, the real world data sources are dynamic. In this paper, we investigate the problem of maintaining k-anonymity against incremental updates, and propose a simple yet effective solution. We analyze how inferences from multiple releases may temper the k-anonymity of data, and propose the monotonic incremental anonymization property. The general idea is to progressively and consistently reduce the generalization granularity as incremental updates arrive. Our new approach guarantees the k-anonymity on each release, and also on the inferred table using multiple releases. At the same time, our new approach utilizes the more and more accumulated data to reduce the information loss.\n",
      "We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.\n",
      "Increased speeds of PCs and networks have made media communications possible on the Internet. Today, the need for desktop videoconferencing is experiencing robust growth in both business and consumer markets. However, the synchronous delivery of high-volume media content is still a big challenge under a current heterogeneous Internet environment. In this paper, we present a multiparty videoconferencing system based on a peer-to-peer (P2P) solution. The contribution of our paper is twofold. On the one hand, we design an application-level multicast scheme which intends to tolerate the heterogeneity in videoconferencing applications. Design tradeoffs are analyzed and our decisions are made based on extensive experimentation. On the other, we design a five-layer architecture for implementing a multiparty videoconferencing system. This architecture makes a clear-cut distinction between different functional modules and therefore provides rich flexibility in feature adaptation. We believe that our work can be a helpful reference in other efforts on building desktop videoconferencing systems.\n",
      "The IBM supply-chain network optimization workbench (SNOW) is a software tool that can help a company make strategic business decisions about the design and operation of its supply chain network. The tool supports supply chain analysis with integrated network optimization and simulation capability. Mathematical programming models are used to first help identify some cost-effective scenarios from a large number of candidates. Optimization results are then converted to simulation models automatically for more detailed analysis with taking into account operational policies and uncertainties. The tool was applied to analyze both IBM's internal supply chains and external clients' supply chains. The combination of optimization and simulation demonstrates great value in real business cases.\n",
      "Nowadays, smart spaces occupy an essential part of ubiquitous computing environment. The spaces integrated with wireless sensors networks, actuators and context-aware services become part of our daily life. Smart spaces are equipped with a large number of wireless sensors that aim to collect large quantities of context information, during the process, there exists a large amount of collisions and energy consumption. Therefore, this paper provides a novel multi-rate based local framing pre-schedule scheme to further reduce collisions and improve energy efficiency in CSMA/TDMA hybrid MAC layer of wireless sensor network. This MAC combines CSMA and TDMA functionalities together while obviates their shortcomings. Having been assigned, slot 0 is preserved as the pre-schedule slot, to inform neighbor nodes the schedule of the senders. During the pre-schedule slot, each node knows exactly the schedule of other neighbor nodes. Multi-rate and power scaling are applied to achieve further energy saving by adpoting an acceptable rate rather than maximum rate. Data rate is dynamically adjusted according to the traffic load of sending nodes, in an energy efficient data rate, to save energy. Being compared with Z-MAC in terms of performances, local framing pre-schedule and multi-rate in this experiment achieved further energy efficiency.\n",
      "In cognitive radio network, power control is necessary to not only decrease the interference among the unlicensed users, but also avoid the negative effect to the licensed users. In this paper, a noncooperative power control model is proposed for the unlicensed users using game theory. In order to restrict the interference to the licensed users, an exponential part indicating the effect to the licensed users is added into the pricing function. Through game theoretic deduction, it is obtained that a unique Nash equilibrium solution exists under appropriating parameter value of the payoff function. Further, the Nash equilibrium solution of the proposed power control game with exponential pricing is Pareto optimality and achieves maximum total throughput under strict constraint of the interference temperature limitation. The appropriate value of the new parameter in the pricing function is discussed. The performance of the proposed power control algorithm is investigated by numeral results.\n",
      "This paper describes a new aerial images segmentation algorithm. Kernel Matching Pursuit (KMP) method is introduced to deal with the nonlinear distribution of the man-made objects' features in the aerial images. In KMP algorithm, a lot of training samples containing substantive information are used to detect the man-made objects. With KMP classifier, pixels in large aerial images will be labeled as different prediction values, which can be classified linearly. Then the modified Mumford-Shah model, which comprises the features of the KMP prediction values, is built to segment the aerial image by necessary level set evolution. The proposed method is proven to be effective by the results of experiments.\n",
      "Many sensor applications such as monitoring and surveillance may require image sensor array to conduct collaborative image transmissions in wireless sensor networks (WSN). The large size image transmissions cause bottlenecks in WSN due to the limited energy resources and network capacity. In this paper, we propose a collaborative transmission scheme for image sensors to utilize inter-sensor correlations to decide transmission patterns based on transmission path diversities, which achieves minimal energy consumption, balanced sensor lifetime and required image quality. This optimization scheme not only allows each image sensor to transmit optimal fractions of the overlapped images through appropriate transmission paths in energy-efficient way, but also provides unequal protection on the overlap image regions through path selections and resource allocations to achieve good transmission image quality. The simulation results show that the proposed image transmission scheme can achieve considerable gains in terms of the network lifetime extension, image distortion reduction, and energy efficiency.\n",
      "Many sensor applications such as monitoring and surveillance may require image sensor array to conduct collaborative image transmissions in wireless sensor network (WSN). The large size image transmission in WSN is a bottleneck due to the limited energy resources. In this paper, we propose an optimal scheme for image sensors to utilize inter-sensor correlations to decide transmission patterns based on a multi-level rate-oriented routing (MLRR) routing scheme, which achieves high energy efficiencies and longer network lifetime. This optimization scheme allows each image sensor to transmit optimal fractions of the overlapped images through appropriate multiple rate-oriented routing paths. The simulation results by using GA algorithm show that the proposed image transmission scheme can achieve considerable gains in terms of the WSN energy efficiency and network lifetime extension.\n",
      "At first, this paper briefly introduces a novel reconfigurable mobile robots system JL-1 and the serial and parallel mechanism using to perform the reconfiguration actions. Based on the kinematics equations of the serial and parallel mechanism, the static transformation equations of it are concluded according to the virtual work theory, which is useful in determining the drivable workspace of the reconfiguration mechanism. Then the valid workspace of the serial and parallel mechanism of JL-1 is concluded via the transformation matrix of the loads in world coordinates to those on the driving motors according to the real parameters of the motors in JL-1 and the weight of JL-1 robot. The degeneration phenomenon of the workspace is found due to the limited torque of the rotating motor, and a drivable workspace is acquired. At last, by choosing the valid joint path in the drivable workspace, a series of experiments are performed to demonstrate the validity of the theory concluded in this paper.\n",
      "This paper describes an innovative framework, iFAO-Simo, which integrates optimization, simulation and GIS (geographic information system) techniques to handle complex spatial facility network optimization problems ever challenged from retailing, banking and logistics nowadays. At the top level of iFAO-Simo, an optimization engine serves to generate and test candidate solutions iteratively by use of optimization algorithms such as Tabu Search and Genetic Algorithms. For each scenario given by the candidate solutions, a discrete event simulation engine is triggered to simulate customer and facility behaviors based on a GIS platform to characterize and visualize the spatial, dynamic and indeterministic environments. As the result, the target measures can be easily calculated to evaluate the solution and feedback to the optimization engine. This paper studies a real case of banking branch network optimization problem, and the results show that iFAO-Simo provides a useful way to handle complex spatial optimization problems.\n",
      "Elevation maps are a widely used spatial data representation in geographical information systems (GIS). Paths on elevation maps can be characterized by profiles, which describe relative elevation as a function of distance. In this research, we address the inverse of this mapping - given a profile, how to efficiently find paths that could have generated it. This is called the profile query problem. Profiles have a wide variety of uses that include registering tracking information, or even other maps, to a given map. We describe a probabilistic model to characterize the maximal likelihood that a point lying on a path matches the query profile. Propagation of such probabilities to neighboring points can effectively prune the search space. This model enables us to efficiently answer queries of arbitrary profiles with user-specified error tolerances. When compared to existing spatial index methods, our approach supports more flexible queries with orders of magnitude speedup.\n",
      "To overcome the overshoot and oscillation in the normal Bang-Bang controller applied in the pneumatic driving glass-wall cleaning robot, this paper presents a F&A compensating variable Bang-Bang control algorithm for the pneumatic position servo. Besides a new cylinder driving plan is designed, the control algorithm is also improved by adding a fiction force compensating item in the chamber force evaluation equation. To adjust the controlling parameters dynamically, certain increasing functions, such as step function, linear function and arc-tangent function, are used to define the piston's expectative acceleration along with the reduction of the piston's position error. To testify the feasibility of the control algorithms with the above three expectative acceleration setting functions respectively, a pneumatic glass-wall cleaning robot \"Sky Cleaner 4\" is used as a testing platform to implement a series of on-load position servo experiments. And the different results of them are compared.\n",
      "The overload of Web application servers (WAS) is a typical and critical problem encountered in nowaday Web commerce. This problem imposes greater demands on WAS to provide QoS support under overload conditions. However, even with a practical solution, the implementation of a flexible QoS model to enable finegrained QoS control in WAS is still a challenging task. In this paper we propose a QoS-enabled workmanager model (WMQ), which hides the complexity of QoS mechanisms implementations and provides a flexible QoS architecture. Hybrid QoS schemes are proposed base on this model to support self-optimization and fine-grained QoS control. We implement this model and integrate it in a Web application server. Our integrating experience shows that the WMQ model significantly reduces the implementation effort in developing a QoS-enabled WAS. We evaluate this model by a TPC-W workload generator in a typical e-commerce application. Evaluation results show that, our overload control allows consistent performance and improves the system throughput up to 22% during extreme overload. In addition, compared with other widely used scheduling policies, our hybrid request scheduling policy shows better performance on improving the system throughput and minimizing the number of aborted requests.\n",
      "Background: Protein domains coordinate to perform multifaceted cellular functions, and domain combinations serve as the functional building blocks of the cell. The available methods to identify functional domain combinations are limited in their scope, e.g. to the identification of combinations falling within individual proteins or within specific regions in a translated genome. Further effort is needed to identify groups of domains that span across two or more proteins and are linked by a cooperative function. Such functional domain combinations can be useful for protein annotation. Results: Using a new computational method, we have identified 114 groups of domains, referred to as domain assembly units (DASSEM units), in the proteome of budding yeast Saccharomyces cerevisiae. The units participate in many important cellular processes such as transcription regulation, translation initiation, and mRNA splicing. Within the units the domains were found to function in a cooperative manner; and each domain contributed to a different aspect of the unit's overall function. The member domains of DASSEM units were found to be significantly enriched among proteins contained in transcription modules, defined as genes sharing similar expression profiles and presumably similar functions. The observation further confirmed the functional coherence of DASSEM units. The functional linkages of units were found in both functionally characterized and uncharacterized proteins, which enabled the assessment of protein function based on domain composition. Conclusion: A new computational method was developed to identify groups of domains that are linked by a common function in the proteome of Saccharomyces cerevisiae. These groups can either lie within individual proteins or span across different proteins. We propose that the functional linkages among the domains within the DASSEM units can be used as a non-homology based tool to annotate uncharacterized proteins.\n",
      "Resource allocation for multimedia selective encryption and energy efficient transmission has not been fully investigated in literature for wireless sensor networks (WSNs). In this article, we propose a new cross-layer approach to optimize selectively encrypted image transmission quality in WSNs with strict energy constraint. A new selective image encryption approach favorable for unequal error protection (UEP) is proposed, which reduces encryption overhead considerably by controlling the structure of image bitstreams. Also, a novel cross-layer UEP scheme based on cipher-plain-text diversity is studied. In this UEP scheme, resources are unequally and optimally allocated in the encrypted bitstream structure, including data position information and magnitude value information. Simulation studies demonstrate that the proposed approach can simultaneously achieve improved image quality and assured energy efficiency with secure transmissions over WSNs.\n",
      "This brief presents a detailed time-domain and frequency-domain analysis of a direct RF sampling mixer. Design considerations such as incomplete charge sharing and large signal nonlinearity are addressed. An accurate frequency-domain transfer function is derived. Estimation of noise figure is given. The analysis applies to the design of sub-sampling mixers that have become important for software-defined radio and analog-to-digital converter.\n",
      "Motivation: Typical high-throughput genotyping techniques produce numerous missing calls that confound subsequent analyses, such as disease association studies. Common remedies for this problem include removing affected markers and/or samples or, otherwise, imputing the missing data. On small marker sets imputation is frequently based on a vote of the K-nearest-neighbor (KNN) haplotypes, but this technique is neither practical nor justifiable for large datasets.#R##N##R##N#Results: We describe a data structure that supports efficient KNN queries over arbitrarily sized, sliding haplotype windows, and evaluate its use for genotype imputation. The performance of our method enables exhaustive exploration over all window sizes and known sites in large (150K, 8.3M) SNP panels. We also compare the accuracy and performance of our methods with competing imputation approaches.#R##N##R##N#Availability: A free open source software package, NPUTE, is available at http://compgen.unc.edu/software, for non-commercial uses.#R##N##R##N#Contact: mcmillan@cs.unc.edu\n",
      "The minimum symbol error rate (SER) precoder design for spatial multiplexing (SM) system with zero forcing detector over correlated channel is investigated. With the knowledge of channel correlation at the transmitter, a precoder scheme that combines eigen-beamforming with power allocation (PA) is proposed. A closed-form expression for optimal PA on the eigenbeams is presented to minimise the overall average SER through an approximate SER analysis. When compared with the minimum mean-square error (MMSE) waterfilling precoder, the pre-processing is very simple in that it avoids loop or iteration operation in waterfilling algorithm. Both the analytical and simulation results have confirmed the effectiveness of the precoder. Moreover, when the number of receive antennas is larger than that of the transmit antennas, the performance of this proposal can approximate or even exceed that of MMSE waterfilling precoder at higher signal-to-noise ratio.\n",
      "An adaptive fuzzy control approach is proposed for a class of multiple-input-multiple-output (MIMO) nonlinear systems with completely unknown nonaffine functions. The MIMO systems are composed of n subsystems and each of subsystems is in the nested lower triangular form. It is difficult and complicated to control this class of systems due to the existence of unknown nonaffine functions and the couplings among the nested subsystems. This difficulty is overcome by introducing some special type Lyapunov functions and taking advantage of the mean-value theorem, the backstepping design method and the approximation property of the fuzzy systems. The proposed control approach can guarantee that all the signals in the closed-loop system are bounded. A simulation experiment is utilized to verify the feasibility of the proposed approach.\n",
      "This paper studies the problem of actuator or controller failure of a class of time-varying delay systems. It is assumed once the actuator or controller fails, the system becomes unstable. It addresses how long and how frequent the failure is so that the system can keep exponentially stable. Based on a switching method, this kind of failure problems is converted into one of finding a switching signal to stabilize the corresponding switched delay system. Then using average dwell time method, the sufficient conditions guaranteeing exponential stability are derived in the case of actuator or controller failure. A numerical example is used to show the effectiveness of the proposed method.\n",
      "In this paper, the representations of fuzzy concepts based on raw data have been investigated within the framework of AFS (Axiomatic Fuzzy Set) theory. First, a brief review of AFS theory is presented and a completely distributive lattice, the E^#I algebra, is proposed. Secondly, two kinds of E^#I algebra representations of fuzzy concepts are derived in detail. In order to represent the membership functions of fuzzy concepts in the interval [0,1], the norm of AFS algebra is defined and studied. Finally, the relationships of various representations with their advantages and drawbacks are analyzed.\n",
      "In cognitive radio network, the interference of the unlicensed users to the licensed users should be limited under interference temperature constraints. In this paper, the optimal power control scheme of a network is analyzed without interference temperature constraints firstly. Based on this, considering interference temperature constraints, the optimal power control in cognitive radio network is modeled as a concave minimization problem. Some useful properties of the power control optimization problem are exploited. According to these properties, an improved branch and bound algorithm which is more efficient than the general branch and bound algorithm is proposed for optimal power control optimization problem in cognitive radio network.\n",
      "Constraints are essential for many sequential pattern mining applications. However, there is no systematic study on constraint-based sequential pattern mining. In this paper, we investigate this issue and point out that the framework developed for constrained frequent-pattern mining does not fit our mission well. An extended framework is developed based on a sequential pattern growth methodology. Our study shows that constraints can be effectively and efficiently pushed deep into the sequential pattern mining under this new framework. Moreover, this framework can be extended to constraint-based structured pattern mining as well.\n",
      "Automatically discovering repetitive clips from large video database is a challenging problem due to the enormous computational cost involved in exploring the huge solution space. Without any  a priori  knowledge of the contents, lengths and total number of the repetitive clips, we need to discover all of them in the video database. To address the large computational cost, we propose a novel method which translates  repetitive clip mining  to the  continuous path finding  problem in a matching trellis, where sequence matching can be accelerated by taking advantage of the temporal redundancies in the videos. By applying the locality sensitive hashing (LSH) for efficient similarity query and the proposed continuous path finding algorithm, our method is of only  quadratic  complexity of the database size. Experiments conducted on a 10.5-hour TRECVID news dataset have shown the effectiveness, which can discover repetitive clips of various lengths and contents in only 25 minutes, with features extracted off-line.\n",
      "In many delay tolerant applications, information is opportunistically exchanged between mobile devices who encounter each other. In order to effect such information exchange, mobile devices must have knowledge of other devices in their vicinity. We consider scenarios in which there is no infrastructure and devices must probe their environment to discover other devices. This can be an extremely energy consuming process and highlights the need for energy conscious contact probing mechanisms. If devices probe very infrequently, they might miss many of their contacts. On the other hand, frequent contact probing might be energy inefficient. In this paper, we investigate the trade-off between the probability of missing a contact and the contact probing frequency. First, via theoretical analysis, we characterize the trade-off between the probability of a missed contact and the contact probing interval for stationary processes. Next, for time varying contact arrival rates, we provide an optimization framework to compute the optimal contact probing interval as a function of the arrival rate. We characterize real world contact patterns via Bluetooth phone contact logging experiments and show that the contact arrival process is self-similar. We design STAR, a contact probing algorithm which adapts to the contact arrival process. Via trace driven simulations on our experimental data, we show that STAR consumes three times less energy when compared to a constant contact probing interval scheme.\n",
      "The shear volume of the results in traditional support based frequent sequential pattern mining methods has led to increasing interest in new intelligent mining methods to find more meaningful and compact results. One such approach is the consensus sequential pattern mining method based on sequence alignment, which has been successfully applied to various areas. However, the current approach to consensus sequential pattern mining has quadratic run time with respect to the database size limiting its application to very large databases. In this paper, we introduce two optimization techniques to reduce the running time significantly. First, we determine the theoretical bound for precision of the proximity matrix and reduce the time spent on calculating the full matrix. Second, we use a sample based iterative clustering method which allows us to use a much faster k-means clustering method with only a minor increase in memory consumption with negligible loss in accuracy.\n",
      "Purpose – This paper presents the design of climbing robots for glass‐wall cleaning.Design/methodology/approach – A systemic analysis of the basic functions of a glass‐wall cleaning system is given based on the research of working targets. Then the constraints for designing a glass‐wall cleaning robot are discussed. The driving method, the attachment principle, mechanical structure and unique aspects of three pneumatic robots named Sky Cleaners follow. In the end a summary of the main special features is given. All three climbing robots are tested on site.Findings – Our groups spent several years in designing and developing a series of robots named Sky Cleaners which are totally actuated by pneumatic cylinders and sucked to the glass walls with vacuum grippers in mid‐air. It was found that they can meet the requirements of glass‐wall cleaning.Research limitation/implications – The air source, cleaning liquid and control signals should be provided by the supporting vehicle stationed on the ground. Even if ...\n",
      "It is a well-known fact that test power consumption may exceed that during functional operation. Leakage power dissipation caused by leakage current in Complementary Metal-Oxide-Semiconductor (CMOS) circuits during test has become a significant part of the total power dissipation. Hence, it is important to reduce leakage power to prolong battery life in portable systems which employ periodic self-test, to increase test reliability and to reduce test cost. This paper analyzes leakage current and presents a kind of leakage current simulator based on the transistor stacking effect. Using it, we propose techniques based on don't care bits (denoted by Xs) in test vectors to optimize leakage current in integrated circuit (IC) test by genetic algorithm. The techniques identify a set of don't care inputs in given test vectors and reassign specified logic values to the X inputs by the genetic algorithm to get minimum leakage vector (MLV). Experimental results indicate that the techniques can effectually optimize leakage current of combinational circuits and sequential circuits during test while maintaining high fault coverage.\n",
      "Consider a large-scale wireless sensor network measuring compressible data, where n distributed data values can be well-approximated using only k <g n coefficients of some known transform. We address the problem of recovering an approximation of the n data values by querying any L sensors, so that the reconstruction error is comparable to the optimal fc-term approximation. To solve this problem, we present a novel distributed algorithm based on sparse random projections, which requires no global coordination or knowledge. The key idea is that the sparsity of the random projections greatly reduces the communication cost of pre-processing the data. Our algorithm allows the collector to choose the number of sensors to query according to the desired approximation error. The reconstruction quality depends only on the number of sensors queried, enabling robust refinable approximation.\n",
      "A two-phase segmentation mechanism is described that allows a global homogeneity-related measure to be optimized in a level-set formulation. The mechanism has uniform treatment toward texture, gray level, and color boundaries. Intensities or colors of the image are first coarsely quantized into a number of classes. Then a class map is formed by having each pixel labeled with the class identity its gray or color level is associated with. With this class map, for any segmented region, it can be determined which pixels inside the region belong to which classes, and it can even be calculated how spread-out each of such classes is inside the region. The average spread-size of the classes in the region, in comparison with the size of the region, then constitutes a good measure in evaluating how homogeneous the region is. With the measure, the segmentation problem can be formulated as the optimization of the average homogeneity of the segmented regions. This work contributes chiefly by expressing the above optimization functional in such a way that allows it to be encoded in a variational formulation and that the solution can be reached by the deformation of an active contour. In addition, to solve the problem of multiple optima, this work incorporates an additional geodesic term into the functional of the optimization to maintain the active contour's mobility at even adverse condition of the deformation process. Experimental results on synthetic and real images are presented to demonstrate the performance of the mechanism.\n",
      "Elliptic curve cryptography (ECC) is recognized as a fast cryptography system and has many applications in security systems. In this paper, a novel sharing scheme is proposed to significantly reduce the number of field multiplications and the usage of lookup tables, providing high speed operations for both hard-ware and software realizations.\n",
      "We designed and implemented MSFSS, a scalable and flexible distributed file system for storage and retrieval of mass small files. MSFSS is a platform built upon the existing commodity file systems. It automatically stores files onto the best-fit commodity file systems according their access patterns. To avoid central bottleneck, it optimizes metadata size, separates metadata operations from file data transfer, and implements batch metadata operations. The system provides data migration, hot file caching, and replication, which are essential for large scale, reliable storage systems. It has successfully been deployed as storage system for our Web application which has about 50 TB of small files. Experimental results show that MSFSS provides high scalability and throughput in file operation services.\n",
      "There is a great deal of research conducted on hyperplane based query such as Support Vector Machine (SVM) in Content-based Image Retrieval(CBIR). However, the SVM-based CBIR always suffers from the problem of the imbalance of image data. Specifically, the number of negative samples (irrelevant images) is far more than that of the positive ones. To deal with this problem, we propose a new active learning approach to enhance the positive sample set in SVM-based Web image retrieval. In our method, instead of using complex parsing methods to analyze Web pages, two kinds of \"lightweight\" image features: the URL of the Web image and its visual features, which can be easily obtained, are applied to estimate the probability of the image being a potential positive sample. The experiments conducted on a test data set with more than 10,000 images from about 50 different Web sites demonstrate that compared with traditional methods, our approach improves the retrieval performance significantly.\n",
      "An XML document is inconsistent if it violates predefined integrity constraints. In this paper, we consider how to compute repairs for an inconsistent XML document. Here repair is defined as the data consistent with the integrity constraints, and also minimally differs from the original document. Based on a repair framework by introducing a chase method, in this paper, we discuss the repairs computing problem and implement a prototype. First we discuss some key points about mends generation and repairs chasing. Next we give a cost model for this repair framework, which can be used to evaluate the cost of each repair. Finally we implement prototypes of our method, and evaluate our framework and algorithms in the experiment.\n",
      "Security and reliability are major concerns in Grid computing systems. Trust mechanism has been focus of much research in recent years providing a safety and reliable Grid computing environment. Based on EigenTrust model, in this paper, we extend the traditional job scheduling strategies and present a new algorithm named Trust-Oriented Sufferage algorithm. Simulations are performed to evaluate the performance of the new algorithm.\n",
      "Coverage is an important issue in wireless sensor networks. The most commonly used coverage model in the literature defines a point to be covered if its Euclidian distance to at least one sensor is less than a fixed threshold. This is a conservative definition of coverage which implicitly assumes that each sensor makes a decision independent of other sensors in the field. Sensors can cooperate to make an accurate estimation, even if any single sensor is unable to do so. We have previously proposed a new notion of information coverage and investigated its properties. In this paper, we study sensor density requirements for complete information coverage of a field with random sensor deployment. We provide an upper bound on the probability that an arbitrary point in a randomly deployed sensor field is not information covered and find the relationship between the sensor density and the average field vacancy. Simulation results validate our theoretical analysis and show that significant savings in terms of sensor density for complete coverage can be achieved with information coverage.\n",
      "Joint routing-and-scheduling has been considered in wireless mesh networks for its significant performance improvement. While existing work assumes it, accurate traffic information is usually not available due to traffic dynamics, as well as inaccuracy and delay in its measurement and dissemination. In addition, the joint routing and scheduling usually requires a centralized controller to calculate the optimal routing and scheduling and distribute such policies to all the nodes. Thus, even if the accurate traffic information is always available, the central controller has to compute the routing and scheduling repeatedly because the traffic demands change continuously. This leads to prohibitive computation and distribution overhead. Therefore, in this paper, we propose a joint routing-scheduling scheme that achieves robust performance under traffic information uncertainty. In particular, it achieves worst-case optimal performance under a range of traffic conditions. This unique feature validates the use of centralized routing and scheduling in wireless mesh networks. As long as the traffic variation is within the estimation range, the routing and scheduling do not need to be recomputed and redistributed. Our simulation shows that the proposed scheme achieves very good average performance under a large error margin on traffic estimation and is robust when the estimation largely deviates from the actual traffic patterns. Our scheme provides insights on the desired properties of multipath routing, namely, spatial reuse and load balancing.\n",
      "Peer-to-peer (P2P) overlay networks provide a new way to retrieve information over networks. How to assurance the reliability of the resource is the crucial issue of security. This paper proposes a trustworthy overlay based on the small world phenomenon that facilitates efficient search for information re- trieval with security assurance in unstructured P2P systems. Each node main- tains a number of short-range links to the trusted other nodes, together with a small collection of long-range links that help increasing recall rate of informa- tion retrieval, and the trust degree of each node is evaluated by Bayesian method. Simulation test shows that the proposed model can not only increase the ratio of resource discovery, improve the interaction performance of the en- tire network, but also assurance the reliability of resource selection.\n",
      "We present a fast algorithm for computing approximate quantiles in high speed data streams with deterministic error bounds. For data streams of size N where N is unknown in advance, our algorithm partitions the stream into sub-streams of exponentially increasing size as they arrive. For each sub-stream which has a fixed size, we compute and maintain a multi-level summary structure using a novel algorithm. In order to achieve high speed performance, the algorithm uses simple block-wise merge and sample operations. Overall, our algorithms for fixed-size streams and arbitrary-size streams have a computational cost of O(N log(1/epsivlogepsivN)) and an average per-element update cost of O(log logN) if epsiv is fixed.\n",
      "In detecting the boundary of an object in an image, if certain prior shape knowledge of the object is available, an effective approach is to have the intensity gradient information in the image and the prior shape knowledge be combined together to drive an active contour for the purpose. While in the classical methods the two terms are almost always summed with a certain weight between them to form the optimization functional, in the method we propose, they are multiplied together so as to avoid the need and thus design of the weight parameter. We show that the object detection result in the traditional formulation could indeed be very much affected by the weight value, and the proposed method, being without its presence, is therefore free from the influence of the important parameter. Experimental results on cells in real biological images, whose boundaries are blurred to very different degrees across the image by the inevitably uneven illumination, are shown to demonstrate the improvement in performance.\n",
      "In recent years, the wireless sensor network (WSN) is employed a wide range of applications. But existing communication protocols for WSN ignore the characteristics of collected data and set routes only according to the mutual distance and residual energy of sensors. In this paper we propose a Data-Aware Clustering Hierarchy (DACH), which organizes the sensors based on both distance information and data distribution in the network Furthermore, we also present a multi-granularity query processing method based on DACH, which can estimate the query result more efficiently. Our empirical study shows that DACH has higher energy efficiency than Low-Energy Adaptive Clustering Hierarchy (LEACH), and the multi-granularity query processing method based on DACH brings more accurate results than a random access system using same cost of energy.\n",
      "In many applications, XML documents need to be modelled as graphs. The query processing of graph-structured XML documents brings new challenges. In this paper, we design a method based on labelling scheme for structural queries processing on graph-structured XML documents. We give each node some labels, the reachability labelling scheme. By extending an interval-based reachability labelling scheme for DAG by Rakesh et al., we design labelling schemes to support the judgements of reachability relationships for general graphs. Based on the labelling schemes, we design graph structural join algorithms to answer the structural queries with only ancestor-descendant relationship efficiently. For the processing of subgraph query, we design a subgraph join algorithm. With efficient data structure, the subgraph join algorithm can process subgraph queries with various structures efficiently. Experimental results show that our algorithms have good performance and scalability.\n",
      "In this paper, a collusion-resistant matrix system (CRMS) for group key managements is presented. The CRMS is defined as a collection of subsets of users, and the keys held by users in CRMS are organized in a hierarchical matrix manner. After describing the join and leave protocols, we prove that CRMS has lceil2N/trceil-collusion resistant capability. Moreover, the suggestion of how to select the parameters in CRMS is given. The simulated experiments show that CRMS is a practical group key management in wireless networks.\n",
      "In this paper, we characterize the graphs with infinite cyclic edge connectivity. Then we design an efficient algorithm to determine whether a graph has finite cyclic edge connectivity or infinite cyclic edge connectivity.\n",
      "This paper considers the coverage problem for hybrid networks which comprise both static and mobile sensors. The mobile sensors in our network only have limited mobility, i.e., they can move only once over a short distance. In random static sensor networks, sensor density should increase as O(log L + k log log L) to provide k-coverage in a network with a size of L. As an alternative, an all-mobile network can provide k-coverage with a constant density of O(k), independent of network size L. We show that the maximum distance for mobile sensors is O( 1/radic(k) log  3/4 (kL)). We then propose a hybrid network structure, comprising static sensors and a small fraction of O( 1/radic(k)) of mobile sensors. For this network structure, we prove that k-coverage is also achievable with a constant sensor density of O(k). Furthermore, for this hybrid structure, we prove that the maximum distance which any mobile sensor has to move is bounded as O(log (3/4) L). We then propose a distributed relocation algorithm, where each mobile sensor only requires local information in order to optimally relocate itself. We verify our analysis via extensive numerical evaluations and show an implementation of the mobility algorithm on real mobile sensor platforms.\n",
      "The performances of the Ad Hoc networks vary rapidly because of the change of topology. How to describe and quantify this dynamic characteristic is the basis of designing and simulating the Ad Hoc network. Describing the move patterns of the nodes is the customary way. This way is called mobility model, which have been found several faults. In this paper, Topology Variety Model (TVM) is proposed to describe the dynamic characteristic from the link layer by means of link duration and connectivity probability and it is realized in the simulator ns-2. The simulation results indicate that with appropriate PDF of link duration for scenario, TVM can replace mobility model to simulate Ad Hoc networks. Meanwhile, the relationship between mobility and TVM is studied. The influence of dynamic on network performance is simulated based on TVM.\n",
      "In this paper we propose a new clustering algorithm which combines the FCM clustering algorithm with the supervised learning normal mixture model; we call the algorithm as the FCM-SLNMM clustering algorithm. The FCM-SLNMM clustering algorithm consists of two steps. The FCM algorithm was applied in the first step. In the second step the supervised learning normal mixture model was applied and the clustering result of the first step was used as training data. The experiments on the real world data from the UCI repository show that the supervised learning normal mixture model can improve the performance of the FCM algorithm sharply, and which also show that the FCM-SLNMM perform much better than the unsupervised learning normal mixture model and other comparison clustering algorithms. This indicates that the FCM-SLNMM algorithm is an effective clustering algorithm.\n",
      "The World Wide Web is a major source of the news now. The news analysis play more and more important roles in the political decision, financial analysis, invest decision, market forecast and so on. This paper proposes a news analysis system based on the hall for workshop of metasynthetic engineering. In this system, in order to analyses online news, the news articles are active fetched, and being classified into standard categories. And then by means of adaptive clustering, the correlative news is obtained into the same cluster, helping the user to reduce the time to search the news which he cares about. Moreover, the news comments are classified into categories such as negative, positive, which are also grouped into clusters helping the experts to get the view of the common people to the news. So the news attention analysis can be obtained to help the experts find which news is the people concerned most effectively.\n",
      "In this paper, we study the problem of making use of target constraints to integrate XML data from different sources under a target schema. We recognize that target constraints are necessary in data integration, as the constraints are essential part of data semantics, and should be satisfied by integrated data. When integrating data from multiple data sources with overlapping data, constraints can express data merging rules at the target as well. We give a general constraint model for XML to express target constraints, which extends the relational equality-generating and tuple- generating dependencies. We provide a chase method to reason about data in the integrated XML document based on target constraints, by inferring data values not given explicitly, and inserting new subtrees as necessary. Singleton and key constraints are used to uniquely specify a certain entity, as a rule for data merging in the integration.\n",
      "Simultaneously clustering columns and rows (co- clustering) of large data matrix is an important problem with wide applications, such as document mining, microarray analysis, and recommendation systems. Several co-clustering algorithms have been shown effective in discovering hidden clustering structures in the data matrix. For a data matrix of m rows and n columns, the time complexity of these methods is usually in the order of mtimesn (if not higher). This limits their applicability to data matrices involving a large number of columns and rows. Moreover, an implicit assumption made by existing co-clustering methods is that the whole data matrix needs to be held in the main memory. In this paper, we propose a general framework, CRD, for co-clustering large datasets utilizing recently developed sampling- based matrix decomposition methods. The time complexity of our approach is linear in m and n. And it does not require the whole data matrix be in the main memory. Experimental results show that CRD achieves competitive accuracy to existing co-clustering methods but with much less computational cost.\n",
      "Intrusion detection is an important technique in the defense-in-depth network security framework. Most current intrusion detection models lack the ability to process massive audit data streams for real-time anomaly detection. In this paper, we present an effective anomaly intrusion detection model based on Principal Component Analysis (PCA). The model is more suitable for high speed processing of massive data streams in real-time from various data sources by considering the frequency property of audit events than by use of the transition property or the correlation property. It can serve as a general framework that a practical Intrusion Detection Systems (IDS) can be implemented in various computing environments. In this method, a multi-pronged anomaly detection model is used to monitor various computer system and network behaviors. Three sources of data, system call data from the University of New Mexico (lpr) and from KLINNS Lab of Xi'an Jiaotong University (ftp), shell command data from AT&T Research laboratory, and network data from MIT Lincoln Lab, are used to validate the model and the method. The frequencies of individual system calls generated by one process and of individual commands embedded in one command block as well as features extracted in one network connection are transformed into an input data vector. Our method is employed to reduce the high dimensional data vectors and thus the detection is handled in a lower dimension with high efficiency and low use of system resources. The distance between a vector and its reconstruction in the reduced subspace is used for anomaly detection. Empirical results show that our model is promising in terms of detection accuracy and computational efficiency, and thus amenable for real-time intrusion detection.\n",
      "With the explosion in the amount of semi-structured data users access and store, there is a need for complex search tools to retrieve often very heterogeneous data in a simple and efficient way. Existing tools usually index text content, allowing for some IR-style ranking on the textual part of the query, but only consider structure (e.g., file directory) and metadata (e.g., date, file type) as filtering conditions. We propose a novel multidimensional querying approach to semi-structured data searches in personal information systems by allowing users to provide fuzzy structure and metadata conditions in addition to traditional keyword conditions. The provided query interface is more comprehensive than content-only searches as it considers three query dimensions (content, structure, metadata) in the search. We have implemented our proposed approach in the Wayfinder file system. In this demo, we will use this implementation to both present an overview of the unified scoring framework underlying the fuzzy multi-dimensional querying approach and demonstrate its potential in improving search results.\n",
      "The problem of ranking has recently gained attention in data learning. The goal ranking is to learn a real-valued ranking function that induces a ranking or ordering over an instance space. In this paper, we apply popular Bayesian techniques on ranking support vector machine. We propose a novel differentiable loss function called trigonometric loss function with the desirable characteristic of natural normalization in the likelihood function, and then set up a Bayesian framework. In this framework, Bayesian inference is used to implement model adaptation, while keeping the merits of ranking SVM. Experimental results on data sets indicate the usefulness of this approach.\n",
      "As the security of software is deeply valued while its complexity and size are increasing, automated verification is highly desirable. On the other hand, verification of pointer programs remains a major challenge. In our previous work pointer logic has been proposed to verify basic safety properties of pointer programs, and in this work, we developed efficient algorithms and techniques to implement pointer logic rules for automated verification. The algorithms and techniques are dedicated to reducing the human effort involved in program verification. Moreover, they have been implemented in a tool -- PLCC to automatically verify a range of non-trivial programs such as basic operations on singly-linked lists, trees, circular doubly-linked list etc. and the experimental results show that in acceptable time pointer logic can be applied to automated verification.\n",
      "We present a new adaptive and energy-efficient broadcast model to support flexible responses to client queries. Clients do not have to request documents by name, since they may know the characteristics of the documents but not the document names or IDs. In our model, clients specify requirements through attributes, and servers broadcast documents that match client requests at a prespecified level of similarity. A given document may satisfy several clients, so the server broadcasts a minimal set of documents that achieves a desired level of satisfaction in the client population. The server obtains randomized feedback from clients and adapts its broadcast program accordingly. Clients use a selective tune-in scheme based on approximate indexing to conserve energy. Our model captures client interest patterns efficiently and accurately and scales very well with the number of clients while reducing the overall client average waiting times. The selective tune-in scheme reduces client energy consumption greatly, with a modest wait time increase.\n",
      "We investigate the problem of clustering on distributed data streams. In particular, we consider the k-median clustering on stream data arriving at distributed sites which communicate through a routing tree. Distributed clustering on high speed data streams is a challenging task due to limited communication capacity, storage space, and computing power at each site. In this paper, we propose a suite of algorithms for computing (1 + epsiv) -approximate k-median clustering over distributed data streams under three different topology settings: topology-oblivious, height-aware, and path-aware. Our algorithms reduce the maximum per node transmission to  polylog   N  (opposed to Omega( N ) for transmitting the raw data). We have simulated our algorithms on a distributed stream system with both real and synthetic datasets composed of millions of data. In practice, our algorithms are able to reduce the data transmission to a small fraction of the original data. Moreover, our results indicate that the algorithms are scalable with respect to the data volume, approximation factor, and the number of sites.\n",
      "With the explosion in the amount of semi-structured data users access and store in personal information management systems, there is a need for complex search tools to retrieve often very heterogeneous data in a simple and efficient way. Existing tools usually index text content, allowing for some IR-style ranking on the textual part of the query, but only consider structure (e.g., file directory) and metadata (e.g., date, file type) as filtering conditions. We propose a novel multi-dimensional approach to semi-structured data searches in personal information management systems by allowing users to provide fuzzy structure and metadata conditions in addition to keyword conditions. Our techniques provide a complex query interface that is more comprehensive than content-only searches as it considers three query dimensions (content, structure, metadata) in the search. We propose techniques to individually score each dimension, as well as a framework to integrate the three dimension scores into a meaningful unified score. Our work is integrated in Wayfinder, an existing fully-functioning file system. We perform a thorough experimental evaluation of our techniques to show the effect of approximating individual dimensions on the overall scores and ranks of files, as well as on query performance. Our experiments show that our scoring strategy adequately takes into account the approximation in each dimension to efficiently evaluate fuzzy multi-dimensional queries. In addition, fuzzy query conditions in non-content dimensions can significantly improve scoring (and thus ranking) accuracy.\n",
      "Recently, great efforts have been dedicated to researches on the management of large-scale graph-based data, where node disjoint subgraph homeomorphism relation between graphs has been shown to be more suitable than (sub)graph isomorphism in many cases, especially in those cases where node skipping and node mismatching are desired. However, no efficient algorithm for node disjoint subgraph homeomorphism determination (ndSHD) has been available. In this paper, we propose two computationally efficient ndSHD algorithms based on state spaces searching with backtracking, which employ many heuristics to prune the search spaces. Experimental results on synthetic data sets show that the proposed algorithms are efficient, require relatively little time in most of cases, can scale to large or dense graphs, and can accommodate to more complex fuzzy matching cases.\n",
      "Information sharing becomes more frequently and easily than before. However, it also brings serious threats towards individual's privacy. It is no doubt that sharing personal data can cause privacy breaches. Moreover, sharing the knowledge discovered by data mining may also pose threats to personal privacy. In this paper, we consider the anonymity of patterns derived from the result of frequent itemset mining. A new projection-based approach for detecting anonymity of patterns is presented. We prove that the approach can detect all the maximal inference channels for non-k-anonymous patterns. The experimental results show that our approach is more efficient than previous work especially when the number of closed frequent itemsets in the mining result is close to or larger than the number of transactions in a database.\n",
      "Many approaches have been proposed to find correlations in binary data. Usually, these methods focus on pair-wise correlations. In biology applications, it is important to find correlations that involve more than just two features. Moreover, a set of strongly correlated features should be non-redundant in the sense that the correlation is strong only when all the interacting features are considered together. Removing any feature will greatly reduce the correlation.#R##N##R##N#In this paper, we explore the problem of finding non-redundant high order correlations in binary data. The high order correlations are formalized using multi-information, a generalization of pairwise mutual information. To reduce the redundancy, we require any subset of a strongly correlated feature subset to be weakly correlated. Such feature subsets are referred to as Non-redundant Interacting Feature Subsets (NIFS). Finding all NIFSs is computationally challenging, because in addition to enumerating feature combinations, we also need to check all their subsets for redundancy. We study several properties of NIFSs and show that these properties are useful in developing efficient algorithms. We further develop two sets of upper and lower bounds on the correlations, which can be incorporated in the algorithm to prune the search space. A simple and effective pruning strategy based on pair-wise mutual information is also developed to further prune the search space. The efficiency and effectiveness of our approach are demonstrated through extensive experiments on synthetic and real-life datasets.\n",
      "In this article we develop a novel graph-based approach toward network forensics analysis. Central to our approach is the evidence graph model that facilitates evidence presentation and automated reasoning. Based on the evidence graph, we propose a hierarchical reasoning framework that consists of two levels. Local reasoning aims to infer the functional states of network entities from local observations. Global reasoning aims to identify important entities from the graph structure and extract groups of densely correlated participants in the attack scenario. This article also presents a framework for interactive hypothesis testing, which helps to identify the attacker's nonexplicit attack activities from secondary evidence. We developed a prototype system that implements the techniques discussed. Experimental results on various attack datasets demonstrate that our analysis mechanism achieves good coverage and accuracy in attack group and scenario extraction with less dependence on hard-coded expert knowledge.\n",
      "We present a multiscale model for numerical simulation of dynamics of crystalline solids. The method couples nonlinear elastodynamics as the continuum description and molecular dynamics as another component at the atomic scale. The governing equations on the macroscale are solved by the discontinuous Galerkin method, which is built up with an appropriate local curl-free space to produce a coherent displacement field. The constitutive data are based on the underlying atomistic model: it is either calibrated prior to the computation or obtained from molecular dynamics as the computation proceeds. The decision to use either the former or the latter is made locally for each cell based on suitable criteria.\n",
      "There has been considerable interest in similarity join in the research community recently. Similarity join is a fundamental operation in many application areas, such as data integration and cleaning, bioinformatics, and pattern recognition. We focus on efficient algorithms for similarity join with edit distance constraints. Existing approaches are mainly based on converting the edit distance constraint to a weaker constraint on the number of matching q-grams between pair of strings.#R##N##R##N#In this paper, we propose the novel perspective of investigating mismatching q-grams. Technically, we derive two new edit distance lower bounds by analyzing the locations and contents of mismatching q-grams. A new algorithm, Ed-Join, is proposed that exploits the new mismatch-based filtering methods; it achieves substantial reduction of the candidate sizes and hence saves computation time. We demonstrate experimentally that the new algorithm outperforms alternative methods on large-scale real datasets under a wide range of parameter settings.\n",
      "In this paper, we present a general internal model (GIM) approach for motion skill learning at elementary and coordination levels. A unified internal model (IM) is developed for describing discrete and rhythmic movements. Through analysis, we show that the GIM possesses temporal and spatial scalabilities which are defined as the ability to generate similar movement patterns directly by means of tuning some parameters of the IM. With scalability, the learning or training process can be avoided when dealing with similar tasks. The coordination is implemented in the GIM with appropriate phase shifts among multiple IMs under an overall architecture. To facilitate the establishment of the GIM, in this paper, we further explored algorithms for detecting periodicity of and phase difference between rhythmic movements, and neural network structures suitable for learning motion patterns. Through three illustrative examples, we show that the human behavior patterns with single or multiple limbs can be easily learned and established by the GIM at the elementary and coordination levels.\n",
      "The safety of pointer programs is an important issue in high-assurance software design, and their verification remains a major challenge. Pointer Logic has been proposed to verify basic safety properties of pointer programs in our previous work, but still lacks support for a wide range of pointer programs. In this work, we present an extension to Pointer Logic by: 1) introducing modular reasoning to scale better on programs involving function calls; 2) allowing pointer arithmetic to take more advantage of pointers in programming. Moreover, to demonstrate that Pointer Logic is a useful approach to verification, we implement a tool - pice to automatically verify a range of non-trivial programs, including basic operations on singly-linked lists, trees, circular doubly-linked list, dynamic arrays etc.\n",
      "Most existing methods of stereo matching focus on dealing with clear image pairs. Consequently, there is a lack of approaches capable of handling degraded images captured under challenging real situations, e.g. motion blur is present and an image pair is in different illumination conditions. In this paper we propose a novel approach to handling these challenging situations by formulating the problem into a Maximum a Posteriori (MAP) estimation framework, and adopt a segment-based symmetric stereo matching method to infer a mask of disparity map which indicates whether a disparity is affected by motion blur and estimate the disparity value. The experimental results show that our stereo matching method is able to compute more accurate disparity maps of this type of degraded images.\n",
      "Image transmissions in wireless multimedia sensor networks (WMSNs) are often energy constrained. They also have requirement on distortion minimization, which may be achieved through unequal error protection (UEP) based communication approaches. In related literature with regard to wireless multimedia transmissions, significantly different importance levels between image-pixel-position information and image-pixel-value information have not been fully exploited by existing UEP schemes. In this paper, we propose an innovative image-pixel-position information based resource allocation scheme to optimize image transmission quality with strict energy budget constraint for image applications in WMSNs, and it works by exploring these uniquely different importance levels among image data streams. Network resources are optimally allocated cross PHY, MAC and APP layers regarding inter-segment dependency, and energy efficiency is assured while the image transmission quality is optimized. Simulation results have demonstrated the effectiveness of the proposed approach in achieving the optimal image quality and energy efficiency. The performance gain in terms of distortion reduction is especially prominent with strict energy budget constraints and lower image compression ratios.\n",
      "This paper studies the stability for a class of systems with time-varying delay subject to controller failure. It explores under what conditions of controller failure the system is still exponentially stable. The concept of controller failure frequency is introduced. First, the delay system with failed controller is modelled as a class of switched delay systems. Next, based on two lemmas developed in this paper and the piecewise Lyapunov functional method, sufficient conditions are provided to guarantee the exponential stability of the system when controller failure occurs, which is in terms of the solvability of a set of linear matrix inequalities (LMIs). Finally, an example of a networked control system is given to illustrate the effectiveness of the proposed method.\n",
      "As a de facto standard for information representation and exchange over the Internet, XML has been used extensively in many applications. And XML query technology has attracted more and more attention in data management research community. Standard XML query languages, e.g. XPath and XQuery, use twig pattern as a basic unit to match relevant fragments from a given XML document. However, in most existing work, only simple containment relationships are involved in the twig pattern, which makes it infeasible in many cases. In this paper, we extend the original twig pattern to complex twig pattern (CTP), which may contain ordered relationship between query nodes. We give a detailed analysis of the hard nuts that prevent us from finding an efficient solution for CTP matching, and then propose a novel holistic join algorithm, LBHJ, to handle the CTP efficiently and effectively. We show in experimental results that LBHJ can largely reduce the size of intermediate results and thus improve the query performance significantly according to various metrics when processing CTP with ordered axes.\n",
      "This paper considers the exponential stability problem for a class of discrete time-varying delay systems with large delay sequences (LDSs). A new method based on a switching technique is presented to solve this problem. A switched delay system, in which one of the discrete subsystems may be unstable, is firstly employed to describe such a system, and then some new concepts about LDSs are introduced. Next, using a novel piecewise Lyapunov functional, explicit delay-dependent conditions are developed to show how long and how frequent the LDSs can be and still maintain the system exponentially stable. Without LDSs, the criterion obtained in this paper includes the established one as a special case. Two numerical examples are given to show the effectiveness of the proposed method.\n",
      "Subspace clustering has attracted great attention due to its capability of finding salient patterns in high dimensional data. Order preserving subspace clusters have been proven to be important in high throughput gene expression analysis, since functionally related genes are often co-expressed under a set of experimental conditions. Such co-expression patterns can be represented by consistent orderings of attributes. Existing order preserving cluster models require all objects in a cluster have identical attribute order without deviation. However, real data are noisy due to measurement technology limitation and experimental variability which prohibits these strict models from revealing true clusters corrupted by noise. In this paper, we study the problem of revealing the order preserving clusters in the presence of noise. We propose a noise-tolerant model called approximate order preserving cluster (AOPC). Instead of requiring all objects in a cluster have identical attribute order, we require that (1) at least a certain fraction of the objects have identical attribute order; (2) other objects in the cluster may deviate from the consensus order by up to a certain fraction of attributes. We also propose an algorithm to mine AOPC. Experiments on gene expression data demonstrate the efficiency and effectiveness of our algorithm.\n",
      "This paper proposes a codebook based limited feedback zero-forcing(ZF) precoding scheme for multi-user multiple-input multiple-output (MIMO) downlink channels. In such system, each user needs to calculate a channel directional index (CDI) and a channel quality indicator (CQI), feedback them to the base station (BS). BS selects the set of active users, and calculates transmit beams through zero forcing method. In case of rank-one feedback, each user should combine the MIMO channel into a multiple-input single-output (MISO) channel through a receiver weight, then feedback CDI and CQI of this combined channel. Current existing schemes use the Hermitian of the left eigenvector associated to the maximum eigenvalue of the channel as the weight. The problem is that when there is multi-user interference, the feedback information is inaccurate. Focusing on this problem, we propose a new feedback scheme which uses a quasi minimum mean squared error (MMSE) weight to calculate CQI and CDI. By the simulations in 3GPP long term evaluation (LTE) environment, the proposed scheme is proved to achieve higher spectrum efficiency than the existing schemes.\n",
      "Relational database is the most widely adopted and mature technology for information storage. As many services on the Web (e.g., blog and wiki sites) and advanced applications (e.g., customer relationship management systems and content management systems) are built on RDBMSs, increasing amount of text data is now stored in relational databases, accompanied by increasing demands of retrieving relevant information by free-style keyword search.\n",
      "Multi-view learning  has become a hot topic during the past few years. In this paper, we first characterize the sample complexity of multi-view  active learning . Under the α- expansion  assumption, we get an exponential improvement in the sample complexity from usual  O (1/ e ) to  O (log 1/ e ), requiring neither strong assumption on data distribution such as the data is distributed uniformly over the unit sphere in R  d   nor strong assumption on hypothesis class such as linear separators through the origin. We also give an upper bound of the error rate when the α- expansion  assumption does not hold. Then, we analyze the combination of multi-view active learning and semi-supervised learning and get a further improvement in the sample complexity. Finally, we study the empirical behavior of the two paradigms, which verifies that the combination of multi-view active learning and semi-supervised learning is efficient.\n",
      "Finding latent patterns in high dimensional data is an important research problem with numerous applications. Existing approaches can be summarized into 3 categories: feature selection, feature transformation (or feature projection) and projected clustering. Being widely used in many applications, these methods aim to capture global patterns and are typically performed in the full feature space. In many emerging biomedical applications, however, scientists are interested in the local latent patterns held by feature subsets, which may be invisible via any global transformation. In this paper, we investigate the problem of finding local linear correlations in high dimensional data. Our goal is to find the latent pattern structures that may exist only in some subspaces. We formalize this problem as finding strongly correlated feature subsets which are supported by a large portion of the data points. Due to the combinatorial nature of the problem and lack of monotonicity of the correlation measurement, it is prohibitively expensive to exhaustively explore the whole search space. In our algorithm, CARE, we utilize spectrum properties and effective heuristic to prune the search space. Extensive experimental results show that our approach is effective in finding local linear correlations that may not be identified by existing methods.\n",
      "Both the clonal selection algorithm (CSA) and the ant colony optimization (ACO) are inspired by natural phenomena and are effective tools for solving complex problems. CSA can exploit and explore the solution space parallely and effectively. However, it can not use enough environment feedback information and thus has to do a large redundancy repeat during search. On the other hand, ACO is based on the concept of indirect cooperative foraging process via secreting pheromones. Its positive feedback ability is nice but its convergence speed is slow because of the little initial pheromones. In this paper, we propose a pheromone-linker to combine these two algorithms. The proposed hybrid clonal selection and ant colony optimization (CSA-ACO) reasonably utilizes the superiorities of both algorithms and also overcomes their inherent disadvantages. Simulation results based on the traveling salesman problems have demonstrated the merit of the proposed algorithm over some traditional techniques.\n",
      "We study the information-theoretic limits of exactly recovering the support of a sparse signal using noisy projections defined by various classes of measurement matrices. Our analysis is high-dimensional in nature, in which the number of observations n, the ambient signal dimension p, and the signal sparsity k are all allowed to tend to infinity in a general manner. This paper makes two novel contributions. First, we provide sharper necessary conditions for exact support recovery using general (non-Gaussian) dense measurement matrices. Combined with previously known sufficient conditions, this result yields a sharp characterization of when the optimal decoder can recover a signal with linear sparsity (k = Theta(p)) using a linear scaling of observations (n = Theta(p)) in the presence of noise. Our second contribution is to prove necessary conditions on the number of observations n required for asymptotically reliable recovery using a class of gamma-sparsified measurement matrices, where the measurement sparsity gamma(n, p, k) isin (0,1] corresponds to the fraction of non-zero entries per row. Our analysis allows general scaling of the quadruplet (n, p, k, gamma), and reveals three different regimes, corresponding to whether measurement sparsity has no effect, a minor effect, or a dramatic effect on the information theoretic limits of the subset recovery problem.\n",
      "Low latency anonymity systems are susceptive to traffic analysis attacks. In this paper, we propose a dependent link padding scheme to protect anonymity systems from traffic analysis attacks while providing a strict delay bound. The covering traffic generated by our scheme uses the minimum sending rate to provide full anonymity for a given set of flows. The relationship between user anonymity and the minimum covering traffic rate is then studied via analysis and simulation. When user flows are Poisson processes with the same sending rate, the minimum covering traffic rate to provide full anonymity to m users is O(log m). For Pareto traffic, we show that the rate of the covering traffic converges to a constant when the number of flows goes to infinity. Finally, we use real Internet trace files to study the behavior of our algorithm when user flows have different rates.\n",
      "With the steady development of WiMAX based broadband wireless access networks, complete and accurate simulation studies become crucial. In an effort to further improve our ns-2 simulation model for mobile WiMAX, we have implemented a selective repeat automatic retransmission request (SR-ARQ) mechanism into our model and evaluated its performance and accuracy. In this paper, we present the design and implementation methodology of the WiMAX SR-ARQ simulation model in ns-2 and discuss its accuracy via extensive simulation studies. Our evaluation clearly shows that SR-ARQ achieves increased successful packet delivery ratio and goodput, which is true for both non-saturated channels and saturated channels. In each case a cost of increased bandwidth consumption and latency overhead for received packets is also quantitatively analyzed. We believe the implementation of SR-ARQ in the ns-2 simulation model and its evaluation studies will provide profound empirical values to WiMAX field test studies.\n",
      "We describe a new approach for inferring the functional relationships between non-homologous protein families by looking at statistical enrichment of alternative function predictions in classification hierarchies such as Gene Ontology (GO) and Structural Classification of Proteins (SCOP). Protein structures are represented by robust graphs, and the Fast Frequent Subgraph Mining algorithm is applied to protein families to generate sets of family-specific packing motifs, i.e. amino acid residue packing patterns shared by most family members but infrequent in other proteins. The function of a protein is inferred by identifying in it motifs characteristic of a known family. We employ these family-specific motifs to elucidate functional relationships between families in the GO and SCOP hierarchies. Specifically, we postulate that two families are functionally related if one family is statistically enriched by motifs characteristic of another family, i.e. if the number of proteins in a family containing a motif from another family is greater than expected by chance. This function inference method can help annotate proteins of unknown function, establish functional neighbors of existing families, and help specify alternate functions for known proteins.\n",
      "In this paper, we analyze the performance of mobile real time services in a large-scale IEEE 802.11 multi-hop network. We present our field measurements with discussion of performance bottlenecks for VoIP services under mobile scenarios. We utilized our test bed which is located in Nebraska and supported by the University of Nebraska-Lincoln (UNL), US Federal Railroads Administrations (FRA), and major US railroad companies. Our UNL-FRA test bed consists of 8 outdoor access point towers, which are deployed along 3.5 mile of BNSF railroad. Passive measurement approaches are taken to ensure the integrity of collected data and multi-layer stream based packet analyzer has been implemented to provide a global view of the entire performance of the monitored network from the physical layer to the application layer. Based on our analysis of collected data, we conclude that in a typical outdoor 802.11 environment similar to our UNL-FRA test bed, uncertain handoff latencies and lack of Quality of Service guarantee are main performance bottlenecks for real-time applications. Furthermore, we discuss the enhancement strategies to support mobility in high-speed railway networks. We believe our work is one of the initial efforts in wireless mobile network field measurement in terms of scale, methodology, and analysis.\n",
      "We describe a new approach for elucidating the nonlinear degrees of freedom in a distribution of shapes depicted in digital images. By combining a deformation-based method for measuring distances between two shape configurations together with multidimensional scaling, a method for determining the number of degrees of freedom in a shape distribution is described. In addition, a method for visualizing the most representative modes of variation (underlying shape parameterization) in a nuclei shape distribution is also presented. The novel approach takes into account the nonlinear nature of shape manifolds and is related to the ISOMAP algorithm. We apply the method to the task of analyzing the shape distribution of HeLa cell nuclei and conclude that approximately three parameters are responsible for their shape variation. Excluding differences in size, translation, and orientation, these are: elongation, bending (concavity), and shifts in mass distribution. In addition, results show that, contrary to common intuition, the most likely nuclear shape configuration is not symmetric.\n",
      "This paper analyses the advantages and disadvantages of the K-means algorithm and the DENCLUE algorithm. In order to realise the automation of clustering analysis and eliminate human factors, both partitioning and density-based methods were adopted, resulting in a new algorithm - Clustering Algorithm based on object Density and Direction (CADD). This paper discusses the theory and algorithm design of the CADD algorithm. As an illustration of its applicability, CADD was used to cluster real world data from the geochemistry domain.\n"
     ]
    }
   ],
   "source": [
    "df_main=pd.DataFrame()\n",
    "for author in author_list_collab_5[:1]:\n",
    "    author_df=df.loc[df['author']==author[0]]\n",
    "    for i in author_df['abstract']:\n",
    "        print(i)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
